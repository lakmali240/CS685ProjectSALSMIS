{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJHiUb9AHUbm"
      },
      "source": [
        "# <font color=\"blue\">**CS685  : Project**</font> \n",
        "\n",
        "###Self-Supervised U-Net model implementation and weight saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ5NLhUyE6J3",
        "outputId": "ee20c7f2-a3d9-4f4f-d9ea-9b77cc5e0445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "## mounting the google drive to load the data from directory\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YFPc85P6hOdf",
        "outputId": "4e3a8c73-6b99-4fc2-bd78-be9d79bd8fae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UQFqyjRhrX-",
        "outputId": "b35891f0-5e5a-446f-a5b1-227e48db39ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6403104569611913164\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14343274496\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 4398778040285535822\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6do4FKFFOaM"
      },
      "source": [
        "#PART 1: Self Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq4C1jM9FHMs"
      },
      "source": [
        "##Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpu6h0-_ElCg"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import scipy\n",
        "import imageio\n",
        "import string\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from skimage.transform import resize\n",
        "try:  # SciPy >= 0.19\n",
        "    from scipy.special import comb\n",
        "except ImportError:\n",
        "    from scipy.misc import comb\n",
        "\n",
        "def bernstein_poly(i, n, t):\n",
        "    \"\"\"\n",
        "     The Bernstein polynomial of n, i as a function of t\n",
        "    \"\"\"\n",
        "\n",
        "    return comb(n, i) * ( t**(n-i) ) * (1 - t)**i\n",
        "\n",
        "def bezier_curve(points, nTimes=1000):\n",
        "    \"\"\"\n",
        "       Given a set of control points, return the\n",
        "       bezier curve defined by the control points.\n",
        "\n",
        "       Control points should be a list of lists, or list of tuples\n",
        "       such as [ [1,1], \n",
        "                 [2,3], \n",
        "                 [4,5], ..[Xn, Yn] ]\n",
        "        nTimes is the number of time steps, defaults to 1000\n",
        "\n",
        "        See http://processingjs.nihongoresources.com/bezierinfo/\n",
        "    \"\"\"\n",
        "\n",
        "    nPoints = len(points)\n",
        "    xPoints = np.array([p[0] for p in points])\n",
        "    yPoints = np.array([p[1] for p in points])\n",
        "\n",
        "    t = np.linspace(0.0, 1.0, nTimes)\n",
        "\n",
        "    polynomial_array = np.array([ bernstein_poly(i, nPoints-1, t) for i in range(0, nPoints)   ])\n",
        "    \n",
        "    xvals = np.dot(xPoints, polynomial_array)\n",
        "    yvals = np.dot(yPoints, polynomial_array)\n",
        "\n",
        "    return xvals, yvals\n",
        "\n",
        "def data_augmentation(x, y, prob=0.5):\n",
        "    # augmentation by flipping\n",
        "    cnt = 3\n",
        "    while random.random() < prob and cnt > 0:\n",
        "        degree = random.choice([0, 1, 2])\n",
        "        x = np.flip(x, axis=degree)\n",
        "        y = np.flip(y, axis=degree)\n",
        "        cnt = cnt - 1\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def nonlinear_transformation(x, prob=0.5):\n",
        "    if random.random() >= prob:\n",
        "        return x\n",
        "    points = [[0, 0], [random.random(), random.random()], [random.random(), random.random()], [1, 1]]\n",
        "    xpoints = [p[0] for p in points]\n",
        "    ypoints = [p[1] for p in points]\n",
        "    xvals, yvals = bezier_curve(points, nTimes=100000)\n",
        "    if random.random() < 0.5:\n",
        "        # Half change to get flip\n",
        "        xvals = np.sort(xvals)\n",
        "    else:\n",
        "        xvals, yvals = np.sort(xvals), np.sort(yvals)\n",
        "    nonlinear_x = np.interp(x, xvals, yvals)\n",
        "    return nonlinear_x\n",
        "\n",
        "def local_pixel_shuffling(x, prob=0.5):\n",
        "    if random.random() >= prob:\n",
        "        return x\n",
        "    image_temp = copy.deepcopy(x)\n",
        "    orig_image = copy.deepcopy(x)\n",
        "    _, img_rows, img_cols = x.shape\n",
        "    num_block = 10000\n",
        "    for _ in range(num_block):\n",
        "        block_noise_size_x = random.randint(1, img_rows//10)\n",
        "        block_noise_size_y = random.randint(1, img_cols//10)\n",
        "        noise_x = random.randint(0, img_rows-block_noise_size_x)\n",
        "        noise_y = random.randint(0, img_cols-block_noise_size_y)\n",
        "        window = orig_image[0, noise_x:noise_x+block_noise_size_x, \n",
        "                               noise_y:noise_y+block_noise_size_y, \n",
        "                           ]\n",
        "        window = window.flatten()\n",
        "        np.random.shuffle(window)\n",
        "        window = window.reshape((block_noise_size_x, \n",
        "                                 block_noise_size_y))\n",
        "        image_temp[0, noise_x:noise_x+block_noise_size_x, \n",
        "                      noise_y:noise_y+block_noise_size_y] = window\n",
        "    local_shuffling_x = image_temp\n",
        "\n",
        "    return local_shuffling_x\n",
        "\n",
        "def image_in_painting(x):\n",
        "    _, img_rows, img_cols = x.shape\n",
        "    cnt = 5\n",
        "    while cnt > 0 and random.random() < 0.95:\n",
        "        block_noise_size_x = random.randint(img_rows//6, img_rows//3)\n",
        "        block_noise_size_y = random.randint(img_cols//6, img_cols//3)\n",
        "        noise_x = random.randint(3, img_rows-block_noise_size_x-3)\n",
        "        noise_y = random.randint(3, img_cols-block_noise_size_y-3)\n",
        "        x[:, \n",
        "          noise_x:noise_x+block_noise_size_x, \n",
        "          noise_y:noise_y+block_noise_size_y] = np.random.rand(block_noise_size_x, \n",
        "                                                               block_noise_size_y, ) * 1.0\n",
        "        cnt -= 1\n",
        "    return x\n",
        "\n",
        "def image_out_painting(x):\n",
        "    _, img_rows, img_cols = x.shape\n",
        "    image_temp = copy.deepcopy(x)\n",
        "    x = np.random.rand(x.shape[0], x.shape[1], x.shape[2], ) * 1.0\n",
        "    block_noise_size_x = img_rows - random.randint(3*img_rows//7, 4*img_rows//7)\n",
        "    block_noise_size_y = img_cols - random.randint(3*img_cols//7, 4*img_cols//7)\n",
        "    noise_x = random.randint(3, img_rows-block_noise_size_x-3)\n",
        "    noise_y = random.randint(3, img_cols-block_noise_size_y-3)\n",
        "    x[:, \n",
        "      noise_x:noise_x+block_noise_size_x, \n",
        "      noise_y:noise_y+block_noise_size_y] = image_temp[:, noise_x:noise_x+block_noise_size_x, \n",
        "                                                       noise_y:noise_y+block_noise_size_y]\n",
        "    cnt = 4\n",
        "    while cnt > 0 and random.random() < 0.95:\n",
        "        block_noise_size_x = img_rows - random.randint(3*img_rows//7, 4*img_rows//7)\n",
        "        block_noise_size_y = img_cols - random.randint(3*img_cols//7, 4*img_cols//7)\n",
        "        noise_x = random.randint(3, img_rows-block_noise_size_x-3)\n",
        "        noise_y = random.randint(3, img_cols-block_noise_size_y-3)\n",
        "        x[:, \n",
        "          noise_x:noise_x+block_noise_size_x, \n",
        "          noise_y:noise_y+block_noise_size_y] = image_temp[:, noise_x:noise_x+block_noise_size_x, \n",
        "                                                           noise_y:noise_y+block_noise_size_y]\n",
        "        cnt -= 1\n",
        "    return x\n",
        "                \n",
        "\n",
        "\n",
        "def generate_pair(img, batch_size, config, status=\"test\"):\n",
        "    img_rows, img_cols = img.shape[2], img.shape[3]\n",
        "    while True:\n",
        "        index = [i for i in range(img.shape[0])]\n",
        "        random.shuffle(index)\n",
        "        y = img[index[:batch_size]]\n",
        "        x = copy.deepcopy(y)\n",
        "        for n in range(batch_size):\n",
        "            \n",
        "            # Autoencoder\n",
        "            x[n] = copy.deepcopy(y[n])\n",
        "            \n",
        "            # Flip\n",
        "            x[n], y[n] = data_augmentation(x[n], y[n], config.flip_rate)\n",
        "\n",
        "            # Local Shuffle Pixel\n",
        "            x[n] = local_pixel_shuffling(x[n], prob=config.local_rate)\n",
        "            \n",
        "            # Apply non-Linear transformation with an assigned probability\n",
        "            x[n] = nonlinear_transformation(x[n], config.nonlinear_rate)\n",
        "            \n",
        "            # Inpainting & Outpainting\n",
        "            if random.random() < config.paint_rate:\n",
        "                if random.random() < config.inpaint_rate:\n",
        "                    # Inpainting\n",
        "                    x[n] = image_in_painting(x[n])\n",
        "                else:\n",
        "                    # Outpainting\n",
        "                    x[n] = image_out_painting(x[n])\n",
        "\n",
        "        # Save sample images module\n",
        "        if config.save_samples is not None and status == \"train\" and random.random() < 0.01:\n",
        "            n_sample = random.choice( [i for i in range(config.batch_size)] )\n",
        "            final_sample = np.concatenate((x[n_sample,0,:,:], y[n_sample,0,:,:]), axis=1)\n",
        "            final_sample = final_sample * 255.0\n",
        "            final_sample = final_sample.astype(np.uint8)\n",
        "            file_name = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(10)])+'.'+config.save_samples\n",
        "            imageio.imwrite(os.path.join(config.sample_path, config.exp_name, file_name), final_sample)\n",
        "\n",
        "        yield (x, y)\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xEowzROFBz2"
      },
      "source": [
        "#UNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7onEP8aE83G"
      },
      "outputs": [],
      "source": [
        "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
        "\n",
        "\"\"\" Parts of the U-Net model \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.conv(x))\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "    \n",
        "        #per_out=[]\n",
        "        \n",
        "        x1 = self.inc(x)\n",
        "        #per_out.append(x1) # conv1\n",
        "        \n",
        "        x2 = self.down1(x1)\n",
        "        #per_out.append(x2) # down1\n",
        "        \n",
        "        x3 = self.down2(x2)\n",
        "        #per_out.append(x3) # down2\n",
        "        \n",
        "        x4 = self.down3(x3)\n",
        "        #per_out.append(x4) # down3\n",
        "        \n",
        "        x5 = self.down4(x4)\n",
        "        #per_out.append(x5) # down4\n",
        "        \n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "    \n",
        "class UNet_hidden(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet_hidden, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "    \n",
        "        #per_out=[]\n",
        "        \n",
        "        x1 = self.inc(x)\n",
        "        #per_out.append(x1) # conv1\n",
        "        \n",
        "        x2 = self.down1(x1)\n",
        "        #per_out.append(x2) # down1\n",
        "        \n",
        "        x3 = self.down2(x2)\n",
        "        #per_out.append(x3) # down2\n",
        "        \n",
        "        x4 = self.down3(x3)\n",
        "        #per_out.append(x4) # down3\n",
        "        \n",
        "        x5 = self.down4(x4)\n",
        "        #per_out.append(x5) # down4\n",
        "        \n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        \n",
        "        return logits, x5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HukLOnkkiMeH"
      },
      "source": [
        "##Swin_UNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNyFqbI4zoFk"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # from torchvision.models.utils import load_state_dict_from_url\n",
        "\n",
        "# # Swin Transformer Encoder Block\n",
        "# class SwinTransformerEncoderBlock(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads, window_size, shift_size, mlp_ratio=4.0):\n",
        "#         super().__init__()\n",
        "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
        "#         self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
        "#         self.mlp = nn.Sequential(\n",
        "#             nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
        "#         )\n",
        "\n",
        "#         # Window partitioning and shifting\n",
        "#         self.window_size = window_size\n",
        "#         self.shift_size = shift_size\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Apply layer normalization\n",
        "#         x = self.norm1(x)\n",
        "\n",
        "#         # Apply self-attention\n",
        "#         x = x.permute(2, 0, 1)\n",
        "#         x, _ = self.attn(x, x, x)\n",
        "#         x = x.permute(1, 2, 0)\n",
        "\n",
        "#         # Apply residual connection and layer normalization\n",
        "#         x = x + x.permute(0, 2, 1)\n",
        "#         x = self.norm2(x)\n",
        "\n",
        "#         # Apply MLP\n",
        "#         y = self.mlp(x)\n",
        "\n",
        "#         # Partition into non-overlapping windows and shift\n",
        "#         B, N, C = y.shape\n",
        "#         h = self.window_size\n",
        "#         w = N // h\n",
        "#         y = y.view(B, h, w, C)\n",
        "#         y = y.permute(0, 3, 1, 2)\n",
        "#         y = torch.nn.functional.pad(y, (0, 0, 0, 0, self.shift_size // 2, self.shift_size // 2), mode=\"constant\")\n",
        "#         y = y.reshape(B, C, h * (w + 2 * (self.shift_size // 2)))\n",
        "#         y = y.permute(0, 2, 1)\n",
        "\n",
        "#         # Apply residual connection\n",
        "#         y = y + x\n",
        "\n",
        "#         return y\n",
        "\n",
        "# # Swin UNet Model\n",
        "# class SwinUNet(nn.Module):\n",
        "#     def __init__(self, input_channels, num_classes):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # Swin Transformer Encoder Blocks\n",
        "#         self.enc1 = SwinTransformerEncoderBlock(embed_dim=64, num_heads=2, window_size=7, shift_size=0)\n",
        "#         self.enc2 = SwinTransformerEncoderBlock(embed_dim=128, num_heads=4, window_size=7, shift_size=0)\n",
        "#         self.enc3 = SwinTransformerEncoderBlock(embed_dim=256, num_heads=8, window_size=7, shift_size=0)\n",
        "#         self.enc4 = SwinTransformerEncoderBlock(embed_dim=512, num_heads=16, window_size=7, shift_size=0)\n",
        "\n",
        "#         # Swin Transformer Decoder Blocks\n",
        "#         self.dec1 = SwinTransformerEncoderBlock(embed_dim=256, num_heads=8, window_size=7, shift_size=0)\n",
        "#         self.dec2 = SwinTransformerEncoderBlock(embed_dim=128, num_heads=4, window_size=7, shift_size=0)\n",
        "#         self.dec3 = SwinTransformerEncoderBlock(embed_dim=64, num_heads=2, window_size=7, shift_size=0)\n",
        "\n",
        "#         # Final convolutional layer for segmentation output\n",
        "#         self.final_conv = nn.Conv2d(96, num_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Encoder\n",
        "#         x1 = self.enc1(x)\n",
        "#         x2 = self.enc2(x1)\n",
        "#         x3 = self.enc3(x2)\n",
        "#         x4 = self.enc4(x3)\n",
        "\n",
        "#         # Decoder\n",
        "#         y1 = self.dec1(x4) + nn.functional.interpolate(x4, scale_factor=2, mode=\"nearest\")\n",
        "#         y2 = self.dec2(y1) + nn.functional.interpolate(y1, scale_factor=2, mode=\"nearest\")\n",
        "#         y3 = self.dec3(y2) + nn.functional.interpolate(y2, scale_factor=2, mode=\"nearest\")\n",
        "\n",
        "#         # Output\n",
        "#         y = self.final_conv(y3)\n",
        "#         return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIjhLRQpiPZQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from torchvision.models.utils import load_state_dict_from_url\n",
        "\n",
        "# Swin Transformer Encoder Block\n",
        "class SwinTransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size, shift_size, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim,(256, 256))\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim,(256, 256))\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
        "        )\n",
        "\n",
        "        # Window partitioning and shifting\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply layer normalization\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Apply self-attention\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        B, H, W, C = x.shape\n",
        "        x = x.reshape(B * H * W, C, 1)\n",
        "        x, _ = self.attn(x, x, x)\n",
        "        x = x.reshape(B, H, W, C)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "\n",
        "        # Apply residual connection and layer normalization\n",
        "        x = x + x.permute(0, 2, 3, 1)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # Apply MLP\n",
        "        y = self.mlp(x)\n",
        "\n",
        "        # Partition into non-overlapping windows and shift\n",
        "        B, N, C = y.shape\n",
        "        h = self.window_size\n",
        "        w = N // h\n",
        "        y = y.view(B, h, w, C)\n",
        "        y = y.permute(0, 3, 1, 2)\n",
        "        y = torch.nn.functional.pad(y, (0, 0, 0, 0, self.shift_size // 2, self.shift_size // 2), mode=\"constant\")\n",
        "        y = y.reshape(B, C, h * (w + 2 * (self.shift_size // 2)))\n",
        "        y = y.permute(0, 2, 1)\n",
        "\n",
        "        # Apply residual connection\n",
        "        y = y + x\n",
        "\n",
        "        return y\n",
        "\n",
        "# Swin UNet Model\n",
        "class SwinUNet(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Swin Transformer Encoder Blocks\n",
        "        self.enc1 = SwinTransformerEncoderBlock(embed_dim=64, num_heads=2, window_size=7, shift_size=0,mlp_ratio=4.0)\n",
        "        self.enc2 = SwinTransformerEncoderBlock(embed_dim=128, num_heads=4, window_size=7, shift_size=0,mlp_ratio=4.0)\n",
        "        self.enc3 = SwinTransformerEncoderBlock(embed_dim=256, num_heads=8, window_size=7, shift_size=0,mlp_ratio=4.0)\n",
        "        self.enc4 = SwinTransformerEncoderBlock(embed_dim=512, num_heads=16, window_size=7, shift_size=0,mlp_ratio=4.0)\n",
        "\n",
        "        # Swin Transformer Decoder Blocks\n",
        "        self.dec1 = SwinTransformerEncoderBlock(embed_dim=256, num_heads=8, window_size=7, shift_size=0,mlp_ratio=4.0)\n",
        "        self.dec2 = SwinTransformerEncoderBlock(embed_dim=128, num_heads=4, window_size=7, shift_size=0,mlp_ratio=4.0)\n",
        "        self.dec3 = SwinTransformerEncoderBlock(embed_dim=64, num_heads=2, window_size=7, shift_size=0,mlp_ratio=4.0)\n",
        "\n",
        "        # Final convolutional layer for segmentation output\n",
        "        self.final_conv = nn.Conv2d(96, num_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # Modify the input channel of the first convolutional layer\n",
        "        self.conv_input = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.conv_input(x)\n",
        "        x1 = self.enc1(x)\n",
        "        x2 = self.enc2(x1)\n",
        "        x3 = self.enc3(x2)\n",
        "        x4 = self.enc4(x3)\n",
        "\n",
        "        # Decoder\n",
        "        y1 = self.dec1(x4) + nn.functional.interpolate(x4, scale_factor=2, mode=\"nearest\")\n",
        "        y2 = self.dec2(y1) + nn.functional.interpolate(y1, scale_factor=2, mode=\"nearest\")\n",
        "        y3 = self.dec3(y2) + nn.functional.interpolate(y2, scale_factor=2, mode=\"nearest\")\n",
        "\n",
        "        # Output\n",
        "        y = self.final_conv(y3)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chYBpoNxFQz1"
      },
      "source": [
        "##Config_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhkePFepFU5_",
        "outputId": "6f8c0db8-a1b9-47ea-97dd-0f94c4283b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path:  /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04\n",
            "log path:  /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04/Logs\n",
            "snapshot path:  /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04/snapshot\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "class models_genesis_config:\n",
        "    model = \"Unet2D\"\n",
        "    suffix = \"genesis_chest_ct\"\n",
        "    exp_name = model + \"-\" + suffix\n",
        "    \n",
        "    # data\n",
        "    data = \"/mnt/dataset/shared/zongwei/LUNA16/Self_Learning_Cubes\" # not use\n",
        "    scale = 32\n",
        "    input_rows = 256\n",
        "    input_cols = 256\n",
        "    input_deps = 1\n",
        "    nb_class = 1\n",
        "\n",
        "    # image deformation\n",
        "    nonlinear_rate = 0.9\n",
        "    paint_rate = 0.9\n",
        "    outpaint_rate = 0.8\n",
        "    inpaint_rate = 1.0 - outpaint_rate\n",
        "    local_rate = 0.5\n",
        "    flip_rate = 0.4\n",
        "    \n",
        "    # logs\n",
        "    # model_dir = \"../SSLModel/Reuslts/pretrained_weights\"\n",
        "    model_dir = \"/content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights\"\n",
        "    timenow = datetime.strftime(datetime.now(pytz.timezone('Asia/Singapore')), '%Y-%m-%d_%H-%M-%S')\n",
        "    model_path = os.path.join(model_dir,timenow)\n",
        "    print('Model path: ',model_path)\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "        \n",
        "    logs_path = os.path.join(model_path, \"Logs\")\n",
        "    print('log path: ',logs_path)\n",
        "    if not os.path.exists(logs_path):\n",
        "        os.makedirs(logs_path)\n",
        "        \n",
        "    shotdir = os.path.join(model_path, 'snapshot')\n",
        "    print('snapshot path: ',shotdir)\n",
        "    if not os.path.exists(shotdir):\n",
        "        os.makedirs(shotdir)\n",
        "    \n",
        "    # model pre-training\n",
        "    verbose = 1\n",
        "    weights = os.path.join(model_path,'ISIC_Unsup.pt')\n",
        "    batch_size = 1\n",
        "    optimizer = \"sgd\"\n",
        "    workers = 10\n",
        "    max_queue_size = workers * 4\n",
        "    save_samples = \"png\"\n",
        "    nb_epoch = 10000\n",
        "    patience = 100\n",
        "    lr = 0.01\n",
        "    \n",
        "    def display(self):\n",
        "        \"\"\"Display Configuration values.\"\"\"\n",
        "        print(\"\\nConfigurations:\")\n",
        "        for a in dir(self):\n",
        "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
        "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osDqssExFcnP"
      },
      "source": [
        "##Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ2VNWYaFfCN",
        "outputId": "515e1eb9-ca05-4aa8-c8e3-f70876ec9c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading dataset...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import imgaug.augmenters as iaa\n",
        "#from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "def Dataset_Loader(path, img_size):\n",
        "    print('\\nLoading dataset...\\n')\n",
        "    read_imgs = np.load(path)\n",
        "    rows = img_size[0]\n",
        "    cols = img_size[1]\n",
        "    \n",
        "    images = np.ndarray((read_imgs.shape[0], read_imgs.shape[-1], rows, cols), dtype=float)\n",
        "    for i in range(read_imgs.shape[0]):\n",
        "        img = cv2.resize(read_imgs[i, 0], (cols, rows), interpolation=cv2.INTER_CUBIC)\n",
        "        images[i, 0, :, :] = img/255.\n",
        "    return images\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_path_train = '/content/drive/MyDrive/Spring_research_2023/data/GrayData'\n",
        "    trainpath = data_path_train + '/imgs_train.npy'\n",
        "    \n",
        "    dataset = Dataset_Loader(trainpath,[256,256])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lflGrDgUE9Vt"
      },
      "source": [
        "##Pre-train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rb16lbVEs-Y",
        "outputId": "f5ed6e7d-7c10-439c-92eb-9975a81fb13a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch = 2.0.0+cu118\n",
            "\n",
            "Configurations:\n",
            "batch_size                     1\n",
            "data                           /mnt/dataset/shared/zongwei/LUNA16/Self_Learning_Cubes\n",
            "exp_name                       Unet2D-genesis_chest_ct\n",
            "flip_rate                      0.4\n",
            "inpaint_rate                   0.19999999999999996\n",
            "input_cols                     256\n",
            "input_deps                     1\n",
            "input_rows                     256\n",
            "local_rate                     0.5\n",
            "logs_path                      /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04/Logs\n",
            "lr                             0.01\n",
            "max_queue_size                 40\n",
            "model                          Unet2D\n",
            "model_dir                      /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights\n",
            "model_path                     /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04\n",
            "nb_class                       1\n",
            "nb_epoch                       10000\n",
            "nonlinear_rate                 0.9\n",
            "optimizer                      sgd\n",
            "outpaint_rate                  0.8\n",
            "paint_rate                     0.9\n",
            "patience                       100\n",
            "save_samples                   png\n",
            "scale                          32\n",
            "shotdir                        /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04/snapshot\n",
            "suffix                         genesis_chest_ct\n",
            "timenow                        2023-04-22_20-58-04\n",
            "verbose                        1\n",
            "weights                        /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04/ISIC_Unsup.pt\n",
            "workers                        10\n",
            "\n",
            "\n",
            "\n",
            "Loading dataset...\n",
            "\n",
            "x_train: (1600, 1, 256, 256) | 0.00 ~ 1.00\n",
            "x_valid: (400, 1, 256, 256) | 0.00 ~ 1.00\n",
            "Total CUDA devices:  0\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 256, 256]             576\n",
            "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
            "              ReLU-3         [-1, 64, 256, 256]               0\n",
            "            Conv2d-4         [-1, 64, 256, 256]          36,864\n",
            "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
            "              ReLU-6         [-1, 64, 256, 256]               0\n",
            "        DoubleConv-7         [-1, 64, 256, 256]               0\n",
            "         MaxPool2d-8         [-1, 64, 128, 128]               0\n",
            "            Conv2d-9        [-1, 128, 128, 128]          73,728\n",
            "      BatchNorm2d-10        [-1, 128, 128, 128]             256\n",
            "             ReLU-11        [-1, 128, 128, 128]               0\n",
            "           Conv2d-12        [-1, 128, 128, 128]         147,456\n",
            "      BatchNorm2d-13        [-1, 128, 128, 128]             256\n",
            "             ReLU-14        [-1, 128, 128, 128]               0\n",
            "       DoubleConv-15        [-1, 128, 128, 128]               0\n",
            "             Down-16        [-1, 128, 128, 128]               0\n",
            "        MaxPool2d-17          [-1, 128, 64, 64]               0\n",
            "           Conv2d-18          [-1, 256, 64, 64]         294,912\n",
            "      BatchNorm2d-19          [-1, 256, 64, 64]             512\n",
            "             ReLU-20          [-1, 256, 64, 64]               0\n",
            "           Conv2d-21          [-1, 256, 64, 64]         589,824\n",
            "      BatchNorm2d-22          [-1, 256, 64, 64]             512\n",
            "             ReLU-23          [-1, 256, 64, 64]               0\n",
            "       DoubleConv-24          [-1, 256, 64, 64]               0\n",
            "             Down-25          [-1, 256, 64, 64]               0\n",
            "        MaxPool2d-26          [-1, 256, 32, 32]               0\n",
            "           Conv2d-27          [-1, 512, 32, 32]       1,179,648\n",
            "      BatchNorm2d-28          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-29          [-1, 512, 32, 32]               0\n",
            "           Conv2d-30          [-1, 512, 32, 32]       2,359,296\n",
            "      BatchNorm2d-31          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-32          [-1, 512, 32, 32]               0\n",
            "       DoubleConv-33          [-1, 512, 32, 32]               0\n",
            "             Down-34          [-1, 512, 32, 32]               0\n",
            "        MaxPool2d-35          [-1, 512, 16, 16]               0\n",
            "           Conv2d-36          [-1, 512, 16, 16]       2,359,296\n",
            "      BatchNorm2d-37          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-38          [-1, 512, 16, 16]               0\n",
            "           Conv2d-39          [-1, 512, 16, 16]       2,359,296\n",
            "      BatchNorm2d-40          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-41          [-1, 512, 16, 16]               0\n",
            "       DoubleConv-42          [-1, 512, 16, 16]               0\n",
            "             Down-43          [-1, 512, 16, 16]               0\n",
            "         Upsample-44          [-1, 512, 32, 32]               0\n",
            "           Conv2d-45          [-1, 512, 32, 32]       4,718,592\n",
            "      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-47          [-1, 512, 32, 32]               0\n",
            "           Conv2d-48          [-1, 256, 32, 32]       1,179,648\n",
            "      BatchNorm2d-49          [-1, 256, 32, 32]             512\n",
            "             ReLU-50          [-1, 256, 32, 32]               0\n",
            "       DoubleConv-51          [-1, 256, 32, 32]               0\n",
            "               Up-52          [-1, 256, 32, 32]               0\n",
            "         Upsample-53          [-1, 256, 64, 64]               0\n",
            "           Conv2d-54          [-1, 256, 64, 64]       1,179,648\n",
            "      BatchNorm2d-55          [-1, 256, 64, 64]             512\n",
            "             ReLU-56          [-1, 256, 64, 64]               0\n",
            "           Conv2d-57          [-1, 128, 64, 64]         294,912\n",
            "      BatchNorm2d-58          [-1, 128, 64, 64]             256\n",
            "             ReLU-59          [-1, 128, 64, 64]               0\n",
            "       DoubleConv-60          [-1, 128, 64, 64]               0\n",
            "               Up-61          [-1, 128, 64, 64]               0\n",
            "         Upsample-62        [-1, 128, 128, 128]               0\n",
            "           Conv2d-63        [-1, 128, 128, 128]         294,912\n",
            "      BatchNorm2d-64        [-1, 128, 128, 128]             256\n",
            "             ReLU-65        [-1, 128, 128, 128]               0\n",
            "           Conv2d-66         [-1, 64, 128, 128]          73,728\n",
            "      BatchNorm2d-67         [-1, 64, 128, 128]             128\n",
            "             ReLU-68         [-1, 64, 128, 128]               0\n",
            "       DoubleConv-69         [-1, 64, 128, 128]               0\n",
            "               Up-70         [-1, 64, 128, 128]               0\n",
            "         Upsample-71         [-1, 64, 256, 256]               0\n",
            "           Conv2d-72         [-1, 64, 256, 256]          73,728\n",
            "      BatchNorm2d-73         [-1, 64, 256, 256]             128\n",
            "             ReLU-74         [-1, 64, 256, 256]               0\n",
            "           Conv2d-75         [-1, 64, 256, 256]          36,864\n",
            "      BatchNorm2d-76         [-1, 64, 256, 256]             128\n",
            "             ReLU-77         [-1, 64, 256, 256]               0\n",
            "       DoubleConv-78         [-1, 64, 256, 256]               0\n",
            "               Up-79         [-1, 64, 256, 256]               0\n",
            "           Conv2d-80          [-1, 1, 256, 256]              65\n",
            "          Sigmoid-81          [-1, 1, 256, 256]               0\n",
            "          OutConv-82          [-1, 1, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 17,261,825\n",
            "Trainable params: 17,261,825\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 942.50\n",
            "Params size (MB): 65.85\n",
            "Estimated Total Size (MB): 1008.60\n",
            "----------------------------------------------------------------\n",
            "/content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04/ISIC_Unsup.pt\n",
            "Epoch [1/10000], iteration 5, Loss: 0.040000\n",
            "Epoch [1/10000], iteration 10, Loss: 0.029000\n",
            "Epoch [1/10000], iteration 15, Loss: 0.029333\n",
            "Epoch [1/10000], iteration 20, Loss: 0.028500\n",
            "Epoch [1/10000], iteration 25, Loss: 0.031200\n",
            "Epoch [1/10000], iteration 30, Loss: 0.030667\n",
            "Epoch [1/10000], iteration 35, Loss: 0.041714\n",
            "Epoch [1/10000], iteration 40, Loss: 0.040500\n",
            "Epoch [1/10000], iteration 45, Loss: 0.037333\n",
            "Epoch [1/10000], iteration 50, Loss: 0.034200\n",
            "Epoch [1/10000], iteration 55, Loss: 0.034182\n",
            "Epoch [1/10000], iteration 60, Loss: 0.033333\n",
            "Epoch [1/10000], iteration 65, Loss: 0.037692\n",
            "Epoch [1/10000], iteration 70, Loss: 0.036286\n",
            "Epoch [1/10000], iteration 75, Loss: 0.034667\n",
            "Epoch [1/10000], iteration 80, Loss: 0.033500\n",
            "Epoch [1/10000], iteration 85, Loss: 0.033412\n",
            "Epoch [1/10000], iteration 90, Loss: 0.032222\n",
            "Epoch [1/10000], iteration 95, Loss: 0.031895\n",
            "Epoch [1/10000], iteration 100, Loss: 0.031700\n",
            "validating....\n",
            "Epoch 1, validation loss is 0.0365, training loss is 0.0317\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 0\n",
            "Epoch [2/10000], iteration 5, Loss: 0.012000\n",
            "Epoch [2/10000], iteration 10, Loss: 0.018000\n",
            "Epoch [2/10000], iteration 15, Loss: 0.027333\n",
            "Epoch [2/10000], iteration 20, Loss: 0.024500\n",
            "Epoch [2/10000], iteration 25, Loss: 0.021600\n",
            "Epoch [2/10000], iteration 30, Loss: 0.021667\n",
            "Epoch [2/10000], iteration 35, Loss: 0.021429\n",
            "Epoch [2/10000], iteration 40, Loss: 0.021500\n",
            "Epoch [2/10000], iteration 45, Loss: 0.023333\n",
            "Epoch [2/10000], iteration 50, Loss: 0.024400\n",
            "Epoch [2/10000], iteration 55, Loss: 0.023818\n",
            "Epoch [2/10000], iteration 60, Loss: 0.030000\n",
            "Epoch [2/10000], iteration 65, Loss: 0.028615\n",
            "Epoch [2/10000], iteration 70, Loss: 0.028714\n",
            "Epoch [2/10000], iteration 75, Loss: 0.028533\n",
            "Epoch [2/10000], iteration 80, Loss: 0.030125\n",
            "Epoch [2/10000], iteration 85, Loss: 0.028941\n",
            "Epoch [2/10000], iteration 90, Loss: 0.029000\n",
            "Epoch [2/10000], iteration 95, Loss: 0.029368\n",
            "Epoch [2/10000], iteration 100, Loss: 0.028400\n",
            "validating....\n",
            "Epoch 2, validation loss is 0.0308, training loss is 0.0284\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 1\n",
            "Epoch [3/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [3/10000], iteration 10, Loss: 0.023000\n",
            "Epoch [3/10000], iteration 15, Loss: 0.021333\n",
            "Epoch [3/10000], iteration 20, Loss: 0.024000\n",
            "Epoch [3/10000], iteration 25, Loss: 0.022000\n",
            "Epoch [3/10000], iteration 30, Loss: 0.023333\n",
            "Epoch [3/10000], iteration 35, Loss: 0.033143\n",
            "Epoch [3/10000], iteration 40, Loss: 0.031000\n",
            "Epoch [3/10000], iteration 45, Loss: 0.030667\n",
            "Epoch [3/10000], iteration 50, Loss: 0.031000\n",
            "Epoch [3/10000], iteration 55, Loss: 0.030364\n",
            "Epoch [3/10000], iteration 60, Loss: 0.030167\n",
            "Epoch [3/10000], iteration 65, Loss: 0.029846\n",
            "Epoch [3/10000], iteration 70, Loss: 0.034571\n",
            "Epoch [3/10000], iteration 75, Loss: 0.034667\n",
            "Epoch [3/10000], iteration 80, Loss: 0.034375\n",
            "Epoch [3/10000], iteration 85, Loss: 0.037529\n",
            "Epoch [3/10000], iteration 90, Loss: 0.036556\n",
            "Epoch [3/10000], iteration 95, Loss: 0.036105\n",
            "Epoch [3/10000], iteration 100, Loss: 0.036700\n",
            "validating....\n",
            "Epoch 3, validation loss is 0.0325, training loss is 0.0367\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 2\n",
            "Epoch [4/10000], iteration 5, Loss: 0.006000\n",
            "Epoch [4/10000], iteration 10, Loss: 0.051000\n",
            "Epoch [4/10000], iteration 15, Loss: 0.044000\n",
            "Epoch [4/10000], iteration 20, Loss: 0.039500\n",
            "Epoch [4/10000], iteration 25, Loss: 0.041200\n",
            "Epoch [4/10000], iteration 30, Loss: 0.038667\n",
            "Epoch [4/10000], iteration 35, Loss: 0.040000\n",
            "Epoch [4/10000], iteration 40, Loss: 0.038000\n",
            "Epoch [4/10000], iteration 45, Loss: 0.037556\n",
            "Epoch [4/10000], iteration 50, Loss: 0.035400\n",
            "Epoch [4/10000], iteration 55, Loss: 0.033273\n",
            "Epoch [4/10000], iteration 60, Loss: 0.031833\n",
            "Epoch [4/10000], iteration 65, Loss: 0.030769\n",
            "Epoch [4/10000], iteration 70, Loss: 0.029286\n",
            "Epoch [4/10000], iteration 75, Loss: 0.028800\n",
            "Epoch [4/10000], iteration 80, Loss: 0.028000\n",
            "Epoch [4/10000], iteration 85, Loss: 0.026941\n",
            "Epoch [4/10000], iteration 90, Loss: 0.026778\n",
            "Epoch [4/10000], iteration 95, Loss: 0.030737\n",
            "Epoch [4/10000], iteration 100, Loss: 0.030300\n",
            "validating....\n",
            "Epoch 4, validation loss is 0.0377, training loss is 0.0303\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 3\n",
            "Epoch [5/10000], iteration 5, Loss: 0.040000\n",
            "Epoch [5/10000], iteration 10, Loss: 0.033000\n",
            "Epoch [5/10000], iteration 15, Loss: 0.030000\n",
            "Epoch [5/10000], iteration 20, Loss: 0.029500\n",
            "Epoch [5/10000], iteration 25, Loss: 0.028000\n",
            "Epoch [5/10000], iteration 30, Loss: 0.033667\n",
            "Epoch [5/10000], iteration 35, Loss: 0.042000\n",
            "Epoch [5/10000], iteration 40, Loss: 0.039250\n",
            "Epoch [5/10000], iteration 45, Loss: 0.035556\n",
            "Epoch [5/10000], iteration 50, Loss: 0.032600\n",
            "Epoch [5/10000], iteration 55, Loss: 0.035455\n",
            "Epoch [5/10000], iteration 60, Loss: 0.042667\n",
            "Epoch [5/10000], iteration 65, Loss: 0.048154\n",
            "Epoch [5/10000], iteration 70, Loss: 0.046286\n",
            "Epoch [5/10000], iteration 75, Loss: 0.044667\n",
            "Epoch [5/10000], iteration 80, Loss: 0.042750\n",
            "Epoch [5/10000], iteration 85, Loss: 0.041765\n",
            "Epoch [5/10000], iteration 90, Loss: 0.041000\n",
            "Epoch [5/10000], iteration 95, Loss: 0.041368\n",
            "Epoch [5/10000], iteration 100, Loss: 0.040600\n",
            "validating....\n",
            "Epoch 5, validation loss is 0.0350, training loss is 0.0406\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 4\n",
            "Epoch [6/10000], iteration 5, Loss: 0.030000\n",
            "Epoch [6/10000], iteration 10, Loss: 0.036000\n",
            "Epoch [6/10000], iteration 15, Loss: 0.030667\n",
            "Epoch [6/10000], iteration 20, Loss: 0.026000\n",
            "Epoch [6/10000], iteration 25, Loss: 0.025600\n",
            "Epoch [6/10000], iteration 30, Loss: 0.024000\n",
            "Epoch [6/10000], iteration 35, Loss: 0.023143\n",
            "Epoch [6/10000], iteration 40, Loss: 0.023500\n",
            "Epoch [6/10000], iteration 45, Loss: 0.022000\n",
            "Epoch [6/10000], iteration 50, Loss: 0.021600\n",
            "Epoch [6/10000], iteration 55, Loss: 0.021273\n",
            "Epoch [6/10000], iteration 60, Loss: 0.021500\n",
            "Epoch [6/10000], iteration 65, Loss: 0.020923\n",
            "Epoch [6/10000], iteration 70, Loss: 0.021571\n",
            "Epoch [6/10000], iteration 75, Loss: 0.026667\n",
            "Epoch [6/10000], iteration 80, Loss: 0.027250\n",
            "Epoch [6/10000], iteration 85, Loss: 0.026824\n",
            "Epoch [6/10000], iteration 90, Loss: 0.026333\n",
            "Epoch [6/10000], iteration 95, Loss: 0.025579\n",
            "Epoch [6/10000], iteration 100, Loss: 0.024800\n",
            "validating....\n",
            "Epoch 6, validation loss is 0.0355, training loss is 0.0248\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 5\n",
            "Epoch [7/10000], iteration 5, Loss: 0.070000\n",
            "Epoch [7/10000], iteration 10, Loss: 0.054000\n",
            "Epoch [7/10000], iteration 15, Loss: 0.070000\n",
            "Epoch [7/10000], iteration 20, Loss: 0.057000\n",
            "Epoch [7/10000], iteration 25, Loss: 0.051200\n",
            "Epoch [7/10000], iteration 30, Loss: 0.047333\n",
            "Epoch [7/10000], iteration 35, Loss: 0.043714\n",
            "Epoch [7/10000], iteration 40, Loss: 0.039500\n",
            "Epoch [7/10000], iteration 45, Loss: 0.042444\n",
            "Epoch [7/10000], iteration 50, Loss: 0.041600\n",
            "Epoch [7/10000], iteration 55, Loss: 0.040727\n",
            "Epoch [7/10000], iteration 60, Loss: 0.040000\n",
            "Epoch [7/10000], iteration 65, Loss: 0.038154\n",
            "Epoch [7/10000], iteration 70, Loss: 0.036714\n",
            "Epoch [7/10000], iteration 75, Loss: 0.034933\n",
            "Epoch [7/10000], iteration 80, Loss: 0.034875\n",
            "Epoch [7/10000], iteration 85, Loss: 0.038118\n",
            "Epoch [7/10000], iteration 90, Loss: 0.041000\n",
            "Epoch [7/10000], iteration 95, Loss: 0.040632\n",
            "Epoch [7/10000], iteration 100, Loss: 0.043200\n",
            "validating....\n",
            "Epoch 7, validation loss is 0.0363, training loss is 0.0432\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 6\n",
            "Epoch [8/10000], iteration 5, Loss: 0.024000\n",
            "Epoch [8/10000], iteration 10, Loss: 0.016000\n",
            "Epoch [8/10000], iteration 15, Loss: 0.014667\n",
            "Epoch [8/10000], iteration 20, Loss: 0.018500\n",
            "Epoch [8/10000], iteration 25, Loss: 0.017200\n",
            "Epoch [8/10000], iteration 30, Loss: 0.019333\n",
            "Epoch [8/10000], iteration 35, Loss: 0.019429\n",
            "Epoch [8/10000], iteration 40, Loss: 0.028750\n",
            "Epoch [8/10000], iteration 45, Loss: 0.035333\n",
            "Epoch [8/10000], iteration 50, Loss: 0.034600\n",
            "Epoch [8/10000], iteration 55, Loss: 0.034545\n",
            "Epoch [8/10000], iteration 60, Loss: 0.033500\n",
            "Epoch [8/10000], iteration 65, Loss: 0.032000\n",
            "Epoch [8/10000], iteration 70, Loss: 0.030143\n",
            "Epoch [8/10000], iteration 75, Loss: 0.029200\n",
            "Epoch [8/10000], iteration 80, Loss: 0.028250\n",
            "Epoch [8/10000], iteration 85, Loss: 0.027294\n",
            "Epoch [8/10000], iteration 90, Loss: 0.026556\n",
            "Epoch [8/10000], iteration 95, Loss: 0.030105\n",
            "Epoch [8/10000], iteration 100, Loss: 0.029600\n",
            "validating....\n",
            "Epoch 8, validation loss is 0.0411, training loss is 0.0296\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 7\n",
            "Epoch [9/10000], iteration 5, Loss: 0.084000\n",
            "Epoch [9/10000], iteration 10, Loss: 0.057000\n",
            "Epoch [9/10000], iteration 15, Loss: 0.048000\n",
            "Epoch [9/10000], iteration 20, Loss: 0.041000\n",
            "Epoch [9/10000], iteration 25, Loss: 0.036400\n",
            "Epoch [9/10000], iteration 30, Loss: 0.034333\n",
            "Epoch [9/10000], iteration 35, Loss: 0.030571\n",
            "Epoch [9/10000], iteration 40, Loss: 0.029500\n",
            "Epoch [9/10000], iteration 45, Loss: 0.029556\n",
            "Epoch [9/10000], iteration 50, Loss: 0.028400\n",
            "Epoch [9/10000], iteration 55, Loss: 0.034182\n",
            "Epoch [9/10000], iteration 60, Loss: 0.033000\n",
            "Epoch [9/10000], iteration 65, Loss: 0.034154\n",
            "Epoch [9/10000], iteration 70, Loss: 0.033143\n",
            "Epoch [9/10000], iteration 75, Loss: 0.032533\n",
            "Epoch [9/10000], iteration 80, Loss: 0.037750\n",
            "Epoch [9/10000], iteration 85, Loss: 0.036471\n",
            "Epoch [9/10000], iteration 90, Loss: 0.036556\n",
            "Epoch [9/10000], iteration 95, Loss: 0.035474\n",
            "Epoch [9/10000], iteration 100, Loss: 0.034000\n",
            "validating....\n",
            "Epoch 9, validation loss is 0.0386, training loss is 0.0340\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 8\n",
            "Epoch [10/10000], iteration 5, Loss: 0.108000\n",
            "Epoch [10/10000], iteration 10, Loss: 0.066000\n",
            "Epoch [10/10000], iteration 15, Loss: 0.050667\n",
            "Epoch [10/10000], iteration 20, Loss: 0.042000\n",
            "Epoch [10/10000], iteration 25, Loss: 0.053600\n",
            "Epoch [10/10000], iteration 30, Loss: 0.050333\n",
            "Epoch [10/10000], iteration 35, Loss: 0.047143\n",
            "Epoch [10/10000], iteration 40, Loss: 0.047500\n",
            "Epoch [10/10000], iteration 45, Loss: 0.043778\n",
            "Epoch [10/10000], iteration 50, Loss: 0.040600\n",
            "Epoch [10/10000], iteration 55, Loss: 0.044182\n",
            "Epoch [10/10000], iteration 60, Loss: 0.042500\n",
            "Epoch [10/10000], iteration 65, Loss: 0.040923\n",
            "Epoch [10/10000], iteration 70, Loss: 0.041429\n",
            "Epoch [10/10000], iteration 75, Loss: 0.040933\n",
            "Epoch [10/10000], iteration 80, Loss: 0.044500\n",
            "Epoch [10/10000], iteration 85, Loss: 0.043529\n",
            "Epoch [10/10000], iteration 90, Loss: 0.042444\n",
            "Epoch [10/10000], iteration 95, Loss: 0.041684\n",
            "Epoch [10/10000], iteration 100, Loss: 0.040400\n",
            "validating....\n",
            "Epoch 10, validation loss is 0.0342, training loss is 0.0404\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 9\n",
            "Epoch [11/10000], iteration 5, Loss: 0.088000\n",
            "Epoch [11/10000], iteration 10, Loss: 0.057000\n",
            "Epoch [11/10000], iteration 15, Loss: 0.046667\n",
            "Epoch [11/10000], iteration 20, Loss: 0.044500\n",
            "Epoch [11/10000], iteration 25, Loss: 0.051600\n",
            "Epoch [11/10000], iteration 30, Loss: 0.046000\n",
            "Epoch [11/10000], iteration 35, Loss: 0.041714\n",
            "Epoch [11/10000], iteration 40, Loss: 0.038750\n",
            "Epoch [11/10000], iteration 45, Loss: 0.037556\n",
            "Epoch [11/10000], iteration 50, Loss: 0.035200\n",
            "Epoch [11/10000], iteration 55, Loss: 0.034545\n",
            "Epoch [11/10000], iteration 60, Loss: 0.033667\n",
            "Epoch [11/10000], iteration 65, Loss: 0.032308\n",
            "Epoch [11/10000], iteration 70, Loss: 0.030714\n",
            "Epoch [11/10000], iteration 75, Loss: 0.038267\n",
            "Epoch [11/10000], iteration 80, Loss: 0.036750\n",
            "Epoch [11/10000], iteration 85, Loss: 0.037176\n",
            "Epoch [11/10000], iteration 90, Loss: 0.036111\n",
            "Epoch [11/10000], iteration 95, Loss: 0.039368\n",
            "Epoch [11/10000], iteration 100, Loss: 0.038900\n",
            "validating....\n",
            "Epoch 11, validation loss is 0.0363, training loss is 0.0389\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 10\n",
            "Epoch [12/10000], iteration 5, Loss: 0.014000\n",
            "Epoch [12/10000], iteration 10, Loss: 0.019000\n",
            "Epoch [12/10000], iteration 15, Loss: 0.020667\n",
            "Epoch [12/10000], iteration 20, Loss: 0.021000\n",
            "Epoch [12/10000], iteration 25, Loss: 0.026000\n",
            "Epoch [12/10000], iteration 30, Loss: 0.024333\n",
            "Epoch [12/10000], iteration 35, Loss: 0.031714\n",
            "Epoch [12/10000], iteration 40, Loss: 0.032500\n",
            "Epoch [12/10000], iteration 45, Loss: 0.031556\n",
            "Epoch [12/10000], iteration 50, Loss: 0.039000\n",
            "Epoch [12/10000], iteration 55, Loss: 0.042909\n",
            "Epoch [12/10000], iteration 60, Loss: 0.040333\n",
            "Epoch [12/10000], iteration 65, Loss: 0.042923\n",
            "Epoch [12/10000], iteration 70, Loss: 0.046000\n",
            "Epoch [12/10000], iteration 75, Loss: 0.045200\n",
            "Epoch [12/10000], iteration 80, Loss: 0.044375\n",
            "Epoch [12/10000], iteration 85, Loss: 0.046824\n",
            "Epoch [12/10000], iteration 90, Loss: 0.048889\n",
            "Epoch [12/10000], iteration 95, Loss: 0.048105\n",
            "Epoch [12/10000], iteration 100, Loss: 0.046800\n",
            "validating....\n",
            "Epoch 12, validation loss is 0.0365, training loss is 0.0468\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 11\n",
            "Epoch [13/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [13/10000], iteration 10, Loss: 0.022000\n",
            "Epoch [13/10000], iteration 15, Loss: 0.017333\n",
            "Epoch [13/10000], iteration 20, Loss: 0.052000\n",
            "Epoch [13/10000], iteration 25, Loss: 0.048400\n",
            "Epoch [13/10000], iteration 30, Loss: 0.044333\n",
            "Epoch [13/10000], iteration 35, Loss: 0.039143\n",
            "Epoch [13/10000], iteration 40, Loss: 0.037750\n",
            "Epoch [13/10000], iteration 45, Loss: 0.038000\n",
            "Epoch [13/10000], iteration 50, Loss: 0.035600\n",
            "Epoch [13/10000], iteration 55, Loss: 0.033818\n",
            "Epoch [13/10000], iteration 60, Loss: 0.031500\n",
            "Epoch [13/10000], iteration 65, Loss: 0.030923\n",
            "Epoch [13/10000], iteration 70, Loss: 0.029571\n",
            "Epoch [13/10000], iteration 75, Loss: 0.029200\n",
            "Epoch [13/10000], iteration 80, Loss: 0.029625\n",
            "Epoch [13/10000], iteration 85, Loss: 0.030118\n",
            "Epoch [13/10000], iteration 90, Loss: 0.029667\n",
            "Epoch [13/10000], iteration 95, Loss: 0.031684\n",
            "Epoch [13/10000], iteration 100, Loss: 0.030700\n",
            "validating....\n",
            "Epoch 13, validation loss is 0.0378, training loss is 0.0307\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 12\n",
            "Epoch [14/10000], iteration 5, Loss: 0.028000\n",
            "Epoch [14/10000], iteration 10, Loss: 0.029000\n",
            "Epoch [14/10000], iteration 15, Loss: 0.023333\n",
            "Epoch [14/10000], iteration 20, Loss: 0.021500\n",
            "Epoch [14/10000], iteration 25, Loss: 0.020400\n",
            "Epoch [14/10000], iteration 30, Loss: 0.021000\n",
            "Epoch [14/10000], iteration 35, Loss: 0.021429\n",
            "Epoch [14/10000], iteration 40, Loss: 0.032500\n",
            "Epoch [14/10000], iteration 45, Loss: 0.029556\n",
            "Epoch [14/10000], iteration 50, Loss: 0.030000\n",
            "Epoch [14/10000], iteration 55, Loss: 0.029636\n",
            "Epoch [14/10000], iteration 60, Loss: 0.029000\n",
            "Epoch [14/10000], iteration 65, Loss: 0.028000\n",
            "Epoch [14/10000], iteration 70, Loss: 0.027000\n",
            "Epoch [14/10000], iteration 75, Loss: 0.027867\n",
            "Epoch [14/10000], iteration 80, Loss: 0.026250\n",
            "Epoch [14/10000], iteration 85, Loss: 0.025882\n",
            "Epoch [14/10000], iteration 90, Loss: 0.025000\n",
            "Epoch [14/10000], iteration 95, Loss: 0.024632\n",
            "Epoch [14/10000], iteration 100, Loss: 0.024400\n",
            "validating....\n",
            "Epoch 14, validation loss is 0.0402, training loss is 0.0244\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 13\n",
            "Epoch [15/10000], iteration 5, Loss: 0.022000\n",
            "Epoch [15/10000], iteration 10, Loss: 0.028000\n",
            "Epoch [15/10000], iteration 15, Loss: 0.028000\n",
            "Epoch [15/10000], iteration 20, Loss: 0.025500\n",
            "Epoch [15/10000], iteration 25, Loss: 0.023200\n",
            "Epoch [15/10000], iteration 30, Loss: 0.032667\n",
            "Epoch [15/10000], iteration 35, Loss: 0.029714\n",
            "Epoch [15/10000], iteration 40, Loss: 0.029750\n",
            "Epoch [15/10000], iteration 45, Loss: 0.028889\n",
            "Epoch [15/10000], iteration 50, Loss: 0.027800\n",
            "Epoch [15/10000], iteration 55, Loss: 0.028545\n",
            "Epoch [15/10000], iteration 60, Loss: 0.028667\n",
            "Epoch [15/10000], iteration 65, Loss: 0.027077\n",
            "Epoch [15/10000], iteration 70, Loss: 0.027286\n",
            "Epoch [15/10000], iteration 75, Loss: 0.026933\n",
            "Epoch [15/10000], iteration 80, Loss: 0.026500\n",
            "Epoch [15/10000], iteration 85, Loss: 0.026235\n",
            "Epoch [15/10000], iteration 90, Loss: 0.029778\n",
            "Epoch [15/10000], iteration 95, Loss: 0.029263\n",
            "Epoch [15/10000], iteration 100, Loss: 0.029100\n",
            "validating....\n",
            "Epoch 15, validation loss is 0.0401, training loss is 0.0291\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 14\n",
            "Epoch [16/10000], iteration 5, Loss: 0.030000\n",
            "Epoch [16/10000], iteration 10, Loss: 0.029000\n",
            "Epoch [16/10000], iteration 15, Loss: 0.025333\n",
            "Epoch [16/10000], iteration 20, Loss: 0.027000\n",
            "Epoch [16/10000], iteration 25, Loss: 0.028000\n",
            "Epoch [16/10000], iteration 30, Loss: 0.049333\n",
            "Epoch [16/10000], iteration 35, Loss: 0.046286\n",
            "Epoch [16/10000], iteration 40, Loss: 0.043000\n",
            "Epoch [16/10000], iteration 45, Loss: 0.042889\n",
            "Epoch [16/10000], iteration 50, Loss: 0.040400\n",
            "Epoch [16/10000], iteration 55, Loss: 0.038909\n",
            "Epoch [16/10000], iteration 60, Loss: 0.038333\n",
            "Epoch [16/10000], iteration 65, Loss: 0.036769\n",
            "Epoch [16/10000], iteration 70, Loss: 0.034857\n",
            "Epoch [16/10000], iteration 75, Loss: 0.033067\n",
            "Epoch [16/10000], iteration 80, Loss: 0.032000\n",
            "Epoch [16/10000], iteration 85, Loss: 0.030941\n",
            "Epoch [16/10000], iteration 90, Loss: 0.031222\n",
            "Epoch [16/10000], iteration 95, Loss: 0.030316\n",
            "Epoch [16/10000], iteration 100, Loss: 0.029500\n",
            "validating....\n",
            "Epoch 16, validation loss is 0.0365, training loss is 0.0295\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 15\n",
            "Epoch [17/10000], iteration 5, Loss: 0.082000\n",
            "Epoch [17/10000], iteration 10, Loss: 0.070000\n",
            "Epoch [17/10000], iteration 15, Loss: 0.052000\n",
            "Epoch [17/10000], iteration 20, Loss: 0.044500\n",
            "Epoch [17/10000], iteration 25, Loss: 0.048400\n",
            "Epoch [17/10000], iteration 30, Loss: 0.056000\n",
            "Epoch [17/10000], iteration 35, Loss: 0.049429\n",
            "Epoch [17/10000], iteration 40, Loss: 0.047000\n",
            "Epoch [17/10000], iteration 45, Loss: 0.045556\n",
            "Epoch [17/10000], iteration 50, Loss: 0.042600\n",
            "Epoch [17/10000], iteration 55, Loss: 0.040000\n",
            "Epoch [17/10000], iteration 60, Loss: 0.039500\n",
            "Epoch [17/10000], iteration 65, Loss: 0.037692\n",
            "Epoch [17/10000], iteration 70, Loss: 0.035857\n",
            "Epoch [17/10000], iteration 75, Loss: 0.034933\n",
            "Epoch [17/10000], iteration 80, Loss: 0.033750\n",
            "Epoch [17/10000], iteration 85, Loss: 0.032706\n",
            "Epoch [17/10000], iteration 90, Loss: 0.031889\n",
            "Epoch [17/10000], iteration 95, Loss: 0.031579\n",
            "Epoch [17/10000], iteration 100, Loss: 0.031000\n",
            "validating....\n",
            "Epoch 17, validation loss is 0.0412, training loss is 0.0310\n",
            "Validation loss does not decrease from 0.0300, num_epoch_no_improvement 16\n",
            "Epoch [18/10000], iteration 5, Loss: 0.084000\n",
            "Epoch [18/10000], iteration 10, Loss: 0.047000\n",
            "Epoch [18/10000], iteration 15, Loss: 0.036667\n",
            "Epoch [18/10000], iteration 20, Loss: 0.030500\n",
            "Epoch [18/10000], iteration 25, Loss: 0.027200\n",
            "Epoch [18/10000], iteration 30, Loss: 0.024667\n",
            "Epoch [18/10000], iteration 35, Loss: 0.022571\n",
            "Epoch [18/10000], iteration 40, Loss: 0.021250\n",
            "Epoch [18/10000], iteration 45, Loss: 0.021333\n",
            "Epoch [18/10000], iteration 50, Loss: 0.023800\n",
            "Epoch [18/10000], iteration 55, Loss: 0.022727\n",
            "Epoch [18/10000], iteration 60, Loss: 0.028500\n",
            "Epoch [18/10000], iteration 65, Loss: 0.027231\n",
            "Epoch [18/10000], iteration 70, Loss: 0.026714\n",
            "Epoch [18/10000], iteration 75, Loss: 0.026800\n",
            "Epoch [18/10000], iteration 80, Loss: 0.025875\n",
            "Epoch [18/10000], iteration 85, Loss: 0.026824\n",
            "Epoch [18/10000], iteration 90, Loss: 0.026444\n",
            "Epoch [18/10000], iteration 95, Loss: 0.030632\n",
            "Epoch [18/10000], iteration 100, Loss: 0.030500\n",
            "validating....\n",
            "Epoch 18, validation loss is 0.0286, training loss is 0.0305\n",
            "Validation loss decreases from 0.0300 to 0.0286\n",
            "Saving model  /content/drive/MyDrive/Spring_research_2023/SSLModel/Reuslts/pretrained_weights/2023-04-22_20-58-04/ISIC_Unsup.pt\n",
            "Epoch [19/10000], iteration 5, Loss: 0.004000\n",
            "Epoch [19/10000], iteration 10, Loss: 0.014000\n",
            "Epoch [19/10000], iteration 15, Loss: 0.015333\n",
            "Epoch [19/10000], iteration 20, Loss: 0.017000\n",
            "Epoch [19/10000], iteration 25, Loss: 0.018000\n",
            "Epoch [19/10000], iteration 30, Loss: 0.019333\n",
            "Epoch [19/10000], iteration 35, Loss: 0.018286\n",
            "Epoch [19/10000], iteration 40, Loss: 0.019250\n",
            "Epoch [19/10000], iteration 45, Loss: 0.018667\n",
            "Epoch [19/10000], iteration 50, Loss: 0.019200\n",
            "Epoch [19/10000], iteration 55, Loss: 0.020000\n",
            "Epoch [19/10000], iteration 60, Loss: 0.020333\n",
            "Epoch [19/10000], iteration 65, Loss: 0.020000\n",
            "Epoch [19/10000], iteration 70, Loss: 0.019857\n",
            "Epoch [19/10000], iteration 75, Loss: 0.020533\n",
            "Epoch [19/10000], iteration 80, Loss: 0.020375\n",
            "Epoch [19/10000], iteration 85, Loss: 0.020235\n",
            "Epoch [19/10000], iteration 90, Loss: 0.020778\n",
            "Epoch [19/10000], iteration 95, Loss: 0.020105\n",
            "Epoch [19/10000], iteration 100, Loss: 0.019800\n",
            "validating....\n",
            "Epoch 19, validation loss is 0.0338, training loss is 0.0198\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 0\n",
            "Epoch [20/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [20/10000], iteration 10, Loss: 0.050000\n",
            "Epoch [20/10000], iteration 15, Loss: 0.040667\n",
            "Epoch [20/10000], iteration 20, Loss: 0.032000\n",
            "Epoch [20/10000], iteration 25, Loss: 0.029600\n",
            "Epoch [20/10000], iteration 30, Loss: 0.027333\n",
            "Epoch [20/10000], iteration 35, Loss: 0.038000\n",
            "Epoch [20/10000], iteration 40, Loss: 0.034250\n",
            "Epoch [20/10000], iteration 45, Loss: 0.033556\n",
            "Epoch [20/10000], iteration 50, Loss: 0.031600\n",
            "Epoch [20/10000], iteration 55, Loss: 0.037091\n",
            "Epoch [20/10000], iteration 60, Loss: 0.037333\n",
            "Epoch [20/10000], iteration 65, Loss: 0.036308\n",
            "Epoch [20/10000], iteration 70, Loss: 0.035857\n",
            "Epoch [20/10000], iteration 75, Loss: 0.035200\n",
            "Epoch [20/10000], iteration 80, Loss: 0.033250\n",
            "Epoch [20/10000], iteration 85, Loss: 0.032000\n",
            "Epoch [20/10000], iteration 90, Loss: 0.030778\n",
            "Epoch [20/10000], iteration 95, Loss: 0.031053\n",
            "Epoch [20/10000], iteration 100, Loss: 0.031200\n",
            "validating....\n",
            "Epoch 20, validation loss is 0.0341, training loss is 0.0312\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 1\n",
            "Epoch [21/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [21/10000], iteration 10, Loss: 0.032000\n",
            "Epoch [21/10000], iteration 15, Loss: 0.025333\n",
            "Epoch [21/10000], iteration 20, Loss: 0.029500\n",
            "Epoch [21/10000], iteration 25, Loss: 0.054000\n",
            "Epoch [21/10000], iteration 30, Loss: 0.061667\n",
            "Epoch [21/10000], iteration 35, Loss: 0.056857\n",
            "Epoch [21/10000], iteration 40, Loss: 0.051750\n",
            "Epoch [21/10000], iteration 45, Loss: 0.050444\n",
            "Epoch [21/10000], iteration 50, Loss: 0.048200\n",
            "Epoch [21/10000], iteration 55, Loss: 0.045636\n",
            "Epoch [21/10000], iteration 60, Loss: 0.049167\n",
            "Epoch [21/10000], iteration 65, Loss: 0.047231\n",
            "Epoch [21/10000], iteration 70, Loss: 0.044857\n",
            "Epoch [21/10000], iteration 75, Loss: 0.042533\n",
            "Epoch [21/10000], iteration 80, Loss: 0.040250\n",
            "Epoch [21/10000], iteration 85, Loss: 0.039882\n",
            "Epoch [21/10000], iteration 90, Loss: 0.039556\n",
            "Epoch [21/10000], iteration 95, Loss: 0.038632\n",
            "Epoch [21/10000], iteration 100, Loss: 0.038500\n",
            "validating....\n",
            "Epoch 21, validation loss is 0.0409, training loss is 0.0385\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 2\n",
            "Epoch [22/10000], iteration 5, Loss: 0.032000\n",
            "Epoch [22/10000], iteration 10, Loss: 0.025000\n",
            "Epoch [22/10000], iteration 15, Loss: 0.024000\n",
            "Epoch [22/10000], iteration 20, Loss: 0.022000\n",
            "Epoch [22/10000], iteration 25, Loss: 0.021200\n",
            "Epoch [22/10000], iteration 30, Loss: 0.019333\n",
            "Epoch [22/10000], iteration 35, Loss: 0.020571\n",
            "Epoch [22/10000], iteration 40, Loss: 0.019000\n",
            "Epoch [22/10000], iteration 45, Loss: 0.020444\n",
            "Epoch [22/10000], iteration 50, Loss: 0.029000\n",
            "Epoch [22/10000], iteration 55, Loss: 0.028182\n",
            "Epoch [22/10000], iteration 60, Loss: 0.033333\n",
            "Epoch [22/10000], iteration 65, Loss: 0.036923\n",
            "Epoch [22/10000], iteration 70, Loss: 0.035714\n",
            "Epoch [22/10000], iteration 75, Loss: 0.039333\n",
            "Epoch [22/10000], iteration 80, Loss: 0.037375\n",
            "Epoch [22/10000], iteration 85, Loss: 0.035647\n",
            "Epoch [22/10000], iteration 90, Loss: 0.035556\n",
            "Epoch [22/10000], iteration 95, Loss: 0.034947\n",
            "Epoch [22/10000], iteration 100, Loss: 0.037800\n",
            "validating....\n",
            "Epoch 22, validation loss is 0.0439, training loss is 0.0378\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 3\n",
            "Epoch [23/10000], iteration 5, Loss: 0.042000\n",
            "Epoch [23/10000], iteration 10, Loss: 0.045000\n",
            "Epoch [23/10000], iteration 15, Loss: 0.035333\n",
            "Epoch [23/10000], iteration 20, Loss: 0.031500\n",
            "Epoch [23/10000], iteration 25, Loss: 0.033200\n",
            "Epoch [23/10000], iteration 30, Loss: 0.029000\n",
            "Epoch [23/10000], iteration 35, Loss: 0.027429\n",
            "Epoch [23/10000], iteration 40, Loss: 0.027750\n",
            "Epoch [23/10000], iteration 45, Loss: 0.026222\n",
            "Epoch [23/10000], iteration 50, Loss: 0.025200\n",
            "Epoch [23/10000], iteration 55, Loss: 0.025273\n",
            "Epoch [23/10000], iteration 60, Loss: 0.025667\n",
            "Epoch [23/10000], iteration 65, Loss: 0.026154\n",
            "Epoch [23/10000], iteration 70, Loss: 0.025143\n",
            "Epoch [23/10000], iteration 75, Loss: 0.025600\n",
            "Epoch [23/10000], iteration 80, Loss: 0.025750\n",
            "Epoch [23/10000], iteration 85, Loss: 0.025412\n",
            "Epoch [23/10000], iteration 90, Loss: 0.029111\n",
            "Epoch [23/10000], iteration 95, Loss: 0.029053\n",
            "Epoch [23/10000], iteration 100, Loss: 0.030800\n",
            "validating....\n",
            "Epoch 23, validation loss is 0.0357, training loss is 0.0308\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 4\n",
            "Epoch [24/10000], iteration 5, Loss: 0.012000\n",
            "Epoch [24/10000], iteration 10, Loss: 0.013000\n",
            "Epoch [24/10000], iteration 15, Loss: 0.014667\n",
            "Epoch [24/10000], iteration 20, Loss: 0.021000\n",
            "Epoch [24/10000], iteration 25, Loss: 0.018400\n",
            "Epoch [24/10000], iteration 30, Loss: 0.016667\n",
            "Epoch [24/10000], iteration 35, Loss: 0.017143\n",
            "Epoch [24/10000], iteration 40, Loss: 0.016250\n",
            "Epoch [24/10000], iteration 45, Loss: 0.022889\n",
            "Epoch [24/10000], iteration 50, Loss: 0.022200\n",
            "Epoch [24/10000], iteration 55, Loss: 0.021455\n",
            "Epoch [24/10000], iteration 60, Loss: 0.021667\n",
            "Epoch [24/10000], iteration 65, Loss: 0.020615\n",
            "Epoch [24/10000], iteration 70, Loss: 0.026286\n",
            "Epoch [24/10000], iteration 75, Loss: 0.025733\n",
            "Epoch [24/10000], iteration 80, Loss: 0.024750\n",
            "Epoch [24/10000], iteration 85, Loss: 0.024706\n",
            "Epoch [24/10000], iteration 90, Loss: 0.024556\n",
            "Epoch [24/10000], iteration 95, Loss: 0.024000\n",
            "Epoch [24/10000], iteration 100, Loss: 0.023800\n",
            "validating....\n",
            "Epoch 24, validation loss is 0.0391, training loss is 0.0238\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 5\n",
            "Epoch [25/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [25/10000], iteration 10, Loss: 0.019000\n",
            "Epoch [25/10000], iteration 15, Loss: 0.018667\n",
            "Epoch [25/10000], iteration 20, Loss: 0.022000\n",
            "Epoch [25/10000], iteration 25, Loss: 0.025200\n",
            "Epoch [25/10000], iteration 30, Loss: 0.026000\n",
            "Epoch [25/10000], iteration 35, Loss: 0.026000\n",
            "Epoch [25/10000], iteration 40, Loss: 0.023500\n",
            "Epoch [25/10000], iteration 45, Loss: 0.022444\n",
            "Epoch [25/10000], iteration 50, Loss: 0.024600\n",
            "Epoch [25/10000], iteration 55, Loss: 0.023636\n",
            "Epoch [25/10000], iteration 60, Loss: 0.023833\n",
            "Epoch [25/10000], iteration 65, Loss: 0.023846\n",
            "Epoch [25/10000], iteration 70, Loss: 0.023429\n",
            "Epoch [25/10000], iteration 75, Loss: 0.023867\n",
            "Epoch [25/10000], iteration 80, Loss: 0.027875\n",
            "Epoch [25/10000], iteration 85, Loss: 0.032000\n",
            "Epoch [25/10000], iteration 90, Loss: 0.030778\n",
            "Epoch [25/10000], iteration 95, Loss: 0.030421\n",
            "Epoch [25/10000], iteration 100, Loss: 0.029600\n",
            "validating....\n",
            "Epoch 25, validation loss is 0.0351, training loss is 0.0296\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 6\n",
            "Epoch [26/10000], iteration 5, Loss: 0.036000\n",
            "Epoch [26/10000], iteration 10, Loss: 0.027000\n",
            "Epoch [26/10000], iteration 15, Loss: 0.025333\n",
            "Epoch [26/10000], iteration 20, Loss: 0.027500\n",
            "Epoch [26/10000], iteration 25, Loss: 0.027200\n",
            "Epoch [26/10000], iteration 30, Loss: 0.026333\n",
            "Epoch [26/10000], iteration 35, Loss: 0.024000\n",
            "Epoch [26/10000], iteration 40, Loss: 0.024750\n",
            "Epoch [26/10000], iteration 45, Loss: 0.032222\n",
            "Epoch [26/10000], iteration 50, Loss: 0.032600\n",
            "Epoch [26/10000], iteration 55, Loss: 0.030909\n",
            "Epoch [26/10000], iteration 60, Loss: 0.035667\n",
            "Epoch [26/10000], iteration 65, Loss: 0.034154\n",
            "Epoch [26/10000], iteration 70, Loss: 0.033000\n",
            "Epoch [26/10000], iteration 75, Loss: 0.035067\n",
            "Epoch [26/10000], iteration 80, Loss: 0.033500\n",
            "Epoch [26/10000], iteration 85, Loss: 0.032118\n",
            "Epoch [26/10000], iteration 90, Loss: 0.031111\n",
            "Epoch [26/10000], iteration 95, Loss: 0.031158\n",
            "Epoch [26/10000], iteration 100, Loss: 0.030200\n",
            "validating....\n",
            "Epoch 26, validation loss is 0.0460, training loss is 0.0302\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 7\n",
            "Epoch [27/10000], iteration 5, Loss: 0.024000\n",
            "Epoch [27/10000], iteration 10, Loss: 0.018000\n",
            "Epoch [27/10000], iteration 15, Loss: 0.019333\n",
            "Epoch [27/10000], iteration 20, Loss: 0.037000\n",
            "Epoch [27/10000], iteration 25, Loss: 0.046800\n",
            "Epoch [27/10000], iteration 30, Loss: 0.044333\n",
            "Epoch [27/10000], iteration 35, Loss: 0.042000\n",
            "Epoch [27/10000], iteration 40, Loss: 0.040500\n",
            "Epoch [27/10000], iteration 45, Loss: 0.040000\n",
            "Epoch [27/10000], iteration 50, Loss: 0.039000\n",
            "Epoch [27/10000], iteration 55, Loss: 0.036000\n",
            "Epoch [27/10000], iteration 60, Loss: 0.033833\n",
            "Epoch [27/10000], iteration 65, Loss: 0.033538\n",
            "Epoch [27/10000], iteration 70, Loss: 0.032143\n",
            "Epoch [27/10000], iteration 75, Loss: 0.031067\n",
            "Epoch [27/10000], iteration 80, Loss: 0.030250\n",
            "Epoch [27/10000], iteration 85, Loss: 0.029176\n",
            "Epoch [27/10000], iteration 90, Loss: 0.029444\n",
            "Epoch [27/10000], iteration 95, Loss: 0.029474\n",
            "Epoch [27/10000], iteration 100, Loss: 0.029200\n",
            "validating....\n",
            "Epoch 27, validation loss is 0.0397, training loss is 0.0292\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 8\n",
            "Epoch [28/10000], iteration 5, Loss: 0.028000\n",
            "Epoch [28/10000], iteration 10, Loss: 0.024000\n",
            "Epoch [28/10000], iteration 15, Loss: 0.022000\n",
            "Epoch [28/10000], iteration 20, Loss: 0.024000\n",
            "Epoch [28/10000], iteration 25, Loss: 0.026000\n",
            "Epoch [28/10000], iteration 30, Loss: 0.024667\n",
            "Epoch [28/10000], iteration 35, Loss: 0.034571\n",
            "Epoch [28/10000], iteration 40, Loss: 0.038250\n",
            "Epoch [28/10000], iteration 45, Loss: 0.035556\n",
            "Epoch [28/10000], iteration 50, Loss: 0.033200\n",
            "Epoch [28/10000], iteration 55, Loss: 0.031091\n",
            "Epoch [28/10000], iteration 60, Loss: 0.032000\n",
            "Epoch [28/10000], iteration 65, Loss: 0.030462\n",
            "Epoch [28/10000], iteration 70, Loss: 0.029429\n",
            "Epoch [28/10000], iteration 75, Loss: 0.029333\n",
            "Epoch [28/10000], iteration 80, Loss: 0.028375\n",
            "Epoch [28/10000], iteration 85, Loss: 0.028000\n",
            "Epoch [28/10000], iteration 90, Loss: 0.028667\n",
            "Epoch [28/10000], iteration 95, Loss: 0.028211\n",
            "Epoch [28/10000], iteration 100, Loss: 0.027400\n",
            "validating....\n",
            "Epoch 28, validation loss is 0.0406, training loss is 0.0274\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 9\n",
            "Epoch [29/10000], iteration 5, Loss: 0.094000\n",
            "Epoch [29/10000], iteration 10, Loss: 0.052000\n",
            "Epoch [29/10000], iteration 15, Loss: 0.038667\n",
            "Epoch [29/10000], iteration 20, Loss: 0.032500\n",
            "Epoch [29/10000], iteration 25, Loss: 0.032800\n",
            "Epoch [29/10000], iteration 30, Loss: 0.031667\n",
            "Epoch [29/10000], iteration 35, Loss: 0.040857\n",
            "Epoch [29/10000], iteration 40, Loss: 0.038250\n",
            "Epoch [29/10000], iteration 45, Loss: 0.034667\n",
            "Epoch [29/10000], iteration 50, Loss: 0.032800\n",
            "Epoch [29/10000], iteration 55, Loss: 0.030909\n",
            "Epoch [29/10000], iteration 60, Loss: 0.030000\n",
            "Epoch [29/10000], iteration 65, Loss: 0.028615\n",
            "Epoch [29/10000], iteration 70, Loss: 0.028000\n",
            "Epoch [29/10000], iteration 75, Loss: 0.027867\n",
            "Epoch [29/10000], iteration 80, Loss: 0.026375\n",
            "Epoch [29/10000], iteration 85, Loss: 0.026706\n",
            "Epoch [29/10000], iteration 90, Loss: 0.026667\n",
            "Epoch [29/10000], iteration 95, Loss: 0.025474\n",
            "Epoch [29/10000], iteration 100, Loss: 0.028700\n",
            "validating....\n",
            "Epoch 29, validation loss is 0.0287, training loss is 0.0287\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 10\n",
            "Epoch [30/10000], iteration 5, Loss: 0.036000\n",
            "Epoch [30/10000], iteration 10, Loss: 0.050000\n",
            "Epoch [30/10000], iteration 15, Loss: 0.043333\n",
            "Epoch [30/10000], iteration 20, Loss: 0.035000\n",
            "Epoch [30/10000], iteration 25, Loss: 0.033600\n",
            "Epoch [30/10000], iteration 30, Loss: 0.031667\n",
            "Epoch [30/10000], iteration 35, Loss: 0.029143\n",
            "Epoch [30/10000], iteration 40, Loss: 0.029000\n",
            "Epoch [30/10000], iteration 45, Loss: 0.029556\n",
            "Epoch [30/10000], iteration 50, Loss: 0.028400\n",
            "Epoch [30/10000], iteration 55, Loss: 0.027273\n",
            "Epoch [30/10000], iteration 60, Loss: 0.029333\n",
            "Epoch [30/10000], iteration 65, Loss: 0.029385\n",
            "Epoch [30/10000], iteration 70, Loss: 0.028000\n",
            "Epoch [30/10000], iteration 75, Loss: 0.032000\n",
            "Epoch [30/10000], iteration 80, Loss: 0.031750\n",
            "Epoch [30/10000], iteration 85, Loss: 0.031647\n",
            "Epoch [30/10000], iteration 90, Loss: 0.030667\n",
            "Epoch [30/10000], iteration 95, Loss: 0.034316\n",
            "Epoch [30/10000], iteration 100, Loss: 0.034000\n",
            "validating....\n",
            "Epoch 30, validation loss is 0.0387, training loss is 0.0340\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 11\n",
            "Epoch [31/10000], iteration 5, Loss: 0.088000\n",
            "Epoch [31/10000], iteration 10, Loss: 0.064000\n",
            "Epoch [31/10000], iteration 15, Loss: 0.051333\n",
            "Epoch [31/10000], iteration 20, Loss: 0.041500\n",
            "Epoch [31/10000], iteration 25, Loss: 0.044800\n",
            "Epoch [31/10000], iteration 30, Loss: 0.039000\n",
            "Epoch [31/10000], iteration 35, Loss: 0.037143\n",
            "Epoch [31/10000], iteration 40, Loss: 0.040750\n",
            "Epoch [31/10000], iteration 45, Loss: 0.039333\n",
            "Epoch [31/10000], iteration 50, Loss: 0.040600\n",
            "Epoch [31/10000], iteration 55, Loss: 0.038364\n",
            "Epoch [31/10000], iteration 60, Loss: 0.036500\n",
            "Epoch [31/10000], iteration 65, Loss: 0.036769\n",
            "Epoch [31/10000], iteration 70, Loss: 0.035286\n",
            "Epoch [31/10000], iteration 75, Loss: 0.040133\n",
            "Epoch [31/10000], iteration 80, Loss: 0.039625\n",
            "Epoch [31/10000], iteration 85, Loss: 0.038471\n",
            "Epoch [31/10000], iteration 90, Loss: 0.036889\n",
            "Epoch [31/10000], iteration 95, Loss: 0.039579\n",
            "Epoch [31/10000], iteration 100, Loss: 0.038700\n",
            "validating....\n",
            "Epoch 31, validation loss is 0.0391, training loss is 0.0387\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 12\n",
            "Epoch [32/10000], iteration 5, Loss: 0.076000\n",
            "Epoch [32/10000], iteration 10, Loss: 0.065000\n",
            "Epoch [32/10000], iteration 15, Loss: 0.046000\n",
            "Epoch [32/10000], iteration 20, Loss: 0.058000\n",
            "Epoch [32/10000], iteration 25, Loss: 0.052000\n",
            "Epoch [32/10000], iteration 30, Loss: 0.046333\n",
            "Epoch [32/10000], iteration 35, Loss: 0.044571\n",
            "Epoch [32/10000], iteration 40, Loss: 0.041500\n",
            "Epoch [32/10000], iteration 45, Loss: 0.040444\n",
            "Epoch [32/10000], iteration 50, Loss: 0.037800\n",
            "Epoch [32/10000], iteration 55, Loss: 0.037636\n",
            "Epoch [32/10000], iteration 60, Loss: 0.036667\n",
            "Epoch [32/10000], iteration 65, Loss: 0.036000\n",
            "Epoch [32/10000], iteration 70, Loss: 0.036000\n",
            "Epoch [32/10000], iteration 75, Loss: 0.035333\n",
            "Epoch [32/10000], iteration 80, Loss: 0.034375\n",
            "Epoch [32/10000], iteration 85, Loss: 0.033647\n",
            "Epoch [32/10000], iteration 90, Loss: 0.033444\n",
            "Epoch [32/10000], iteration 95, Loss: 0.032211\n",
            "Epoch [32/10000], iteration 100, Loss: 0.032500\n",
            "validating....\n",
            "Epoch 32, validation loss is 0.0414, training loss is 0.0325\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 13\n",
            "Epoch [33/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [33/10000], iteration 10, Loss: 0.040000\n",
            "Epoch [33/10000], iteration 15, Loss: 0.056667\n",
            "Epoch [33/10000], iteration 20, Loss: 0.047000\n",
            "Epoch [33/10000], iteration 25, Loss: 0.044000\n",
            "Epoch [33/10000], iteration 30, Loss: 0.040667\n",
            "Epoch [33/10000], iteration 35, Loss: 0.036857\n",
            "Epoch [33/10000], iteration 40, Loss: 0.034500\n",
            "Epoch [33/10000], iteration 45, Loss: 0.035111\n",
            "Epoch [33/10000], iteration 50, Loss: 0.040600\n",
            "Epoch [33/10000], iteration 55, Loss: 0.039273\n",
            "Epoch [33/10000], iteration 60, Loss: 0.037500\n",
            "Epoch [33/10000], iteration 65, Loss: 0.037692\n",
            "Epoch [33/10000], iteration 70, Loss: 0.037286\n",
            "Epoch [33/10000], iteration 75, Loss: 0.036267\n",
            "Epoch [33/10000], iteration 80, Loss: 0.036000\n",
            "Epoch [33/10000], iteration 85, Loss: 0.034588\n",
            "Epoch [33/10000], iteration 90, Loss: 0.033556\n",
            "Epoch [33/10000], iteration 95, Loss: 0.033368\n",
            "Epoch [33/10000], iteration 100, Loss: 0.036000\n",
            "validating....\n",
            "Epoch 33, validation loss is 0.0378, training loss is 0.0360\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 14\n",
            "Epoch [34/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [34/10000], iteration 10, Loss: 0.014000\n",
            "Epoch [34/10000], iteration 15, Loss: 0.042000\n",
            "Epoch [34/10000], iteration 20, Loss: 0.035500\n",
            "Epoch [34/10000], iteration 25, Loss: 0.029600\n",
            "Epoch [34/10000], iteration 30, Loss: 0.040000\n",
            "Epoch [34/10000], iteration 35, Loss: 0.035429\n",
            "Epoch [34/10000], iteration 40, Loss: 0.032500\n",
            "Epoch [34/10000], iteration 45, Loss: 0.030000\n",
            "Epoch [34/10000], iteration 50, Loss: 0.029000\n",
            "Epoch [34/10000], iteration 55, Loss: 0.035636\n",
            "Epoch [34/10000], iteration 60, Loss: 0.035500\n",
            "Epoch [34/10000], iteration 65, Loss: 0.034615\n",
            "Epoch [34/10000], iteration 70, Loss: 0.034143\n",
            "Epoch [34/10000], iteration 75, Loss: 0.033067\n",
            "Epoch [34/10000], iteration 80, Loss: 0.033000\n",
            "Epoch [34/10000], iteration 85, Loss: 0.032353\n",
            "Epoch [34/10000], iteration 90, Loss: 0.032667\n",
            "Epoch [34/10000], iteration 95, Loss: 0.031053\n",
            "Epoch [34/10000], iteration 100, Loss: 0.033500\n",
            "validating....\n",
            "Epoch 34, validation loss is 0.0397, training loss is 0.0335\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 15\n",
            "Epoch [35/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [35/10000], iteration 10, Loss: 0.019000\n",
            "Epoch [35/10000], iteration 15, Loss: 0.022000\n",
            "Epoch [35/10000], iteration 20, Loss: 0.025500\n",
            "Epoch [35/10000], iteration 25, Loss: 0.023200\n",
            "Epoch [35/10000], iteration 30, Loss: 0.022333\n",
            "Epoch [35/10000], iteration 35, Loss: 0.022571\n",
            "Epoch [35/10000], iteration 40, Loss: 0.021000\n",
            "Epoch [35/10000], iteration 45, Loss: 0.020889\n",
            "Epoch [35/10000], iteration 50, Loss: 0.021400\n",
            "Epoch [35/10000], iteration 55, Loss: 0.020727\n",
            "Epoch [35/10000], iteration 60, Loss: 0.021167\n",
            "Epoch [35/10000], iteration 65, Loss: 0.020615\n",
            "Epoch [35/10000], iteration 70, Loss: 0.026000\n",
            "Epoch [35/10000], iteration 75, Loss: 0.025733\n",
            "Epoch [35/10000], iteration 80, Loss: 0.025375\n",
            "Epoch [35/10000], iteration 85, Loss: 0.024824\n",
            "Epoch [35/10000], iteration 90, Loss: 0.028444\n",
            "Epoch [35/10000], iteration 95, Loss: 0.031158\n",
            "Epoch [35/10000], iteration 100, Loss: 0.030400\n",
            "validating....\n",
            "Epoch 35, validation loss is 0.0377, training loss is 0.0304\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 16\n",
            "Epoch [36/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [36/10000], iteration 10, Loss: 0.024000\n",
            "Epoch [36/10000], iteration 15, Loss: 0.025333\n",
            "Epoch [36/10000], iteration 20, Loss: 0.025500\n",
            "Epoch [36/10000], iteration 25, Loss: 0.025200\n",
            "Epoch [36/10000], iteration 30, Loss: 0.023333\n",
            "Epoch [36/10000], iteration 35, Loss: 0.021714\n",
            "Epoch [36/10000], iteration 40, Loss: 0.023000\n",
            "Epoch [36/10000], iteration 45, Loss: 0.030444\n",
            "Epoch [36/10000], iteration 50, Loss: 0.036400\n",
            "Epoch [36/10000], iteration 55, Loss: 0.035636\n",
            "Epoch [36/10000], iteration 60, Loss: 0.041167\n",
            "Epoch [36/10000], iteration 65, Loss: 0.040462\n",
            "Epoch [36/10000], iteration 70, Loss: 0.038857\n",
            "Epoch [36/10000], iteration 75, Loss: 0.037200\n",
            "Epoch [36/10000], iteration 80, Loss: 0.036500\n",
            "Epoch [36/10000], iteration 85, Loss: 0.035176\n",
            "Epoch [36/10000], iteration 90, Loss: 0.034222\n",
            "Epoch [36/10000], iteration 95, Loss: 0.033579\n",
            "Epoch [36/10000], iteration 100, Loss: 0.032400\n",
            "validating....\n",
            "Epoch 36, validation loss is 0.0361, training loss is 0.0324\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 17\n",
            "Epoch [37/10000], iteration 5, Loss: 0.014000\n",
            "Epoch [37/10000], iteration 10, Loss: 0.018000\n",
            "Epoch [37/10000], iteration 15, Loss: 0.017333\n",
            "Epoch [37/10000], iteration 20, Loss: 0.015500\n",
            "Epoch [37/10000], iteration 25, Loss: 0.015600\n",
            "Epoch [37/10000], iteration 30, Loss: 0.017333\n",
            "Epoch [37/10000], iteration 35, Loss: 0.020571\n",
            "Epoch [37/10000], iteration 40, Loss: 0.019500\n",
            "Epoch [37/10000], iteration 45, Loss: 0.019556\n",
            "Epoch [37/10000], iteration 50, Loss: 0.027600\n",
            "Epoch [37/10000], iteration 55, Loss: 0.026909\n",
            "Epoch [37/10000], iteration 60, Loss: 0.027833\n",
            "Epoch [37/10000], iteration 65, Loss: 0.026462\n",
            "Epoch [37/10000], iteration 70, Loss: 0.025571\n",
            "Epoch [37/10000], iteration 75, Loss: 0.026933\n",
            "Epoch [37/10000], iteration 80, Loss: 0.026125\n",
            "Epoch [37/10000], iteration 85, Loss: 0.025882\n",
            "Epoch [37/10000], iteration 90, Loss: 0.025556\n",
            "Epoch [37/10000], iteration 95, Loss: 0.026105\n",
            "Epoch [37/10000], iteration 100, Loss: 0.025600\n",
            "validating....\n",
            "Epoch 37, validation loss is 0.0401, training loss is 0.0256\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 18\n",
            "Epoch [38/10000], iteration 5, Loss: 0.112000\n",
            "Epoch [38/10000], iteration 10, Loss: 0.099000\n",
            "Epoch [38/10000], iteration 15, Loss: 0.075333\n",
            "Epoch [38/10000], iteration 20, Loss: 0.080000\n",
            "Epoch [38/10000], iteration 25, Loss: 0.068000\n",
            "Epoch [38/10000], iteration 30, Loss: 0.059667\n",
            "Epoch [38/10000], iteration 35, Loss: 0.056000\n",
            "Epoch [38/10000], iteration 40, Loss: 0.060750\n",
            "Epoch [38/10000], iteration 45, Loss: 0.055111\n",
            "Epoch [38/10000], iteration 50, Loss: 0.054600\n",
            "Epoch [38/10000], iteration 55, Loss: 0.050364\n",
            "Epoch [38/10000], iteration 60, Loss: 0.047500\n",
            "Epoch [38/10000], iteration 65, Loss: 0.046154\n",
            "Epoch [38/10000], iteration 70, Loss: 0.045143\n",
            "Epoch [38/10000], iteration 75, Loss: 0.042933\n",
            "Epoch [38/10000], iteration 80, Loss: 0.050250\n",
            "Epoch [38/10000], iteration 85, Loss: 0.049176\n",
            "Epoch [38/10000], iteration 90, Loss: 0.047667\n",
            "Epoch [38/10000], iteration 95, Loss: 0.047474\n",
            "Epoch [38/10000], iteration 100, Loss: 0.053000\n",
            "validating....\n",
            "Epoch 38, validation loss is 0.0372, training loss is 0.0530\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 19\n",
            "Epoch [39/10000], iteration 5, Loss: 0.048000\n",
            "Epoch [39/10000], iteration 10, Loss: 0.065000\n",
            "Epoch [39/10000], iteration 15, Loss: 0.050667\n",
            "Epoch [39/10000], iteration 20, Loss: 0.045500\n",
            "Epoch [39/10000], iteration 25, Loss: 0.042400\n",
            "Epoch [39/10000], iteration 30, Loss: 0.038667\n",
            "Epoch [39/10000], iteration 35, Loss: 0.034571\n",
            "Epoch [39/10000], iteration 40, Loss: 0.033000\n",
            "Epoch [39/10000], iteration 45, Loss: 0.030667\n",
            "Epoch [39/10000], iteration 50, Loss: 0.028800\n",
            "Epoch [39/10000], iteration 55, Loss: 0.030727\n",
            "Epoch [39/10000], iteration 60, Loss: 0.028833\n",
            "Epoch [39/10000], iteration 65, Loss: 0.027846\n",
            "Epoch [39/10000], iteration 70, Loss: 0.028143\n",
            "Epoch [39/10000], iteration 75, Loss: 0.028800\n",
            "Epoch [39/10000], iteration 80, Loss: 0.033000\n",
            "Epoch [39/10000], iteration 85, Loss: 0.037294\n",
            "Epoch [39/10000], iteration 90, Loss: 0.044000\n",
            "Epoch [39/10000], iteration 95, Loss: 0.043579\n",
            "Epoch [39/10000], iteration 100, Loss: 0.041700\n",
            "validating....\n",
            "Epoch 39, validation loss is 0.0386, training loss is 0.0417\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 20\n",
            "Epoch [40/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [40/10000], iteration 10, Loss: 0.055000\n",
            "Epoch [40/10000], iteration 15, Loss: 0.050000\n",
            "Epoch [40/10000], iteration 20, Loss: 0.041500\n",
            "Epoch [40/10000], iteration 25, Loss: 0.037600\n",
            "Epoch [40/10000], iteration 30, Loss: 0.035000\n",
            "Epoch [40/10000], iteration 35, Loss: 0.032857\n",
            "Epoch [40/10000], iteration 40, Loss: 0.031500\n",
            "Epoch [40/10000], iteration 45, Loss: 0.030444\n",
            "Epoch [40/10000], iteration 50, Loss: 0.030600\n",
            "Epoch [40/10000], iteration 55, Loss: 0.030000\n",
            "Epoch [40/10000], iteration 60, Loss: 0.029167\n",
            "Epoch [40/10000], iteration 65, Loss: 0.028615\n",
            "Epoch [40/10000], iteration 70, Loss: 0.027571\n",
            "Epoch [40/10000], iteration 75, Loss: 0.026933\n",
            "Epoch [40/10000], iteration 80, Loss: 0.026625\n",
            "Epoch [40/10000], iteration 85, Loss: 0.026235\n",
            "Epoch [40/10000], iteration 90, Loss: 0.026333\n",
            "Epoch [40/10000], iteration 95, Loss: 0.030105\n",
            "Epoch [40/10000], iteration 100, Loss: 0.029500\n",
            "validating....\n",
            "Epoch 40, validation loss is 0.0412, training loss is 0.0295\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 21\n",
            "Epoch [41/10000], iteration 5, Loss: 0.082000\n",
            "Epoch [41/10000], iteration 10, Loss: 0.045000\n",
            "Epoch [41/10000], iteration 15, Loss: 0.036667\n",
            "Epoch [41/10000], iteration 20, Loss: 0.030500\n",
            "Epoch [41/10000], iteration 25, Loss: 0.029600\n",
            "Epoch [41/10000], iteration 30, Loss: 0.027333\n",
            "Epoch [41/10000], iteration 35, Loss: 0.027714\n",
            "Epoch [41/10000], iteration 40, Loss: 0.026500\n",
            "Epoch [41/10000], iteration 45, Loss: 0.032444\n",
            "Epoch [41/10000], iteration 50, Loss: 0.031400\n",
            "Epoch [41/10000], iteration 55, Loss: 0.029455\n",
            "Epoch [41/10000], iteration 60, Loss: 0.029000\n",
            "Epoch [41/10000], iteration 65, Loss: 0.028923\n",
            "Epoch [41/10000], iteration 70, Loss: 0.027429\n",
            "Epoch [41/10000], iteration 75, Loss: 0.027600\n",
            "Epoch [41/10000], iteration 80, Loss: 0.028375\n",
            "Epoch [41/10000], iteration 85, Loss: 0.028000\n",
            "Epoch [41/10000], iteration 90, Loss: 0.027778\n",
            "Epoch [41/10000], iteration 95, Loss: 0.027684\n",
            "Epoch [41/10000], iteration 100, Loss: 0.026500\n",
            "validating....\n",
            "Epoch 41, validation loss is 0.0370, training loss is 0.0265\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 22\n",
            "Epoch [42/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [42/10000], iteration 10, Loss: 0.056000\n",
            "Epoch [42/10000], iteration 15, Loss: 0.052667\n",
            "Epoch [42/10000], iteration 20, Loss: 0.049000\n",
            "Epoch [42/10000], iteration 25, Loss: 0.043600\n",
            "Epoch [42/10000], iteration 30, Loss: 0.042667\n",
            "Epoch [42/10000], iteration 35, Loss: 0.037143\n",
            "Epoch [42/10000], iteration 40, Loss: 0.034750\n",
            "Epoch [42/10000], iteration 45, Loss: 0.033778\n",
            "Epoch [42/10000], iteration 50, Loss: 0.039400\n",
            "Epoch [42/10000], iteration 55, Loss: 0.037091\n",
            "Epoch [42/10000], iteration 60, Loss: 0.035667\n",
            "Epoch [42/10000], iteration 65, Loss: 0.034923\n",
            "Epoch [42/10000], iteration 70, Loss: 0.034857\n",
            "Epoch [42/10000], iteration 75, Loss: 0.034267\n",
            "Epoch [42/10000], iteration 80, Loss: 0.038875\n",
            "Epoch [42/10000], iteration 85, Loss: 0.040706\n",
            "Epoch [42/10000], iteration 90, Loss: 0.039444\n",
            "Epoch [42/10000], iteration 95, Loss: 0.038842\n",
            "Epoch [42/10000], iteration 100, Loss: 0.037200\n",
            "validating....\n",
            "Epoch 42, validation loss is 0.0386, training loss is 0.0372\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 23\n",
            "Epoch [43/10000], iteration 5, Loss: 0.006000\n",
            "Epoch [43/10000], iteration 10, Loss: 0.015000\n",
            "Epoch [43/10000], iteration 15, Loss: 0.010667\n",
            "Epoch [43/10000], iteration 20, Loss: 0.014000\n",
            "Epoch [43/10000], iteration 25, Loss: 0.018000\n",
            "Epoch [43/10000], iteration 30, Loss: 0.017000\n",
            "Epoch [43/10000], iteration 35, Loss: 0.016000\n",
            "Epoch [43/10000], iteration 40, Loss: 0.019250\n",
            "Epoch [43/10000], iteration 45, Loss: 0.020444\n",
            "Epoch [43/10000], iteration 50, Loss: 0.020400\n",
            "Epoch [43/10000], iteration 55, Loss: 0.026909\n",
            "Epoch [43/10000], iteration 60, Loss: 0.025500\n",
            "Epoch [43/10000], iteration 65, Loss: 0.024769\n",
            "Epoch [43/10000], iteration 70, Loss: 0.024571\n",
            "Epoch [43/10000], iteration 75, Loss: 0.024000\n",
            "Epoch [43/10000], iteration 80, Loss: 0.023500\n",
            "Epoch [43/10000], iteration 85, Loss: 0.023647\n",
            "Epoch [43/10000], iteration 90, Loss: 0.023000\n",
            "Epoch [43/10000], iteration 95, Loss: 0.022842\n",
            "Epoch [43/10000], iteration 100, Loss: 0.021800\n",
            "validating....\n",
            "Epoch 43, validation loss is 0.0376, training loss is 0.0218\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 24\n",
            "Epoch [44/10000], iteration 5, Loss: 0.006000\n",
            "Epoch [44/10000], iteration 10, Loss: 0.016000\n",
            "Epoch [44/10000], iteration 15, Loss: 0.017333\n",
            "Epoch [44/10000], iteration 20, Loss: 0.022000\n",
            "Epoch [44/10000], iteration 25, Loss: 0.036800\n",
            "Epoch [44/10000], iteration 30, Loss: 0.037000\n",
            "Epoch [44/10000], iteration 35, Loss: 0.035429\n",
            "Epoch [44/10000], iteration 40, Loss: 0.033250\n",
            "Epoch [44/10000], iteration 45, Loss: 0.031333\n",
            "Epoch [44/10000], iteration 50, Loss: 0.029800\n",
            "Epoch [44/10000], iteration 55, Loss: 0.034727\n",
            "Epoch [44/10000], iteration 60, Loss: 0.032667\n",
            "Epoch [44/10000], iteration 65, Loss: 0.036308\n",
            "Epoch [44/10000], iteration 70, Loss: 0.040429\n",
            "Epoch [44/10000], iteration 75, Loss: 0.039333\n",
            "Epoch [44/10000], iteration 80, Loss: 0.038125\n",
            "Epoch [44/10000], iteration 85, Loss: 0.036235\n",
            "Epoch [44/10000], iteration 90, Loss: 0.034667\n",
            "Epoch [44/10000], iteration 95, Loss: 0.034316\n",
            "Epoch [44/10000], iteration 100, Loss: 0.033600\n",
            "validating....\n",
            "Epoch 44, validation loss is 0.0402, training loss is 0.0336\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 25\n",
            "Epoch [45/10000], iteration 5, Loss: 0.118000\n",
            "Epoch [45/10000], iteration 10, Loss: 0.074000\n",
            "Epoch [45/10000], iteration 15, Loss: 0.057333\n",
            "Epoch [45/10000], iteration 20, Loss: 0.049000\n",
            "Epoch [45/10000], iteration 25, Loss: 0.044000\n",
            "Epoch [45/10000], iteration 30, Loss: 0.052333\n",
            "Epoch [45/10000], iteration 35, Loss: 0.047143\n",
            "Epoch [45/10000], iteration 40, Loss: 0.042250\n",
            "Epoch [45/10000], iteration 45, Loss: 0.038444\n",
            "Epoch [45/10000], iteration 50, Loss: 0.037000\n",
            "Epoch [45/10000], iteration 55, Loss: 0.035636\n",
            "Epoch [45/10000], iteration 60, Loss: 0.034167\n",
            "Epoch [45/10000], iteration 65, Loss: 0.033538\n",
            "Epoch [45/10000], iteration 70, Loss: 0.032571\n",
            "Epoch [45/10000], iteration 75, Loss: 0.031200\n",
            "Epoch [45/10000], iteration 80, Loss: 0.030125\n",
            "Epoch [45/10000], iteration 85, Loss: 0.033765\n",
            "Epoch [45/10000], iteration 90, Loss: 0.033111\n",
            "Epoch [45/10000], iteration 95, Loss: 0.032526\n",
            "Epoch [45/10000], iteration 100, Loss: 0.032000\n",
            "validating....\n",
            "Epoch 45, validation loss is 0.0356, training loss is 0.0320\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 26\n",
            "Epoch [46/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [46/10000], iteration 10, Loss: 0.031000\n",
            "Epoch [46/10000], iteration 15, Loss: 0.032667\n",
            "Epoch [46/10000], iteration 20, Loss: 0.029500\n",
            "Epoch [46/10000], iteration 25, Loss: 0.057600\n",
            "Epoch [46/10000], iteration 30, Loss: 0.050000\n",
            "Epoch [46/10000], iteration 35, Loss: 0.044571\n",
            "Epoch [46/10000], iteration 40, Loss: 0.043250\n",
            "Epoch [46/10000], iteration 45, Loss: 0.040222\n",
            "Epoch [46/10000], iteration 50, Loss: 0.038000\n",
            "Epoch [46/10000], iteration 55, Loss: 0.036727\n",
            "Epoch [46/10000], iteration 60, Loss: 0.034333\n",
            "Epoch [46/10000], iteration 65, Loss: 0.033385\n",
            "Epoch [46/10000], iteration 70, Loss: 0.032286\n",
            "Epoch [46/10000], iteration 75, Loss: 0.031867\n",
            "Epoch [46/10000], iteration 80, Loss: 0.031500\n",
            "Epoch [46/10000], iteration 85, Loss: 0.030941\n",
            "Epoch [46/10000], iteration 90, Loss: 0.030222\n",
            "Epoch [46/10000], iteration 95, Loss: 0.029789\n",
            "Epoch [46/10000], iteration 100, Loss: 0.029000\n",
            "validating....\n",
            "Epoch 46, validation loss is 0.0375, training loss is 0.0290\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 27\n",
            "Epoch [47/10000], iteration 5, Loss: 0.088000\n",
            "Epoch [47/10000], iteration 10, Loss: 0.060000\n",
            "Epoch [47/10000], iteration 15, Loss: 0.044667\n",
            "Epoch [47/10000], iteration 20, Loss: 0.043000\n",
            "Epoch [47/10000], iteration 25, Loss: 0.044800\n",
            "Epoch [47/10000], iteration 30, Loss: 0.064333\n",
            "Epoch [47/10000], iteration 35, Loss: 0.057714\n",
            "Epoch [47/10000], iteration 40, Loss: 0.052500\n",
            "Epoch [47/10000], iteration 45, Loss: 0.050667\n",
            "Epoch [47/10000], iteration 50, Loss: 0.047000\n",
            "Epoch [47/10000], iteration 55, Loss: 0.046364\n",
            "Epoch [47/10000], iteration 60, Loss: 0.044000\n",
            "Epoch [47/10000], iteration 65, Loss: 0.041231\n",
            "Epoch [47/10000], iteration 70, Loss: 0.039857\n",
            "Epoch [47/10000], iteration 75, Loss: 0.037600\n",
            "Epoch [47/10000], iteration 80, Loss: 0.037125\n",
            "Epoch [47/10000], iteration 85, Loss: 0.035765\n",
            "Epoch [47/10000], iteration 90, Loss: 0.034333\n",
            "Epoch [47/10000], iteration 95, Loss: 0.033579\n",
            "Epoch [47/10000], iteration 100, Loss: 0.032000\n",
            "validating....\n",
            "Epoch 47, validation loss is 0.0447, training loss is 0.0320\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 28\n",
            "Epoch [48/10000], iteration 5, Loss: 0.038000\n",
            "Epoch [48/10000], iteration 10, Loss: 0.027000\n",
            "Epoch [48/10000], iteration 15, Loss: 0.022667\n",
            "Epoch [48/10000], iteration 20, Loss: 0.022000\n",
            "Epoch [48/10000], iteration 25, Loss: 0.019600\n",
            "Epoch [48/10000], iteration 30, Loss: 0.019333\n",
            "Epoch [48/10000], iteration 35, Loss: 0.020286\n",
            "Epoch [48/10000], iteration 40, Loss: 0.021750\n",
            "Epoch [48/10000], iteration 45, Loss: 0.027556\n",
            "Epoch [48/10000], iteration 50, Loss: 0.027200\n",
            "Epoch [48/10000], iteration 55, Loss: 0.030909\n",
            "Epoch [48/10000], iteration 60, Loss: 0.028667\n",
            "Epoch [48/10000], iteration 65, Loss: 0.027538\n",
            "Epoch [48/10000], iteration 70, Loss: 0.026286\n",
            "Epoch [48/10000], iteration 75, Loss: 0.025867\n",
            "Epoch [48/10000], iteration 80, Loss: 0.024375\n",
            "Epoch [48/10000], iteration 85, Loss: 0.024941\n",
            "Epoch [48/10000], iteration 90, Loss: 0.024333\n",
            "Epoch [48/10000], iteration 95, Loss: 0.023895\n",
            "Epoch [48/10000], iteration 100, Loss: 0.024900\n",
            "validating....\n",
            "Epoch 48, validation loss is 0.0354, training loss is 0.0249\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 29\n",
            "Epoch [49/10000], iteration 5, Loss: 0.008000\n",
            "Epoch [49/10000], iteration 10, Loss: 0.011000\n",
            "Epoch [49/10000], iteration 15, Loss: 0.016000\n",
            "Epoch [49/10000], iteration 20, Loss: 0.015500\n",
            "Epoch [49/10000], iteration 25, Loss: 0.018400\n",
            "Epoch [49/10000], iteration 30, Loss: 0.019333\n",
            "Epoch [49/10000], iteration 35, Loss: 0.029714\n",
            "Epoch [49/10000], iteration 40, Loss: 0.031000\n",
            "Epoch [49/10000], iteration 45, Loss: 0.030000\n",
            "Epoch [49/10000], iteration 50, Loss: 0.028800\n",
            "Epoch [49/10000], iteration 55, Loss: 0.028545\n",
            "Epoch [49/10000], iteration 60, Loss: 0.027333\n",
            "Epoch [49/10000], iteration 65, Loss: 0.026308\n",
            "Epoch [49/10000], iteration 70, Loss: 0.026286\n",
            "Epoch [49/10000], iteration 75, Loss: 0.034133\n",
            "Epoch [49/10000], iteration 80, Loss: 0.033250\n",
            "Epoch [49/10000], iteration 85, Loss: 0.034824\n",
            "Epoch [49/10000], iteration 90, Loss: 0.035000\n",
            "Epoch [49/10000], iteration 95, Loss: 0.034105\n",
            "Epoch [49/10000], iteration 100, Loss: 0.033100\n",
            "validating....\n",
            "Epoch 49, validation loss is 0.0355, training loss is 0.0331\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 30\n",
            "Epoch [50/10000], iteration 5, Loss: 0.008000\n",
            "Epoch [50/10000], iteration 10, Loss: 0.017000\n",
            "Epoch [50/10000], iteration 15, Loss: 0.032000\n",
            "Epoch [50/10000], iteration 20, Loss: 0.032500\n",
            "Epoch [50/10000], iteration 25, Loss: 0.029600\n",
            "Epoch [50/10000], iteration 30, Loss: 0.027333\n",
            "Epoch [50/10000], iteration 35, Loss: 0.025714\n",
            "Epoch [50/10000], iteration 40, Loss: 0.023750\n",
            "Epoch [50/10000], iteration 45, Loss: 0.025556\n",
            "Epoch [50/10000], iteration 50, Loss: 0.024200\n",
            "Epoch [50/10000], iteration 55, Loss: 0.030000\n",
            "Epoch [50/10000], iteration 60, Loss: 0.028833\n",
            "Epoch [50/10000], iteration 65, Loss: 0.027692\n",
            "Epoch [50/10000], iteration 70, Loss: 0.028000\n",
            "Epoch [50/10000], iteration 75, Loss: 0.026800\n",
            "Epoch [50/10000], iteration 80, Loss: 0.026125\n",
            "Epoch [50/10000], iteration 85, Loss: 0.025765\n",
            "Epoch [50/10000], iteration 90, Loss: 0.025000\n",
            "Epoch [50/10000], iteration 95, Loss: 0.026421\n",
            "Epoch [50/10000], iteration 100, Loss: 0.026400\n",
            "validating....\n",
            "Epoch 50, validation loss is 0.0364, training loss is 0.0264\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 31\n",
            "Epoch [51/10000], iteration 5, Loss: 0.014000\n",
            "Epoch [51/10000], iteration 10, Loss: 0.015000\n",
            "Epoch [51/10000], iteration 15, Loss: 0.013333\n",
            "Epoch [51/10000], iteration 20, Loss: 0.017500\n",
            "Epoch [51/10000], iteration 25, Loss: 0.020800\n",
            "Epoch [51/10000], iteration 30, Loss: 0.024000\n",
            "Epoch [51/10000], iteration 35, Loss: 0.022286\n",
            "Epoch [51/10000], iteration 40, Loss: 0.022250\n",
            "Epoch [51/10000], iteration 45, Loss: 0.020889\n",
            "Epoch [51/10000], iteration 50, Loss: 0.024000\n",
            "Epoch [51/10000], iteration 55, Loss: 0.023273\n",
            "Epoch [51/10000], iteration 60, Loss: 0.022167\n",
            "Epoch [51/10000], iteration 65, Loss: 0.028308\n",
            "Epoch [51/10000], iteration 70, Loss: 0.028000\n",
            "Epoch [51/10000], iteration 75, Loss: 0.026933\n",
            "Epoch [51/10000], iteration 80, Loss: 0.026000\n",
            "Epoch [51/10000], iteration 85, Loss: 0.025882\n",
            "Epoch [51/10000], iteration 90, Loss: 0.025889\n",
            "Epoch [51/10000], iteration 95, Loss: 0.025158\n",
            "Epoch [51/10000], iteration 100, Loss: 0.024800\n",
            "validating....\n",
            "Epoch 51, validation loss is 0.0359, training loss is 0.0248\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 32\n",
            "Epoch [52/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [52/10000], iteration 10, Loss: 0.084000\n",
            "Epoch [52/10000], iteration 15, Loss: 0.062667\n",
            "Epoch [52/10000], iteration 20, Loss: 0.066500\n",
            "Epoch [52/10000], iteration 25, Loss: 0.057600\n",
            "Epoch [52/10000], iteration 30, Loss: 0.052000\n",
            "Epoch [52/10000], iteration 35, Loss: 0.047714\n",
            "Epoch [52/10000], iteration 40, Loss: 0.044750\n",
            "Epoch [52/10000], iteration 45, Loss: 0.041556\n",
            "Epoch [52/10000], iteration 50, Loss: 0.038600\n",
            "Epoch [52/10000], iteration 55, Loss: 0.036364\n",
            "Epoch [52/10000], iteration 60, Loss: 0.036167\n",
            "Epoch [52/10000], iteration 65, Loss: 0.034923\n",
            "Epoch [52/10000], iteration 70, Loss: 0.035143\n",
            "Epoch [52/10000], iteration 75, Loss: 0.034667\n",
            "Epoch [52/10000], iteration 80, Loss: 0.034125\n",
            "Epoch [52/10000], iteration 85, Loss: 0.033529\n",
            "Epoch [52/10000], iteration 90, Loss: 0.032778\n",
            "Epoch [52/10000], iteration 95, Loss: 0.033263\n",
            "Epoch [52/10000], iteration 100, Loss: 0.036300\n",
            "validating....\n",
            "Epoch 52, validation loss is 0.0362, training loss is 0.0363\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 33\n",
            "Epoch [53/10000], iteration 5, Loss: 0.022000\n",
            "Epoch [53/10000], iteration 10, Loss: 0.019000\n",
            "Epoch [53/10000], iteration 15, Loss: 0.016000\n",
            "Epoch [53/10000], iteration 20, Loss: 0.014500\n",
            "Epoch [53/10000], iteration 25, Loss: 0.016000\n",
            "Epoch [53/10000], iteration 30, Loss: 0.016000\n",
            "Epoch [53/10000], iteration 35, Loss: 0.017429\n",
            "Epoch [53/10000], iteration 40, Loss: 0.018000\n",
            "Epoch [53/10000], iteration 45, Loss: 0.016889\n",
            "Epoch [53/10000], iteration 50, Loss: 0.030200\n",
            "Epoch [53/10000], iteration 55, Loss: 0.038364\n",
            "Epoch [53/10000], iteration 60, Loss: 0.037167\n",
            "Epoch [53/10000], iteration 65, Loss: 0.035692\n",
            "Epoch [53/10000], iteration 70, Loss: 0.034286\n",
            "Epoch [53/10000], iteration 75, Loss: 0.033733\n",
            "Epoch [53/10000], iteration 80, Loss: 0.033250\n",
            "Epoch [53/10000], iteration 85, Loss: 0.031765\n",
            "Epoch [53/10000], iteration 90, Loss: 0.035333\n",
            "Epoch [53/10000], iteration 95, Loss: 0.035053\n",
            "Epoch [53/10000], iteration 100, Loss: 0.033800\n",
            "validating....\n",
            "Epoch 53, validation loss is 0.0430, training loss is 0.0338\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 34\n",
            "Epoch [54/10000], iteration 5, Loss: 0.156000\n",
            "Epoch [54/10000], iteration 10, Loss: 0.083000\n",
            "Epoch [54/10000], iteration 15, Loss: 0.061333\n",
            "Epoch [54/10000], iteration 20, Loss: 0.050500\n",
            "Epoch [54/10000], iteration 25, Loss: 0.042800\n",
            "Epoch [54/10000], iteration 30, Loss: 0.039000\n",
            "Epoch [54/10000], iteration 35, Loss: 0.035429\n",
            "Epoch [54/10000], iteration 40, Loss: 0.032750\n",
            "Epoch [54/10000], iteration 45, Loss: 0.032222\n",
            "Epoch [54/10000], iteration 50, Loss: 0.031400\n",
            "Epoch [54/10000], iteration 55, Loss: 0.029091\n",
            "Epoch [54/10000], iteration 60, Loss: 0.031667\n",
            "Epoch [54/10000], iteration 65, Loss: 0.030462\n",
            "Epoch [54/10000], iteration 70, Loss: 0.029429\n",
            "Epoch [54/10000], iteration 75, Loss: 0.033867\n",
            "Epoch [54/10000], iteration 80, Loss: 0.034875\n",
            "Epoch [54/10000], iteration 85, Loss: 0.033529\n",
            "Epoch [54/10000], iteration 90, Loss: 0.035556\n",
            "Epoch [54/10000], iteration 95, Loss: 0.038632\n",
            "Epoch [54/10000], iteration 100, Loss: 0.041100\n",
            "validating....\n",
            "Epoch 54, validation loss is 0.0365, training loss is 0.0411\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 35\n",
            "Epoch [55/10000], iteration 5, Loss: 0.196000\n",
            "Epoch [55/10000], iteration 10, Loss: 0.111000\n",
            "Epoch [55/10000], iteration 15, Loss: 0.100000\n",
            "Epoch [55/10000], iteration 20, Loss: 0.091000\n",
            "Epoch [55/10000], iteration 25, Loss: 0.078800\n",
            "Epoch [55/10000], iteration 30, Loss: 0.071333\n",
            "Epoch [55/10000], iteration 35, Loss: 0.063143\n",
            "Epoch [55/10000], iteration 40, Loss: 0.056500\n",
            "Epoch [55/10000], iteration 45, Loss: 0.051111\n",
            "Epoch [55/10000], iteration 50, Loss: 0.048000\n",
            "Epoch [55/10000], iteration 55, Loss: 0.050364\n",
            "Epoch [55/10000], iteration 60, Loss: 0.053833\n",
            "Epoch [55/10000], iteration 65, Loss: 0.061692\n",
            "Epoch [55/10000], iteration 70, Loss: 0.059000\n",
            "Epoch [55/10000], iteration 75, Loss: 0.058667\n",
            "Epoch [55/10000], iteration 80, Loss: 0.056125\n",
            "Epoch [55/10000], iteration 85, Loss: 0.054235\n",
            "Epoch [55/10000], iteration 90, Loss: 0.052556\n",
            "Epoch [55/10000], iteration 95, Loss: 0.051789\n",
            "Epoch [55/10000], iteration 100, Loss: 0.050300\n",
            "validating....\n",
            "Epoch 55, validation loss is 0.0400, training loss is 0.0503\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 36\n",
            "Epoch [56/10000], iteration 5, Loss: 0.016000\n",
            "Epoch [56/10000], iteration 10, Loss: 0.010000\n",
            "Epoch [56/10000], iteration 15, Loss: 0.016000\n",
            "Epoch [56/10000], iteration 20, Loss: 0.017500\n",
            "Epoch [56/10000], iteration 25, Loss: 0.017600\n",
            "Epoch [56/10000], iteration 30, Loss: 0.016667\n",
            "Epoch [56/10000], iteration 35, Loss: 0.014857\n",
            "Epoch [56/10000], iteration 40, Loss: 0.023250\n",
            "Epoch [56/10000], iteration 45, Loss: 0.030000\n",
            "Epoch [56/10000], iteration 50, Loss: 0.035400\n",
            "Epoch [56/10000], iteration 55, Loss: 0.034000\n",
            "Epoch [56/10000], iteration 60, Loss: 0.035667\n",
            "Epoch [56/10000], iteration 65, Loss: 0.034615\n",
            "Epoch [56/10000], iteration 70, Loss: 0.033857\n",
            "Epoch [56/10000], iteration 75, Loss: 0.032667\n",
            "Epoch [56/10000], iteration 80, Loss: 0.036250\n",
            "Epoch [56/10000], iteration 85, Loss: 0.036118\n",
            "Epoch [56/10000], iteration 90, Loss: 0.035444\n",
            "Epoch [56/10000], iteration 95, Loss: 0.034316\n",
            "Epoch [56/10000], iteration 100, Loss: 0.033000\n",
            "validating....\n",
            "Epoch 56, validation loss is 0.0389, training loss is 0.0330\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 37\n",
            "Epoch [57/10000], iteration 5, Loss: 0.016000\n",
            "Epoch [57/10000], iteration 10, Loss: 0.014000\n",
            "Epoch [57/10000], iteration 15, Loss: 0.014000\n",
            "Epoch [57/10000], iteration 20, Loss: 0.014500\n",
            "Epoch [57/10000], iteration 25, Loss: 0.013600\n",
            "Epoch [57/10000], iteration 30, Loss: 0.013667\n",
            "Epoch [57/10000], iteration 35, Loss: 0.016000\n",
            "Epoch [57/10000], iteration 40, Loss: 0.016000\n",
            "Epoch [57/10000], iteration 45, Loss: 0.019333\n",
            "Epoch [57/10000], iteration 50, Loss: 0.019600\n",
            "Epoch [57/10000], iteration 55, Loss: 0.019091\n",
            "Epoch [57/10000], iteration 60, Loss: 0.018333\n",
            "Epoch [57/10000], iteration 65, Loss: 0.019385\n",
            "Epoch [57/10000], iteration 70, Loss: 0.019571\n",
            "Epoch [57/10000], iteration 75, Loss: 0.020267\n",
            "Epoch [57/10000], iteration 80, Loss: 0.020000\n",
            "Epoch [57/10000], iteration 85, Loss: 0.020588\n",
            "Epoch [57/10000], iteration 90, Loss: 0.024111\n",
            "Epoch [57/10000], iteration 95, Loss: 0.024000\n",
            "Epoch [57/10000], iteration 100, Loss: 0.023000\n",
            "validating....\n",
            "Epoch 57, validation loss is 0.0402, training loss is 0.0230\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 38\n",
            "Epoch [58/10000], iteration 5, Loss: 0.030000\n",
            "Epoch [58/10000], iteration 10, Loss: 0.027000\n",
            "Epoch [58/10000], iteration 15, Loss: 0.023333\n",
            "Epoch [58/10000], iteration 20, Loss: 0.021500\n",
            "Epoch [58/10000], iteration 25, Loss: 0.039600\n",
            "Epoch [58/10000], iteration 30, Loss: 0.038333\n",
            "Epoch [58/10000], iteration 35, Loss: 0.034286\n",
            "Epoch [58/10000], iteration 40, Loss: 0.032000\n",
            "Epoch [58/10000], iteration 45, Loss: 0.029333\n",
            "Epoch [58/10000], iteration 50, Loss: 0.028200\n",
            "Epoch [58/10000], iteration 55, Loss: 0.026545\n",
            "Epoch [58/10000], iteration 60, Loss: 0.026333\n",
            "Epoch [58/10000], iteration 65, Loss: 0.025846\n",
            "Epoch [58/10000], iteration 70, Loss: 0.034857\n",
            "Epoch [58/10000], iteration 75, Loss: 0.033333\n",
            "Epoch [58/10000], iteration 80, Loss: 0.034250\n",
            "Epoch [58/10000], iteration 85, Loss: 0.033059\n",
            "Epoch [58/10000], iteration 90, Loss: 0.031889\n",
            "Epoch [58/10000], iteration 95, Loss: 0.031158\n",
            "Epoch [58/10000], iteration 100, Loss: 0.029900\n",
            "validating....\n",
            "Epoch 58, validation loss is 0.0413, training loss is 0.0299\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 39\n",
            "Epoch [59/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [59/10000], iteration 10, Loss: 0.020000\n",
            "Epoch [59/10000], iteration 15, Loss: 0.017333\n",
            "Epoch [59/10000], iteration 20, Loss: 0.019000\n",
            "Epoch [59/10000], iteration 25, Loss: 0.018000\n",
            "Epoch [59/10000], iteration 30, Loss: 0.017667\n",
            "Epoch [59/10000], iteration 35, Loss: 0.018571\n",
            "Epoch [59/10000], iteration 40, Loss: 0.018000\n",
            "Epoch [59/10000], iteration 45, Loss: 0.017556\n",
            "Epoch [59/10000], iteration 50, Loss: 0.017400\n",
            "Epoch [59/10000], iteration 55, Loss: 0.016182\n",
            "Epoch [59/10000], iteration 60, Loss: 0.022167\n",
            "Epoch [59/10000], iteration 65, Loss: 0.021538\n",
            "Epoch [59/10000], iteration 70, Loss: 0.026714\n",
            "Epoch [59/10000], iteration 75, Loss: 0.026533\n",
            "Epoch [59/10000], iteration 80, Loss: 0.026750\n",
            "Epoch [59/10000], iteration 85, Loss: 0.026118\n",
            "Epoch [59/10000], iteration 90, Loss: 0.025667\n",
            "Epoch [59/10000], iteration 95, Loss: 0.024842\n",
            "Epoch [59/10000], iteration 100, Loss: 0.025000\n",
            "validating....\n",
            "Epoch 59, validation loss is 0.0377, training loss is 0.0250\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 40\n",
            "Epoch [60/10000], iteration 5, Loss: 0.012000\n",
            "Epoch [60/10000], iteration 10, Loss: 0.014000\n",
            "Epoch [60/10000], iteration 15, Loss: 0.017333\n",
            "Epoch [60/10000], iteration 20, Loss: 0.037500\n",
            "Epoch [60/10000], iteration 25, Loss: 0.035200\n",
            "Epoch [60/10000], iteration 30, Loss: 0.044667\n",
            "Epoch [60/10000], iteration 35, Loss: 0.043143\n",
            "Epoch [60/10000], iteration 40, Loss: 0.039500\n",
            "Epoch [60/10000], iteration 45, Loss: 0.039778\n",
            "Epoch [60/10000], iteration 50, Loss: 0.038000\n",
            "Epoch [60/10000], iteration 55, Loss: 0.041273\n",
            "Epoch [60/10000], iteration 60, Loss: 0.038833\n",
            "Epoch [60/10000], iteration 65, Loss: 0.037846\n",
            "Epoch [60/10000], iteration 70, Loss: 0.042429\n",
            "Epoch [60/10000], iteration 75, Loss: 0.040400\n",
            "Epoch [60/10000], iteration 80, Loss: 0.040000\n",
            "Epoch [60/10000], iteration 85, Loss: 0.038941\n",
            "Epoch [60/10000], iteration 90, Loss: 0.042000\n",
            "Epoch [60/10000], iteration 95, Loss: 0.041368\n",
            "Epoch [60/10000], iteration 100, Loss: 0.039400\n",
            "validating....\n",
            "Epoch 60, validation loss is 0.0335, training loss is 0.0394\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 41\n",
            "Epoch [61/10000], iteration 5, Loss: 0.008000\n",
            "Epoch [61/10000], iteration 10, Loss: 0.012000\n",
            "Epoch [61/10000], iteration 15, Loss: 0.015333\n",
            "Epoch [61/10000], iteration 20, Loss: 0.017500\n",
            "Epoch [61/10000], iteration 25, Loss: 0.016000\n",
            "Epoch [61/10000], iteration 30, Loss: 0.016333\n",
            "Epoch [61/10000], iteration 35, Loss: 0.015714\n",
            "Epoch [61/10000], iteration 40, Loss: 0.015750\n",
            "Epoch [61/10000], iteration 45, Loss: 0.015333\n",
            "Epoch [61/10000], iteration 50, Loss: 0.015000\n",
            "Epoch [61/10000], iteration 55, Loss: 0.018909\n",
            "Epoch [61/10000], iteration 60, Loss: 0.019167\n",
            "Epoch [61/10000], iteration 65, Loss: 0.018154\n",
            "Epoch [61/10000], iteration 70, Loss: 0.018857\n",
            "Epoch [61/10000], iteration 75, Loss: 0.019067\n",
            "Epoch [61/10000], iteration 80, Loss: 0.019250\n",
            "Epoch [61/10000], iteration 85, Loss: 0.019294\n",
            "Epoch [61/10000], iteration 90, Loss: 0.019333\n",
            "Epoch [61/10000], iteration 95, Loss: 0.018947\n",
            "Epoch [61/10000], iteration 100, Loss: 0.018400\n",
            "validating....\n",
            "Epoch 61, validation loss is 0.0343, training loss is 0.0184\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 42\n",
            "Epoch [62/10000], iteration 5, Loss: 0.086000\n",
            "Epoch [62/10000], iteration 10, Loss: 0.047000\n",
            "Epoch [62/10000], iteration 15, Loss: 0.035333\n",
            "Epoch [62/10000], iteration 20, Loss: 0.032000\n",
            "Epoch [62/10000], iteration 25, Loss: 0.032400\n",
            "Epoch [62/10000], iteration 30, Loss: 0.027667\n",
            "Epoch [62/10000], iteration 35, Loss: 0.028571\n",
            "Epoch [62/10000], iteration 40, Loss: 0.027000\n",
            "Epoch [62/10000], iteration 45, Loss: 0.024667\n",
            "Epoch [62/10000], iteration 50, Loss: 0.023200\n",
            "Epoch [62/10000], iteration 55, Loss: 0.022364\n",
            "Epoch [62/10000], iteration 60, Loss: 0.023000\n",
            "Epoch [62/10000], iteration 65, Loss: 0.022769\n",
            "Epoch [62/10000], iteration 70, Loss: 0.023000\n",
            "Epoch [62/10000], iteration 75, Loss: 0.022267\n",
            "Epoch [62/10000], iteration 80, Loss: 0.022500\n",
            "Epoch [62/10000], iteration 85, Loss: 0.021882\n",
            "Epoch [62/10000], iteration 90, Loss: 0.022444\n",
            "Epoch [62/10000], iteration 95, Loss: 0.022632\n",
            "Epoch [62/10000], iteration 100, Loss: 0.022400\n",
            "validating....\n",
            "Epoch 62, validation loss is 0.0372, training loss is 0.0224\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 43\n",
            "Epoch [63/10000], iteration 5, Loss: 0.014000\n",
            "Epoch [63/10000], iteration 10, Loss: 0.022000\n",
            "Epoch [63/10000], iteration 15, Loss: 0.023333\n",
            "Epoch [63/10000], iteration 20, Loss: 0.024000\n",
            "Epoch [63/10000], iteration 25, Loss: 0.021600\n",
            "Epoch [63/10000], iteration 30, Loss: 0.019667\n",
            "Epoch [63/10000], iteration 35, Loss: 0.020286\n",
            "Epoch [63/10000], iteration 40, Loss: 0.019500\n",
            "Epoch [63/10000], iteration 45, Loss: 0.019111\n",
            "Epoch [63/10000], iteration 50, Loss: 0.018200\n",
            "Epoch [63/10000], iteration 55, Loss: 0.026545\n",
            "Epoch [63/10000], iteration 60, Loss: 0.032167\n",
            "Epoch [63/10000], iteration 65, Loss: 0.030154\n",
            "Epoch [63/10000], iteration 70, Loss: 0.034143\n",
            "Epoch [63/10000], iteration 75, Loss: 0.032933\n",
            "Epoch [63/10000], iteration 80, Loss: 0.031750\n",
            "Epoch [63/10000], iteration 85, Loss: 0.034824\n",
            "Epoch [63/10000], iteration 90, Loss: 0.037222\n",
            "Epoch [63/10000], iteration 95, Loss: 0.036632\n",
            "Epoch [63/10000], iteration 100, Loss: 0.035400\n",
            "validating....\n",
            "Epoch 63, validation loss is 0.0364, training loss is 0.0354\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 44\n",
            "Epoch [64/10000], iteration 5, Loss: 0.022000\n",
            "Epoch [64/10000], iteration 10, Loss: 0.025000\n",
            "Epoch [64/10000], iteration 15, Loss: 0.022667\n",
            "Epoch [64/10000], iteration 20, Loss: 0.022000\n",
            "Epoch [64/10000], iteration 25, Loss: 0.018400\n",
            "Epoch [64/10000], iteration 30, Loss: 0.018667\n",
            "Epoch [64/10000], iteration 35, Loss: 0.019714\n",
            "Epoch [64/10000], iteration 40, Loss: 0.019750\n",
            "Epoch [64/10000], iteration 45, Loss: 0.025556\n",
            "Epoch [64/10000], iteration 50, Loss: 0.025000\n",
            "Epoch [64/10000], iteration 55, Loss: 0.025636\n",
            "Epoch [64/10000], iteration 60, Loss: 0.025333\n",
            "Epoch [64/10000], iteration 65, Loss: 0.025385\n",
            "Epoch [64/10000], iteration 70, Loss: 0.024714\n",
            "Epoch [64/10000], iteration 75, Loss: 0.024800\n",
            "Epoch [64/10000], iteration 80, Loss: 0.023875\n",
            "Epoch [64/10000], iteration 85, Loss: 0.023176\n",
            "Epoch [64/10000], iteration 90, Loss: 0.024111\n",
            "Epoch [64/10000], iteration 95, Loss: 0.024316\n",
            "Epoch [64/10000], iteration 100, Loss: 0.023900\n",
            "validating....\n",
            "Epoch 64, validation loss is 0.0422, training loss is 0.0239\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 45\n",
            "Epoch [65/10000], iteration 5, Loss: 0.098000\n",
            "Epoch [65/10000], iteration 10, Loss: 0.058000\n",
            "Epoch [65/10000], iteration 15, Loss: 0.043333\n",
            "Epoch [65/10000], iteration 20, Loss: 0.035000\n",
            "Epoch [65/10000], iteration 25, Loss: 0.033600\n",
            "Epoch [65/10000], iteration 30, Loss: 0.031000\n",
            "Epoch [65/10000], iteration 35, Loss: 0.030857\n",
            "Epoch [65/10000], iteration 40, Loss: 0.038250\n",
            "Epoch [65/10000], iteration 45, Loss: 0.036444\n",
            "Epoch [65/10000], iteration 50, Loss: 0.043800\n",
            "Epoch [65/10000], iteration 55, Loss: 0.041455\n",
            "Epoch [65/10000], iteration 60, Loss: 0.040333\n",
            "Epoch [65/10000], iteration 65, Loss: 0.038769\n",
            "Epoch [65/10000], iteration 70, Loss: 0.042143\n",
            "Epoch [65/10000], iteration 75, Loss: 0.045333\n",
            "Epoch [65/10000], iteration 80, Loss: 0.044000\n",
            "Epoch [65/10000], iteration 85, Loss: 0.042588\n",
            "Epoch [65/10000], iteration 90, Loss: 0.040556\n",
            "Epoch [65/10000], iteration 95, Loss: 0.038947\n",
            "Epoch [65/10000], iteration 100, Loss: 0.038500\n",
            "validating....\n",
            "Epoch 65, validation loss is 0.0451, training loss is 0.0385\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 46\n",
            "Epoch [66/10000], iteration 5, Loss: 0.048000\n",
            "Epoch [66/10000], iteration 10, Loss: 0.065000\n",
            "Epoch [66/10000], iteration 15, Loss: 0.070000\n",
            "Epoch [66/10000], iteration 20, Loss: 0.059500\n",
            "Epoch [66/10000], iteration 25, Loss: 0.056800\n",
            "Epoch [66/10000], iteration 30, Loss: 0.050333\n",
            "Epoch [66/10000], iteration 35, Loss: 0.048857\n",
            "Epoch [66/10000], iteration 40, Loss: 0.044250\n",
            "Epoch [66/10000], iteration 45, Loss: 0.041111\n",
            "Epoch [66/10000], iteration 50, Loss: 0.037800\n",
            "Epoch [66/10000], iteration 55, Loss: 0.038000\n",
            "Epoch [66/10000], iteration 60, Loss: 0.036167\n",
            "Epoch [66/10000], iteration 65, Loss: 0.035231\n",
            "Epoch [66/10000], iteration 70, Loss: 0.034143\n",
            "Epoch [66/10000], iteration 75, Loss: 0.037067\n",
            "Epoch [66/10000], iteration 80, Loss: 0.035500\n",
            "Epoch [66/10000], iteration 85, Loss: 0.034000\n",
            "Epoch [66/10000], iteration 90, Loss: 0.032667\n",
            "Epoch [66/10000], iteration 95, Loss: 0.032316\n",
            "Epoch [66/10000], iteration 100, Loss: 0.032200\n",
            "validating....\n",
            "Epoch 66, validation loss is 0.0352, training loss is 0.0322\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 47\n",
            "Epoch [67/10000], iteration 5, Loss: 0.010000\n",
            "Epoch [67/10000], iteration 10, Loss: 0.011000\n",
            "Epoch [67/10000], iteration 15, Loss: 0.013333\n",
            "Epoch [67/10000], iteration 20, Loss: 0.013500\n",
            "Epoch [67/10000], iteration 25, Loss: 0.039200\n",
            "Epoch [67/10000], iteration 30, Loss: 0.036000\n",
            "Epoch [67/10000], iteration 35, Loss: 0.032571\n",
            "Epoch [67/10000], iteration 40, Loss: 0.031750\n",
            "Epoch [67/10000], iteration 45, Loss: 0.029778\n",
            "Epoch [67/10000], iteration 50, Loss: 0.028200\n",
            "Epoch [67/10000], iteration 55, Loss: 0.027818\n",
            "Epoch [67/10000], iteration 60, Loss: 0.027333\n",
            "Epoch [67/10000], iteration 65, Loss: 0.026769\n",
            "Epoch [67/10000], iteration 70, Loss: 0.026286\n",
            "Epoch [67/10000], iteration 75, Loss: 0.030667\n",
            "Epoch [67/10000], iteration 80, Loss: 0.029750\n",
            "Epoch [67/10000], iteration 85, Loss: 0.030000\n",
            "Epoch [67/10000], iteration 90, Loss: 0.029889\n",
            "Epoch [67/10000], iteration 95, Loss: 0.028842\n",
            "Epoch [67/10000], iteration 100, Loss: 0.028000\n",
            "validating....\n",
            "Epoch 67, validation loss is 0.0434, training loss is 0.0280\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 48\n",
            "Epoch [68/10000], iteration 5, Loss: 0.024000\n",
            "Epoch [68/10000], iteration 10, Loss: 0.021000\n",
            "Epoch [68/10000], iteration 15, Loss: 0.021333\n",
            "Epoch [68/10000], iteration 20, Loss: 0.022000\n",
            "Epoch [68/10000], iteration 25, Loss: 0.026000\n",
            "Epoch [68/10000], iteration 30, Loss: 0.026000\n",
            "Epoch [68/10000], iteration 35, Loss: 0.024857\n",
            "Epoch [68/10000], iteration 40, Loss: 0.024000\n",
            "Epoch [68/10000], iteration 45, Loss: 0.023333\n",
            "Epoch [68/10000], iteration 50, Loss: 0.030000\n",
            "Epoch [68/10000], iteration 55, Loss: 0.028545\n",
            "Epoch [68/10000], iteration 60, Loss: 0.027000\n",
            "Epoch [68/10000], iteration 65, Loss: 0.029846\n",
            "Epoch [68/10000], iteration 70, Loss: 0.029000\n",
            "Epoch [68/10000], iteration 75, Loss: 0.028533\n",
            "Epoch [68/10000], iteration 80, Loss: 0.028000\n",
            "Epoch [68/10000], iteration 85, Loss: 0.027529\n",
            "Epoch [68/10000], iteration 90, Loss: 0.027444\n",
            "Epoch [68/10000], iteration 95, Loss: 0.027158\n",
            "Epoch [68/10000], iteration 100, Loss: 0.030200\n",
            "validating....\n",
            "Epoch 68, validation loss is 0.0403, training loss is 0.0302\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 49\n",
            "Epoch [69/10000], iteration 5, Loss: 0.030000\n",
            "Epoch [69/10000], iteration 10, Loss: 0.051000\n",
            "Epoch [69/10000], iteration 15, Loss: 0.040000\n",
            "Epoch [69/10000], iteration 20, Loss: 0.033500\n",
            "Epoch [69/10000], iteration 25, Loss: 0.029200\n",
            "Epoch [69/10000], iteration 30, Loss: 0.027667\n",
            "Epoch [69/10000], iteration 35, Loss: 0.031143\n",
            "Epoch [69/10000], iteration 40, Loss: 0.028500\n",
            "Epoch [69/10000], iteration 45, Loss: 0.027111\n",
            "Epoch [69/10000], iteration 50, Loss: 0.026200\n",
            "Epoch [69/10000], iteration 55, Loss: 0.028364\n",
            "Epoch [69/10000], iteration 60, Loss: 0.026500\n",
            "Epoch [69/10000], iteration 65, Loss: 0.025231\n",
            "Epoch [69/10000], iteration 70, Loss: 0.024857\n",
            "Epoch [69/10000], iteration 75, Loss: 0.024800\n",
            "Epoch [69/10000], iteration 80, Loss: 0.024875\n",
            "Epoch [69/10000], iteration 85, Loss: 0.033529\n",
            "Epoch [69/10000], iteration 90, Loss: 0.032556\n",
            "Epoch [69/10000], iteration 95, Loss: 0.032105\n",
            "Epoch [69/10000], iteration 100, Loss: 0.031100\n",
            "validating....\n",
            "Epoch 69, validation loss is 0.0426, training loss is 0.0311\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 50\n",
            "Epoch [70/10000], iteration 5, Loss: 0.038000\n",
            "Epoch [70/10000], iteration 10, Loss: 0.071000\n",
            "Epoch [70/10000], iteration 15, Loss: 0.051333\n",
            "Epoch [70/10000], iteration 20, Loss: 0.045500\n",
            "Epoch [70/10000], iteration 25, Loss: 0.042000\n",
            "Epoch [70/10000], iteration 30, Loss: 0.041000\n",
            "Epoch [70/10000], iteration 35, Loss: 0.037143\n",
            "Epoch [70/10000], iteration 40, Loss: 0.033250\n",
            "Epoch [70/10000], iteration 45, Loss: 0.032667\n",
            "Epoch [70/10000], iteration 50, Loss: 0.044200\n",
            "Epoch [70/10000], iteration 55, Loss: 0.042000\n",
            "Epoch [70/10000], iteration 60, Loss: 0.040167\n",
            "Epoch [70/10000], iteration 65, Loss: 0.038308\n",
            "Epoch [70/10000], iteration 70, Loss: 0.037143\n",
            "Epoch [70/10000], iteration 75, Loss: 0.035467\n",
            "Epoch [70/10000], iteration 80, Loss: 0.034500\n",
            "Epoch [70/10000], iteration 85, Loss: 0.034471\n",
            "Epoch [70/10000], iteration 90, Loss: 0.037778\n",
            "Epoch [70/10000], iteration 95, Loss: 0.040947\n",
            "Epoch [70/10000], iteration 100, Loss: 0.039400\n",
            "validating....\n",
            "Epoch 70, validation loss is 0.0383, training loss is 0.0394\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 51\n",
            "Epoch [71/10000], iteration 5, Loss: 0.028000\n",
            "Epoch [71/10000], iteration 10, Loss: 0.024000\n",
            "Epoch [71/10000], iteration 15, Loss: 0.022000\n",
            "Epoch [71/10000], iteration 20, Loss: 0.022500\n",
            "Epoch [71/10000], iteration 25, Loss: 0.023600\n",
            "Epoch [71/10000], iteration 30, Loss: 0.023000\n",
            "Epoch [71/10000], iteration 35, Loss: 0.027429\n",
            "Epoch [71/10000], iteration 40, Loss: 0.026250\n",
            "Epoch [71/10000], iteration 45, Loss: 0.025333\n",
            "Epoch [71/10000], iteration 50, Loss: 0.031000\n",
            "Epoch [71/10000], iteration 55, Loss: 0.030545\n",
            "Epoch [71/10000], iteration 60, Loss: 0.031167\n",
            "Epoch [71/10000], iteration 65, Loss: 0.029692\n",
            "Epoch [71/10000], iteration 70, Loss: 0.029000\n",
            "Epoch [71/10000], iteration 75, Loss: 0.032400\n",
            "Epoch [71/10000], iteration 80, Loss: 0.031000\n",
            "Epoch [71/10000], iteration 85, Loss: 0.034588\n",
            "Epoch [71/10000], iteration 90, Loss: 0.033444\n",
            "Epoch [71/10000], iteration 95, Loss: 0.037895\n",
            "Epoch [71/10000], iteration 100, Loss: 0.036800\n",
            "validating....\n",
            "Epoch 71, validation loss is 0.0425, training loss is 0.0368\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 52\n",
            "Epoch [72/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [72/10000], iteration 10, Loss: 0.015000\n",
            "Epoch [72/10000], iteration 15, Loss: 0.014667\n",
            "Epoch [72/10000], iteration 20, Loss: 0.027000\n",
            "Epoch [72/10000], iteration 25, Loss: 0.027600\n",
            "Epoch [72/10000], iteration 30, Loss: 0.024667\n",
            "Epoch [72/10000], iteration 35, Loss: 0.024000\n",
            "Epoch [72/10000], iteration 40, Loss: 0.022750\n",
            "Epoch [72/10000], iteration 45, Loss: 0.023778\n",
            "Epoch [72/10000], iteration 50, Loss: 0.023800\n",
            "Epoch [72/10000], iteration 55, Loss: 0.022545\n",
            "Epoch [72/10000], iteration 60, Loss: 0.021500\n",
            "Epoch [72/10000], iteration 65, Loss: 0.021231\n",
            "Epoch [72/10000], iteration 70, Loss: 0.021571\n",
            "Epoch [72/10000], iteration 75, Loss: 0.021200\n",
            "Epoch [72/10000], iteration 80, Loss: 0.021000\n",
            "Epoch [72/10000], iteration 85, Loss: 0.020824\n",
            "Epoch [72/10000], iteration 90, Loss: 0.024222\n",
            "Epoch [72/10000], iteration 95, Loss: 0.024632\n",
            "Epoch [72/10000], iteration 100, Loss: 0.024500\n",
            "validating....\n",
            "Epoch 72, validation loss is 0.0381, training loss is 0.0245\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 53\n",
            "Epoch [73/10000], iteration 5, Loss: 0.024000\n",
            "Epoch [73/10000], iteration 10, Loss: 0.024000\n",
            "Epoch [73/10000], iteration 15, Loss: 0.022667\n",
            "Epoch [73/10000], iteration 20, Loss: 0.023500\n",
            "Epoch [73/10000], iteration 25, Loss: 0.020000\n",
            "Epoch [73/10000], iteration 30, Loss: 0.020000\n",
            "Epoch [73/10000], iteration 35, Loss: 0.021143\n",
            "Epoch [73/10000], iteration 40, Loss: 0.022250\n",
            "Epoch [73/10000], iteration 45, Loss: 0.021556\n",
            "Epoch [73/10000], iteration 50, Loss: 0.022000\n",
            "Epoch [73/10000], iteration 55, Loss: 0.022727\n",
            "Epoch [73/10000], iteration 60, Loss: 0.028167\n",
            "Epoch [73/10000], iteration 65, Loss: 0.032462\n",
            "Epoch [73/10000], iteration 70, Loss: 0.031857\n",
            "Epoch [73/10000], iteration 75, Loss: 0.029867\n",
            "Epoch [73/10000], iteration 80, Loss: 0.030750\n",
            "Epoch [73/10000], iteration 85, Loss: 0.030118\n",
            "Epoch [73/10000], iteration 90, Loss: 0.029444\n",
            "Epoch [73/10000], iteration 95, Loss: 0.029053\n",
            "Epoch [73/10000], iteration 100, Loss: 0.029300\n",
            "validating....\n",
            "Epoch 73, validation loss is 0.0366, training loss is 0.0293\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 54\n",
            "Epoch [74/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [74/10000], iteration 10, Loss: 0.027000\n",
            "Epoch [74/10000], iteration 15, Loss: 0.021333\n",
            "Epoch [74/10000], iteration 20, Loss: 0.020000\n",
            "Epoch [74/10000], iteration 25, Loss: 0.017600\n",
            "Epoch [74/10000], iteration 30, Loss: 0.018000\n",
            "Epoch [74/10000], iteration 35, Loss: 0.028000\n",
            "Epoch [74/10000], iteration 40, Loss: 0.026500\n",
            "Epoch [74/10000], iteration 45, Loss: 0.024444\n",
            "Epoch [74/10000], iteration 50, Loss: 0.025600\n",
            "Epoch [74/10000], iteration 55, Loss: 0.024909\n",
            "Epoch [74/10000], iteration 60, Loss: 0.025000\n",
            "Epoch [74/10000], iteration 65, Loss: 0.024154\n",
            "Epoch [74/10000], iteration 70, Loss: 0.023429\n",
            "Epoch [74/10000], iteration 75, Loss: 0.027467\n",
            "Epoch [74/10000], iteration 80, Loss: 0.026625\n",
            "Epoch [74/10000], iteration 85, Loss: 0.026000\n",
            "Epoch [74/10000], iteration 90, Loss: 0.029000\n",
            "Epoch [74/10000], iteration 95, Loss: 0.028421\n",
            "Epoch [74/10000], iteration 100, Loss: 0.028500\n",
            "validating....\n",
            "Epoch 74, validation loss is 0.0408, training loss is 0.0285\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 55\n",
            "Epoch [75/10000], iteration 5, Loss: 0.008000\n",
            "Epoch [75/10000], iteration 10, Loss: 0.014000\n",
            "Epoch [75/10000], iteration 15, Loss: 0.015333\n",
            "Epoch [75/10000], iteration 20, Loss: 0.014000\n",
            "Epoch [75/10000], iteration 25, Loss: 0.012000\n",
            "Epoch [75/10000], iteration 30, Loss: 0.013000\n",
            "Epoch [75/10000], iteration 35, Loss: 0.012857\n",
            "Epoch [75/10000], iteration 40, Loss: 0.015500\n",
            "Epoch [75/10000], iteration 45, Loss: 0.014889\n",
            "Epoch [75/10000], iteration 50, Loss: 0.030600\n",
            "Epoch [75/10000], iteration 55, Loss: 0.036364\n",
            "Epoch [75/10000], iteration 60, Loss: 0.035500\n",
            "Epoch [75/10000], iteration 65, Loss: 0.034000\n",
            "Epoch [75/10000], iteration 70, Loss: 0.033143\n",
            "Epoch [75/10000], iteration 75, Loss: 0.032267\n",
            "Epoch [75/10000], iteration 80, Loss: 0.034875\n",
            "Epoch [75/10000], iteration 85, Loss: 0.034118\n",
            "Epoch [75/10000], iteration 90, Loss: 0.032667\n",
            "Epoch [75/10000], iteration 95, Loss: 0.032000\n",
            "Epoch [75/10000], iteration 100, Loss: 0.031900\n",
            "validating....\n",
            "Epoch 75, validation loss is 0.0406, training loss is 0.0319\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 56\n",
            "Epoch [76/10000], iteration 5, Loss: 0.016000\n",
            "Epoch [76/10000], iteration 10, Loss: 0.020000\n",
            "Epoch [76/10000], iteration 15, Loss: 0.025333\n",
            "Epoch [76/10000], iteration 20, Loss: 0.023000\n",
            "Epoch [76/10000], iteration 25, Loss: 0.021600\n",
            "Epoch [76/10000], iteration 30, Loss: 0.019000\n",
            "Epoch [76/10000], iteration 35, Loss: 0.017429\n",
            "Epoch [76/10000], iteration 40, Loss: 0.019000\n",
            "Epoch [76/10000], iteration 45, Loss: 0.017778\n",
            "Epoch [76/10000], iteration 50, Loss: 0.018400\n",
            "Epoch [76/10000], iteration 55, Loss: 0.017636\n",
            "Epoch [76/10000], iteration 60, Loss: 0.018167\n",
            "Epoch [76/10000], iteration 65, Loss: 0.022308\n",
            "Epoch [76/10000], iteration 70, Loss: 0.021286\n",
            "Epoch [76/10000], iteration 75, Loss: 0.024533\n",
            "Epoch [76/10000], iteration 80, Loss: 0.024250\n",
            "Epoch [76/10000], iteration 85, Loss: 0.023647\n",
            "Epoch [76/10000], iteration 90, Loss: 0.022889\n",
            "Epoch [76/10000], iteration 95, Loss: 0.022211\n",
            "Epoch [76/10000], iteration 100, Loss: 0.021500\n",
            "validating....\n",
            "Epoch 76, validation loss is 0.0404, training loss is 0.0215\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 57\n",
            "Epoch [77/10000], iteration 5, Loss: 0.030000\n",
            "Epoch [77/10000], iteration 10, Loss: 0.042000\n",
            "Epoch [77/10000], iteration 15, Loss: 0.036667\n",
            "Epoch [77/10000], iteration 20, Loss: 0.040000\n",
            "Epoch [77/10000], iteration 25, Loss: 0.040000\n",
            "Epoch [77/10000], iteration 30, Loss: 0.037000\n",
            "Epoch [77/10000], iteration 35, Loss: 0.033429\n",
            "Epoch [77/10000], iteration 40, Loss: 0.035750\n",
            "Epoch [77/10000], iteration 45, Loss: 0.034222\n",
            "Epoch [77/10000], iteration 50, Loss: 0.031600\n",
            "Epoch [77/10000], iteration 55, Loss: 0.030000\n",
            "Epoch [77/10000], iteration 60, Loss: 0.028167\n",
            "Epoch [77/10000], iteration 65, Loss: 0.027077\n",
            "Epoch [77/10000], iteration 70, Loss: 0.026429\n",
            "Epoch [77/10000], iteration 75, Loss: 0.031200\n",
            "Epoch [77/10000], iteration 80, Loss: 0.029750\n",
            "Epoch [77/10000], iteration 85, Loss: 0.029176\n",
            "Epoch [77/10000], iteration 90, Loss: 0.028444\n",
            "Epoch [77/10000], iteration 95, Loss: 0.027474\n",
            "Epoch [77/10000], iteration 100, Loss: 0.028300\n",
            "validating....\n",
            "Epoch 77, validation loss is 0.0348, training loss is 0.0283\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 58\n",
            "Epoch [78/10000], iteration 5, Loss: 0.008000\n",
            "Epoch [78/10000], iteration 10, Loss: 0.026000\n",
            "Epoch [78/10000], iteration 15, Loss: 0.022667\n",
            "Epoch [78/10000], iteration 20, Loss: 0.022000\n",
            "Epoch [78/10000], iteration 25, Loss: 0.024000\n",
            "Epoch [78/10000], iteration 30, Loss: 0.023667\n",
            "Epoch [78/10000], iteration 35, Loss: 0.021429\n",
            "Epoch [78/10000], iteration 40, Loss: 0.019500\n",
            "Epoch [78/10000], iteration 45, Loss: 0.017778\n",
            "Epoch [78/10000], iteration 50, Loss: 0.018400\n",
            "Epoch [78/10000], iteration 55, Loss: 0.019455\n",
            "Epoch [78/10000], iteration 60, Loss: 0.018333\n",
            "Epoch [78/10000], iteration 65, Loss: 0.019385\n",
            "Epoch [78/10000], iteration 70, Loss: 0.019714\n",
            "Epoch [78/10000], iteration 75, Loss: 0.020133\n",
            "Epoch [78/10000], iteration 80, Loss: 0.019750\n",
            "Epoch [78/10000], iteration 85, Loss: 0.019294\n",
            "Epoch [78/10000], iteration 90, Loss: 0.018778\n",
            "Epoch [78/10000], iteration 95, Loss: 0.019368\n",
            "Epoch [78/10000], iteration 100, Loss: 0.019200\n",
            "validating....\n",
            "Epoch 78, validation loss is 0.0383, training loss is 0.0192\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 59\n",
            "Epoch [79/10000], iteration 5, Loss: 0.042000\n",
            "Epoch [79/10000], iteration 10, Loss: 0.057000\n",
            "Epoch [79/10000], iteration 15, Loss: 0.046000\n",
            "Epoch [79/10000], iteration 20, Loss: 0.038000\n",
            "Epoch [79/10000], iteration 25, Loss: 0.032800\n",
            "Epoch [79/10000], iteration 30, Loss: 0.029333\n",
            "Epoch [79/10000], iteration 35, Loss: 0.030857\n",
            "Epoch [79/10000], iteration 40, Loss: 0.030250\n",
            "Epoch [79/10000], iteration 45, Loss: 0.037333\n",
            "Epoch [79/10000], iteration 50, Loss: 0.035200\n",
            "Epoch [79/10000], iteration 55, Loss: 0.033818\n",
            "Epoch [79/10000], iteration 60, Loss: 0.032167\n",
            "Epoch [79/10000], iteration 65, Loss: 0.030615\n",
            "Epoch [79/10000], iteration 70, Loss: 0.029857\n",
            "Epoch [79/10000], iteration 75, Loss: 0.028800\n",
            "Epoch [79/10000], iteration 80, Loss: 0.033000\n",
            "Epoch [79/10000], iteration 85, Loss: 0.036706\n",
            "Epoch [79/10000], iteration 90, Loss: 0.035333\n",
            "Epoch [79/10000], iteration 95, Loss: 0.035895\n",
            "Epoch [79/10000], iteration 100, Loss: 0.034900\n",
            "validating....\n",
            "Epoch 79, validation loss is 0.0415, training loss is 0.0349\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 60\n",
            "Epoch [80/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [80/10000], iteration 10, Loss: 0.012000\n",
            "Epoch [80/10000], iteration 15, Loss: 0.014667\n",
            "Epoch [80/10000], iteration 20, Loss: 0.015500\n",
            "Epoch [80/10000], iteration 25, Loss: 0.016000\n",
            "Epoch [80/10000], iteration 30, Loss: 0.018333\n",
            "Epoch [80/10000], iteration 35, Loss: 0.016286\n",
            "Epoch [80/10000], iteration 40, Loss: 0.016000\n",
            "Epoch [80/10000], iteration 45, Loss: 0.017333\n",
            "Epoch [80/10000], iteration 50, Loss: 0.017600\n",
            "Epoch [80/10000], iteration 55, Loss: 0.020000\n",
            "Epoch [80/10000], iteration 60, Loss: 0.020000\n",
            "Epoch [80/10000], iteration 65, Loss: 0.024769\n",
            "Epoch [80/10000], iteration 70, Loss: 0.023429\n",
            "Epoch [80/10000], iteration 75, Loss: 0.023867\n",
            "Epoch [80/10000], iteration 80, Loss: 0.023625\n",
            "Epoch [80/10000], iteration 85, Loss: 0.022941\n",
            "Epoch [80/10000], iteration 90, Loss: 0.022556\n",
            "Epoch [80/10000], iteration 95, Loss: 0.026211\n",
            "Epoch [80/10000], iteration 100, Loss: 0.027400\n",
            "validating....\n",
            "Epoch 80, validation loss is 0.0344, training loss is 0.0274\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 61\n",
            "Epoch [81/10000], iteration 5, Loss: 0.004000\n",
            "Epoch [81/10000], iteration 10, Loss: 0.010000\n",
            "Epoch [81/10000], iteration 15, Loss: 0.018667\n",
            "Epoch [81/10000], iteration 20, Loss: 0.032500\n",
            "Epoch [81/10000], iteration 25, Loss: 0.027600\n",
            "Epoch [81/10000], iteration 30, Loss: 0.024667\n",
            "Epoch [81/10000], iteration 35, Loss: 0.032571\n",
            "Epoch [81/10000], iteration 40, Loss: 0.038500\n",
            "Epoch [81/10000], iteration 45, Loss: 0.038889\n",
            "Epoch [81/10000], iteration 50, Loss: 0.038000\n",
            "Epoch [81/10000], iteration 55, Loss: 0.035636\n",
            "Epoch [81/10000], iteration 60, Loss: 0.034000\n",
            "Epoch [81/10000], iteration 65, Loss: 0.036769\n",
            "Epoch [81/10000], iteration 70, Loss: 0.036571\n",
            "Epoch [81/10000], iteration 75, Loss: 0.036133\n",
            "Epoch [81/10000], iteration 80, Loss: 0.036000\n",
            "Epoch [81/10000], iteration 85, Loss: 0.035176\n",
            "Epoch [81/10000], iteration 90, Loss: 0.035000\n",
            "Epoch [81/10000], iteration 95, Loss: 0.034421\n",
            "Epoch [81/10000], iteration 100, Loss: 0.033100\n",
            "validating....\n",
            "Epoch 81, validation loss is 0.0416, training loss is 0.0331\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 62\n",
            "Epoch [82/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [82/10000], iteration 10, Loss: 0.017000\n",
            "Epoch [82/10000], iteration 15, Loss: 0.018667\n",
            "Epoch [82/10000], iteration 20, Loss: 0.017500\n",
            "Epoch [82/10000], iteration 25, Loss: 0.018800\n",
            "Epoch [82/10000], iteration 30, Loss: 0.018333\n",
            "Epoch [82/10000], iteration 35, Loss: 0.018000\n",
            "Epoch [82/10000], iteration 40, Loss: 0.016250\n",
            "Epoch [82/10000], iteration 45, Loss: 0.018000\n",
            "Epoch [82/10000], iteration 50, Loss: 0.024000\n",
            "Epoch [82/10000], iteration 55, Loss: 0.024000\n",
            "Epoch [82/10000], iteration 60, Loss: 0.024833\n",
            "Epoch [82/10000], iteration 65, Loss: 0.024615\n",
            "Epoch [82/10000], iteration 70, Loss: 0.023714\n",
            "Epoch [82/10000], iteration 75, Loss: 0.031333\n",
            "Epoch [82/10000], iteration 80, Loss: 0.030500\n",
            "Epoch [82/10000], iteration 85, Loss: 0.030000\n",
            "Epoch [82/10000], iteration 90, Loss: 0.033889\n",
            "Epoch [82/10000], iteration 95, Loss: 0.033053\n",
            "Epoch [82/10000], iteration 100, Loss: 0.032500\n",
            "validating....\n",
            "Epoch 82, validation loss is 0.0376, training loss is 0.0325\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 63\n",
            "Epoch [83/10000], iteration 5, Loss: 0.074000\n",
            "Epoch [83/10000], iteration 10, Loss: 0.112000\n",
            "Epoch [83/10000], iteration 15, Loss: 0.080667\n",
            "Epoch [83/10000], iteration 20, Loss: 0.070000\n",
            "Epoch [83/10000], iteration 25, Loss: 0.059200\n",
            "Epoch [83/10000], iteration 30, Loss: 0.051000\n",
            "Epoch [83/10000], iteration 35, Loss: 0.046286\n",
            "Epoch [83/10000], iteration 40, Loss: 0.042250\n",
            "Epoch [83/10000], iteration 45, Loss: 0.039111\n",
            "Epoch [83/10000], iteration 50, Loss: 0.036600\n",
            "Epoch [83/10000], iteration 55, Loss: 0.034545\n",
            "Epoch [83/10000], iteration 60, Loss: 0.037500\n",
            "Epoch [83/10000], iteration 65, Loss: 0.036308\n",
            "Epoch [83/10000], iteration 70, Loss: 0.034857\n",
            "Epoch [83/10000], iteration 75, Loss: 0.033867\n",
            "Epoch [83/10000], iteration 80, Loss: 0.032375\n",
            "Epoch [83/10000], iteration 85, Loss: 0.030824\n",
            "Epoch [83/10000], iteration 90, Loss: 0.029667\n",
            "Epoch [83/10000], iteration 95, Loss: 0.029474\n",
            "Epoch [83/10000], iteration 100, Loss: 0.033700\n",
            "validating....\n",
            "Epoch 83, validation loss is 0.0340, training loss is 0.0337\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 64\n",
            "Epoch [84/10000], iteration 5, Loss: 0.022000\n",
            "Epoch [84/10000], iteration 10, Loss: 0.032000\n",
            "Epoch [84/10000], iteration 15, Loss: 0.028000\n",
            "Epoch [84/10000], iteration 20, Loss: 0.025500\n",
            "Epoch [84/10000], iteration 25, Loss: 0.034000\n",
            "Epoch [84/10000], iteration 30, Loss: 0.035000\n",
            "Epoch [84/10000], iteration 35, Loss: 0.036857\n",
            "Epoch [84/10000], iteration 40, Loss: 0.034000\n",
            "Epoch [84/10000], iteration 45, Loss: 0.033333\n",
            "Epoch [84/10000], iteration 50, Loss: 0.031400\n",
            "Epoch [84/10000], iteration 55, Loss: 0.029818\n",
            "Epoch [84/10000], iteration 60, Loss: 0.033667\n",
            "Epoch [84/10000], iteration 65, Loss: 0.033385\n",
            "Epoch [84/10000], iteration 70, Loss: 0.039143\n",
            "Epoch [84/10000], iteration 75, Loss: 0.037867\n",
            "Epoch [84/10000], iteration 80, Loss: 0.036250\n",
            "Epoch [84/10000], iteration 85, Loss: 0.038000\n",
            "Epoch [84/10000], iteration 90, Loss: 0.036333\n",
            "Epoch [84/10000], iteration 95, Loss: 0.035579\n",
            "Epoch [84/10000], iteration 100, Loss: 0.034800\n",
            "validating....\n",
            "Epoch 84, validation loss is 0.0409, training loss is 0.0348\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 65\n",
            "Epoch [85/10000], iteration 5, Loss: 0.010000\n",
            "Epoch [85/10000], iteration 10, Loss: 0.044000\n",
            "Epoch [85/10000], iteration 15, Loss: 0.037333\n",
            "Epoch [85/10000], iteration 20, Loss: 0.032000\n",
            "Epoch [85/10000], iteration 25, Loss: 0.041600\n",
            "Epoch [85/10000], iteration 30, Loss: 0.041667\n",
            "Epoch [85/10000], iteration 35, Loss: 0.038286\n",
            "Epoch [85/10000], iteration 40, Loss: 0.038750\n",
            "Epoch [85/10000], iteration 45, Loss: 0.035333\n",
            "Epoch [85/10000], iteration 50, Loss: 0.036200\n",
            "Epoch [85/10000], iteration 55, Loss: 0.035091\n",
            "Epoch [85/10000], iteration 60, Loss: 0.033833\n",
            "Epoch [85/10000], iteration 65, Loss: 0.037692\n",
            "Epoch [85/10000], iteration 70, Loss: 0.035857\n",
            "Epoch [85/10000], iteration 75, Loss: 0.036267\n",
            "Epoch [85/10000], iteration 80, Loss: 0.039250\n",
            "Epoch [85/10000], iteration 85, Loss: 0.037294\n",
            "Epoch [85/10000], iteration 90, Loss: 0.039222\n",
            "Epoch [85/10000], iteration 95, Loss: 0.041474\n",
            "Epoch [85/10000], iteration 100, Loss: 0.040000\n",
            "validating....\n",
            "Epoch 85, validation loss is 0.0377, training loss is 0.0400\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 66\n",
            "Epoch [86/10000], iteration 5, Loss: 0.024000\n",
            "Epoch [86/10000], iteration 10, Loss: 0.049000\n",
            "Epoch [86/10000], iteration 15, Loss: 0.042000\n",
            "Epoch [86/10000], iteration 20, Loss: 0.040000\n",
            "Epoch [86/10000], iteration 25, Loss: 0.034800\n",
            "Epoch [86/10000], iteration 30, Loss: 0.035667\n",
            "Epoch [86/10000], iteration 35, Loss: 0.044857\n",
            "Epoch [86/10000], iteration 40, Loss: 0.041250\n",
            "Epoch [86/10000], iteration 45, Loss: 0.039778\n",
            "Epoch [86/10000], iteration 50, Loss: 0.038200\n",
            "Epoch [86/10000], iteration 55, Loss: 0.036000\n",
            "Epoch [86/10000], iteration 60, Loss: 0.034000\n",
            "Epoch [86/10000], iteration 65, Loss: 0.033077\n",
            "Epoch [86/10000], iteration 70, Loss: 0.033571\n",
            "Epoch [86/10000], iteration 75, Loss: 0.035600\n",
            "Epoch [86/10000], iteration 80, Loss: 0.035250\n",
            "Epoch [86/10000], iteration 85, Loss: 0.033882\n",
            "Epoch [86/10000], iteration 90, Loss: 0.034000\n",
            "Epoch [86/10000], iteration 95, Loss: 0.032947\n",
            "Epoch [86/10000], iteration 100, Loss: 0.032600\n",
            "validating....\n",
            "Epoch 86, validation loss is 0.0359, training loss is 0.0326\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 67\n",
            "Epoch [87/10000], iteration 5, Loss: 0.014000\n",
            "Epoch [87/10000], iteration 10, Loss: 0.029000\n",
            "Epoch [87/10000], iteration 15, Loss: 0.028000\n",
            "Epoch [87/10000], iteration 20, Loss: 0.023000\n",
            "Epoch [87/10000], iteration 25, Loss: 0.020000\n",
            "Epoch [87/10000], iteration 30, Loss: 0.020000\n",
            "Epoch [87/10000], iteration 35, Loss: 0.018000\n",
            "Epoch [87/10000], iteration 40, Loss: 0.017250\n",
            "Epoch [87/10000], iteration 45, Loss: 0.017333\n",
            "Epoch [87/10000], iteration 50, Loss: 0.018800\n",
            "Epoch [87/10000], iteration 55, Loss: 0.018364\n",
            "Epoch [87/10000], iteration 60, Loss: 0.024333\n",
            "Epoch [87/10000], iteration 65, Loss: 0.023538\n",
            "Epoch [87/10000], iteration 70, Loss: 0.023143\n",
            "Epoch [87/10000], iteration 75, Loss: 0.023600\n",
            "Epoch [87/10000], iteration 80, Loss: 0.023875\n",
            "Epoch [87/10000], iteration 85, Loss: 0.024235\n",
            "Epoch [87/10000], iteration 90, Loss: 0.023444\n",
            "Epoch [87/10000], iteration 95, Loss: 0.024105\n",
            "Epoch [87/10000], iteration 100, Loss: 0.024100\n",
            "validating....\n",
            "Epoch 87, validation loss is 0.0386, training loss is 0.0241\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 68\n",
            "Epoch [88/10000], iteration 5, Loss: 0.014000\n",
            "Epoch [88/10000], iteration 10, Loss: 0.018000\n",
            "Epoch [88/10000], iteration 15, Loss: 0.018000\n",
            "Epoch [88/10000], iteration 20, Loss: 0.017500\n",
            "Epoch [88/10000], iteration 25, Loss: 0.030400\n",
            "Epoch [88/10000], iteration 30, Loss: 0.027000\n",
            "Epoch [88/10000], iteration 35, Loss: 0.041143\n",
            "Epoch [88/10000], iteration 40, Loss: 0.037750\n",
            "Epoch [88/10000], iteration 45, Loss: 0.036000\n",
            "Epoch [88/10000], iteration 50, Loss: 0.034000\n",
            "Epoch [88/10000], iteration 55, Loss: 0.031818\n",
            "Epoch [88/10000], iteration 60, Loss: 0.030500\n",
            "Epoch [88/10000], iteration 65, Loss: 0.029385\n",
            "Epoch [88/10000], iteration 70, Loss: 0.028286\n",
            "Epoch [88/10000], iteration 75, Loss: 0.030000\n",
            "Epoch [88/10000], iteration 80, Loss: 0.028875\n",
            "Epoch [88/10000], iteration 85, Loss: 0.028353\n",
            "Epoch [88/10000], iteration 90, Loss: 0.027778\n",
            "Epoch [88/10000], iteration 95, Loss: 0.027684\n",
            "Epoch [88/10000], iteration 100, Loss: 0.027000\n",
            "validating....\n",
            "Epoch 88, validation loss is 0.0326, training loss is 0.0270\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 69\n",
            "Epoch [89/10000], iteration 5, Loss: 0.056000\n",
            "Epoch [89/10000], iteration 10, Loss: 0.071000\n",
            "Epoch [89/10000], iteration 15, Loss: 0.052000\n",
            "Epoch [89/10000], iteration 20, Loss: 0.059000\n",
            "Epoch [89/10000], iteration 25, Loss: 0.051200\n",
            "Epoch [89/10000], iteration 30, Loss: 0.044667\n",
            "Epoch [89/10000], iteration 35, Loss: 0.040571\n",
            "Epoch [89/10000], iteration 40, Loss: 0.036500\n",
            "Epoch [89/10000], iteration 45, Loss: 0.035111\n",
            "Epoch [89/10000], iteration 50, Loss: 0.034200\n",
            "Epoch [89/10000], iteration 55, Loss: 0.032545\n",
            "Epoch [89/10000], iteration 60, Loss: 0.032000\n",
            "Epoch [89/10000], iteration 65, Loss: 0.031077\n",
            "Epoch [89/10000], iteration 70, Loss: 0.030286\n",
            "Epoch [89/10000], iteration 75, Loss: 0.029200\n",
            "Epoch [89/10000], iteration 80, Loss: 0.035875\n",
            "Epoch [89/10000], iteration 85, Loss: 0.034824\n",
            "Epoch [89/10000], iteration 90, Loss: 0.034667\n",
            "Epoch [89/10000], iteration 95, Loss: 0.033263\n",
            "Epoch [89/10000], iteration 100, Loss: 0.036300\n",
            "validating....\n",
            "Epoch 89, validation loss is 0.0366, training loss is 0.0363\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 70\n",
            "Epoch [90/10000], iteration 5, Loss: 0.084000\n",
            "Epoch [90/10000], iteration 10, Loss: 0.050000\n",
            "Epoch [90/10000], iteration 15, Loss: 0.038000\n",
            "Epoch [90/10000], iteration 20, Loss: 0.032000\n",
            "Epoch [90/10000], iteration 25, Loss: 0.027600\n",
            "Epoch [90/10000], iteration 30, Loss: 0.026000\n",
            "Epoch [90/10000], iteration 35, Loss: 0.033429\n",
            "Epoch [90/10000], iteration 40, Loss: 0.032750\n",
            "Epoch [90/10000], iteration 45, Loss: 0.032222\n",
            "Epoch [90/10000], iteration 50, Loss: 0.030400\n",
            "Epoch [90/10000], iteration 55, Loss: 0.028364\n",
            "Epoch [90/10000], iteration 60, Loss: 0.027167\n",
            "Epoch [90/10000], iteration 65, Loss: 0.030769\n",
            "Epoch [90/10000], iteration 70, Loss: 0.031286\n",
            "Epoch [90/10000], iteration 75, Loss: 0.030533\n",
            "Epoch [90/10000], iteration 80, Loss: 0.033000\n",
            "Epoch [90/10000], iteration 85, Loss: 0.033059\n",
            "Epoch [90/10000], iteration 90, Loss: 0.033778\n",
            "Epoch [90/10000], iteration 95, Loss: 0.032737\n",
            "Epoch [90/10000], iteration 100, Loss: 0.031800\n",
            "validating....\n",
            "Epoch 90, validation loss is 0.0411, training loss is 0.0318\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 71\n",
            "Epoch [91/10000], iteration 5, Loss: 0.024000\n",
            "Epoch [91/10000], iteration 10, Loss: 0.021000\n",
            "Epoch [91/10000], iteration 15, Loss: 0.020000\n",
            "Epoch [91/10000], iteration 20, Loss: 0.037000\n",
            "Epoch [91/10000], iteration 25, Loss: 0.031200\n",
            "Epoch [91/10000], iteration 30, Loss: 0.028667\n",
            "Epoch [91/10000], iteration 35, Loss: 0.028286\n",
            "Epoch [91/10000], iteration 40, Loss: 0.025250\n",
            "Epoch [91/10000], iteration 45, Loss: 0.027111\n",
            "Epoch [91/10000], iteration 50, Loss: 0.027000\n",
            "Epoch [91/10000], iteration 55, Loss: 0.025818\n",
            "Epoch [91/10000], iteration 60, Loss: 0.026667\n",
            "Epoch [91/10000], iteration 65, Loss: 0.030308\n",
            "Epoch [91/10000], iteration 70, Loss: 0.029429\n",
            "Epoch [91/10000], iteration 75, Loss: 0.028933\n",
            "Epoch [91/10000], iteration 80, Loss: 0.028125\n",
            "Epoch [91/10000], iteration 85, Loss: 0.027882\n",
            "Epoch [91/10000], iteration 90, Loss: 0.028111\n",
            "Epoch [91/10000], iteration 95, Loss: 0.027474\n",
            "Epoch [91/10000], iteration 100, Loss: 0.028400\n",
            "validating....\n",
            "Epoch 91, validation loss is 0.0353, training loss is 0.0284\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 72\n",
            "Epoch [92/10000], iteration 5, Loss: 0.030000\n",
            "Epoch [92/10000], iteration 10, Loss: 0.024000\n",
            "Epoch [92/10000], iteration 15, Loss: 0.022667\n",
            "Epoch [92/10000], iteration 20, Loss: 0.019500\n",
            "Epoch [92/10000], iteration 25, Loss: 0.023200\n",
            "Epoch [92/10000], iteration 30, Loss: 0.021333\n",
            "Epoch [92/10000], iteration 35, Loss: 0.021429\n",
            "Epoch [92/10000], iteration 40, Loss: 0.020750\n",
            "Epoch [92/10000], iteration 45, Loss: 0.026000\n",
            "Epoch [92/10000], iteration 50, Loss: 0.024000\n",
            "Epoch [92/10000], iteration 55, Loss: 0.022364\n",
            "Epoch [92/10000], iteration 60, Loss: 0.026833\n",
            "Epoch [92/10000], iteration 65, Loss: 0.026462\n",
            "Epoch [92/10000], iteration 70, Loss: 0.029286\n",
            "Epoch [92/10000], iteration 75, Loss: 0.028400\n",
            "Epoch [92/10000], iteration 80, Loss: 0.027625\n",
            "Epoch [92/10000], iteration 85, Loss: 0.031294\n",
            "Epoch [92/10000], iteration 90, Loss: 0.030000\n",
            "Epoch [92/10000], iteration 95, Loss: 0.029158\n",
            "Epoch [92/10000], iteration 100, Loss: 0.028400\n",
            "validating....\n",
            "Epoch 92, validation loss is 0.0365, training loss is 0.0284\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 73\n",
            "Epoch [93/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [93/10000], iteration 10, Loss: 0.051000\n",
            "Epoch [93/10000], iteration 15, Loss: 0.039333\n",
            "Epoch [93/10000], iteration 20, Loss: 0.033000\n",
            "Epoch [93/10000], iteration 25, Loss: 0.031200\n",
            "Epoch [93/10000], iteration 30, Loss: 0.038333\n",
            "Epoch [93/10000], iteration 35, Loss: 0.034286\n",
            "Epoch [93/10000], iteration 40, Loss: 0.033250\n",
            "Epoch [93/10000], iteration 45, Loss: 0.030444\n",
            "Epoch [93/10000], iteration 50, Loss: 0.028600\n",
            "Epoch [93/10000], iteration 55, Loss: 0.027455\n",
            "Epoch [93/10000], iteration 60, Loss: 0.027500\n",
            "Epoch [93/10000], iteration 65, Loss: 0.026308\n",
            "Epoch [93/10000], iteration 70, Loss: 0.025286\n",
            "Epoch [93/10000], iteration 75, Loss: 0.024667\n",
            "Epoch [93/10000], iteration 80, Loss: 0.024125\n",
            "Epoch [93/10000], iteration 85, Loss: 0.023529\n",
            "Epoch [93/10000], iteration 90, Loss: 0.026778\n",
            "Epoch [93/10000], iteration 95, Loss: 0.029895\n",
            "Epoch [93/10000], iteration 100, Loss: 0.029900\n",
            "validating....\n",
            "Epoch 93, validation loss is 0.0391, training loss is 0.0299\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 74\n",
            "Epoch [94/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [94/10000], iteration 10, Loss: 0.019000\n",
            "Epoch [94/10000], iteration 15, Loss: 0.018667\n",
            "Epoch [94/10000], iteration 20, Loss: 0.016500\n",
            "Epoch [94/10000], iteration 25, Loss: 0.016000\n",
            "Epoch [94/10000], iteration 30, Loss: 0.016333\n",
            "Epoch [94/10000], iteration 35, Loss: 0.015429\n",
            "Epoch [94/10000], iteration 40, Loss: 0.016750\n",
            "Epoch [94/10000], iteration 45, Loss: 0.017111\n",
            "Epoch [94/10000], iteration 50, Loss: 0.016800\n",
            "Epoch [94/10000], iteration 55, Loss: 0.018000\n",
            "Epoch [94/10000], iteration 60, Loss: 0.019000\n",
            "Epoch [94/10000], iteration 65, Loss: 0.023077\n",
            "Epoch [94/10000], iteration 70, Loss: 0.023000\n",
            "Epoch [94/10000], iteration 75, Loss: 0.022267\n",
            "Epoch [94/10000], iteration 80, Loss: 0.021625\n",
            "Epoch [94/10000], iteration 85, Loss: 0.020824\n",
            "Epoch [94/10000], iteration 90, Loss: 0.021222\n",
            "Epoch [94/10000], iteration 95, Loss: 0.021053\n",
            "Epoch [94/10000], iteration 100, Loss: 0.020200\n",
            "validating....\n",
            "Epoch 94, validation loss is 0.0413, training loss is 0.0202\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 75\n",
            "Epoch [95/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [95/10000], iteration 10, Loss: 0.013000\n",
            "Epoch [95/10000], iteration 15, Loss: 0.010667\n",
            "Epoch [95/10000], iteration 20, Loss: 0.011000\n",
            "Epoch [95/10000], iteration 25, Loss: 0.014800\n",
            "Epoch [95/10000], iteration 30, Loss: 0.017000\n",
            "Epoch [95/10000], iteration 35, Loss: 0.017429\n",
            "Epoch [95/10000], iteration 40, Loss: 0.017000\n",
            "Epoch [95/10000], iteration 45, Loss: 0.019111\n",
            "Epoch [95/10000], iteration 50, Loss: 0.018600\n",
            "Epoch [95/10000], iteration 55, Loss: 0.018000\n",
            "Epoch [95/10000], iteration 60, Loss: 0.020500\n",
            "Epoch [95/10000], iteration 65, Loss: 0.020000\n",
            "Epoch [95/10000], iteration 70, Loss: 0.019571\n",
            "Epoch [95/10000], iteration 75, Loss: 0.019600\n",
            "Epoch [95/10000], iteration 80, Loss: 0.018750\n",
            "Epoch [95/10000], iteration 85, Loss: 0.018471\n",
            "Epoch [95/10000], iteration 90, Loss: 0.020000\n",
            "Epoch [95/10000], iteration 95, Loss: 0.020316\n",
            "Epoch [95/10000], iteration 100, Loss: 0.020400\n",
            "validating....\n",
            "Epoch 95, validation loss is 0.0351, training loss is 0.0204\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 76\n",
            "Epoch [96/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [96/10000], iteration 10, Loss: 0.085000\n",
            "Epoch [96/10000], iteration 15, Loss: 0.063333\n",
            "Epoch [96/10000], iteration 20, Loss: 0.050500\n",
            "Epoch [96/10000], iteration 25, Loss: 0.043600\n",
            "Epoch [96/10000], iteration 30, Loss: 0.039333\n",
            "Epoch [96/10000], iteration 35, Loss: 0.036286\n",
            "Epoch [96/10000], iteration 40, Loss: 0.034750\n",
            "Epoch [96/10000], iteration 45, Loss: 0.045556\n",
            "Epoch [96/10000], iteration 50, Loss: 0.043400\n",
            "Epoch [96/10000], iteration 55, Loss: 0.040727\n",
            "Epoch [96/10000], iteration 60, Loss: 0.038500\n",
            "Epoch [96/10000], iteration 65, Loss: 0.037692\n",
            "Epoch [96/10000], iteration 70, Loss: 0.040000\n",
            "Epoch [96/10000], iteration 75, Loss: 0.038800\n",
            "Epoch [96/10000], iteration 80, Loss: 0.038625\n",
            "Epoch [96/10000], iteration 85, Loss: 0.038471\n",
            "Epoch [96/10000], iteration 90, Loss: 0.036889\n",
            "Epoch [96/10000], iteration 95, Loss: 0.035895\n",
            "Epoch [96/10000], iteration 100, Loss: 0.035300\n",
            "validating....\n",
            "Epoch 96, validation loss is 0.0399, training loss is 0.0353\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 77\n",
            "Epoch [97/10000], iteration 5, Loss: 0.036000\n",
            "Epoch [97/10000], iteration 10, Loss: 0.029000\n",
            "Epoch [97/10000], iteration 15, Loss: 0.022667\n",
            "Epoch [97/10000], iteration 20, Loss: 0.020000\n",
            "Epoch [97/10000], iteration 25, Loss: 0.036000\n",
            "Epoch [97/10000], iteration 30, Loss: 0.036333\n",
            "Epoch [97/10000], iteration 35, Loss: 0.036000\n",
            "Epoch [97/10000], iteration 40, Loss: 0.043250\n",
            "Epoch [97/10000], iteration 45, Loss: 0.040444\n",
            "Epoch [97/10000], iteration 50, Loss: 0.037000\n",
            "Epoch [97/10000], iteration 55, Loss: 0.039273\n",
            "Epoch [97/10000], iteration 60, Loss: 0.043167\n",
            "Epoch [97/10000], iteration 65, Loss: 0.041692\n",
            "Epoch [97/10000], iteration 70, Loss: 0.039857\n",
            "Epoch [97/10000], iteration 75, Loss: 0.039067\n",
            "Epoch [97/10000], iteration 80, Loss: 0.037125\n",
            "Epoch [97/10000], iteration 85, Loss: 0.036588\n",
            "Epoch [97/10000], iteration 90, Loss: 0.035000\n",
            "Epoch [97/10000], iteration 95, Loss: 0.033895\n",
            "Epoch [97/10000], iteration 100, Loss: 0.032600\n",
            "validating....\n",
            "Epoch 97, validation loss is 0.0421, training loss is 0.0326\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 78\n",
            "Epoch [98/10000], iteration 5, Loss: 0.010000\n",
            "Epoch [98/10000], iteration 10, Loss: 0.040000\n",
            "Epoch [98/10000], iteration 15, Loss: 0.032000\n",
            "Epoch [98/10000], iteration 20, Loss: 0.029500\n",
            "Epoch [98/10000], iteration 25, Loss: 0.034400\n",
            "Epoch [98/10000], iteration 30, Loss: 0.030000\n",
            "Epoch [98/10000], iteration 35, Loss: 0.027429\n",
            "Epoch [98/10000], iteration 40, Loss: 0.026000\n",
            "Epoch [98/10000], iteration 45, Loss: 0.026889\n",
            "Epoch [98/10000], iteration 50, Loss: 0.026000\n",
            "Epoch [98/10000], iteration 55, Loss: 0.030000\n",
            "Epoch [98/10000], iteration 60, Loss: 0.029167\n",
            "Epoch [98/10000], iteration 65, Loss: 0.028462\n",
            "Epoch [98/10000], iteration 70, Loss: 0.027571\n",
            "Epoch [98/10000], iteration 75, Loss: 0.028000\n",
            "Epoch [98/10000], iteration 80, Loss: 0.027500\n",
            "Epoch [98/10000], iteration 85, Loss: 0.027059\n",
            "Epoch [98/10000], iteration 90, Loss: 0.027111\n",
            "Epoch [98/10000], iteration 95, Loss: 0.027474\n",
            "Epoch [98/10000], iteration 100, Loss: 0.029900\n",
            "validating....\n",
            "Epoch 98, validation loss is 0.0358, training loss is 0.0299\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 79\n",
            "Epoch [99/10000], iteration 5, Loss: 0.010000\n",
            "Epoch [99/10000], iteration 10, Loss: 0.055000\n",
            "Epoch [99/10000], iteration 15, Loss: 0.042667\n",
            "Epoch [99/10000], iteration 20, Loss: 0.050000\n",
            "Epoch [99/10000], iteration 25, Loss: 0.044800\n",
            "Epoch [99/10000], iteration 30, Loss: 0.038333\n",
            "Epoch [99/10000], iteration 35, Loss: 0.038571\n",
            "Epoch [99/10000], iteration 40, Loss: 0.035500\n",
            "Epoch [99/10000], iteration 45, Loss: 0.032889\n",
            "Epoch [99/10000], iteration 50, Loss: 0.032200\n",
            "Epoch [99/10000], iteration 55, Loss: 0.035273\n",
            "Epoch [99/10000], iteration 60, Loss: 0.035500\n",
            "Epoch [99/10000], iteration 65, Loss: 0.034154\n",
            "Epoch [99/10000], iteration 70, Loss: 0.032429\n",
            "Epoch [99/10000], iteration 75, Loss: 0.032267\n",
            "Epoch [99/10000], iteration 80, Loss: 0.031750\n",
            "Epoch [99/10000], iteration 85, Loss: 0.031176\n",
            "Epoch [99/10000], iteration 90, Loss: 0.030667\n",
            "Epoch [99/10000], iteration 95, Loss: 0.030211\n",
            "Epoch [99/10000], iteration 100, Loss: 0.029700\n",
            "validating....\n",
            "Epoch 99, validation loss is 0.0394, training loss is 0.0297\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 80\n",
            "Epoch [100/10000], iteration 5, Loss: 0.012000\n",
            "Epoch [100/10000], iteration 10, Loss: 0.010000\n",
            "Epoch [100/10000], iteration 15, Loss: 0.012000\n",
            "Epoch [100/10000], iteration 20, Loss: 0.023500\n",
            "Epoch [100/10000], iteration 25, Loss: 0.044800\n",
            "Epoch [100/10000], iteration 30, Loss: 0.039000\n",
            "Epoch [100/10000], iteration 35, Loss: 0.034571\n",
            "Epoch [100/10000], iteration 40, Loss: 0.033250\n",
            "Epoch [100/10000], iteration 45, Loss: 0.032667\n",
            "Epoch [100/10000], iteration 50, Loss: 0.033600\n",
            "Epoch [100/10000], iteration 55, Loss: 0.034545\n",
            "Epoch [100/10000], iteration 60, Loss: 0.038833\n",
            "Epoch [100/10000], iteration 65, Loss: 0.036923\n",
            "Epoch [100/10000], iteration 70, Loss: 0.034714\n",
            "Epoch [100/10000], iteration 75, Loss: 0.033067\n",
            "Epoch [100/10000], iteration 80, Loss: 0.032125\n",
            "Epoch [100/10000], iteration 85, Loss: 0.033647\n",
            "Epoch [100/10000], iteration 90, Loss: 0.033000\n",
            "Epoch [100/10000], iteration 95, Loss: 0.034947\n",
            "Epoch [100/10000], iteration 100, Loss: 0.033600\n",
            "validating....\n",
            "Epoch 100, validation loss is 0.0353, training loss is 0.0336\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 81\n",
            "Epoch [101/10000], iteration 5, Loss: 0.016000\n",
            "Epoch [101/10000], iteration 10, Loss: 0.020000\n",
            "Epoch [101/10000], iteration 15, Loss: 0.018000\n",
            "Epoch [101/10000], iteration 20, Loss: 0.016000\n",
            "Epoch [101/10000], iteration 25, Loss: 0.020400\n",
            "Epoch [101/10000], iteration 30, Loss: 0.019667\n",
            "Epoch [101/10000], iteration 35, Loss: 0.019143\n",
            "Epoch [101/10000], iteration 40, Loss: 0.018500\n",
            "Epoch [101/10000], iteration 45, Loss: 0.018000\n",
            "Epoch [101/10000], iteration 50, Loss: 0.017200\n",
            "Epoch [101/10000], iteration 55, Loss: 0.016545\n",
            "Epoch [101/10000], iteration 60, Loss: 0.015833\n",
            "Epoch [101/10000], iteration 65, Loss: 0.015385\n",
            "Epoch [101/10000], iteration 70, Loss: 0.015143\n",
            "Epoch [101/10000], iteration 75, Loss: 0.018533\n",
            "Epoch [101/10000], iteration 80, Loss: 0.018500\n",
            "Epoch [101/10000], iteration 85, Loss: 0.021647\n",
            "Epoch [101/10000], iteration 90, Loss: 0.021333\n",
            "Epoch [101/10000], iteration 95, Loss: 0.020737\n",
            "Epoch [101/10000], iteration 100, Loss: 0.020700\n",
            "validating....\n",
            "Epoch 101, validation loss is 0.0343, training loss is 0.0207\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 82\n",
            "Epoch [102/10000], iteration 5, Loss: 0.092000\n",
            "Epoch [102/10000], iteration 10, Loss: 0.071000\n",
            "Epoch [102/10000], iteration 15, Loss: 0.059333\n",
            "Epoch [102/10000], iteration 20, Loss: 0.065000\n",
            "Epoch [102/10000], iteration 25, Loss: 0.058800\n",
            "Epoch [102/10000], iteration 30, Loss: 0.050333\n",
            "Epoch [102/10000], iteration 35, Loss: 0.045143\n",
            "Epoch [102/10000], iteration 40, Loss: 0.040500\n",
            "Epoch [102/10000], iteration 45, Loss: 0.037778\n",
            "Epoch [102/10000], iteration 50, Loss: 0.035000\n",
            "Epoch [102/10000], iteration 55, Loss: 0.039091\n",
            "Epoch [102/10000], iteration 60, Loss: 0.039333\n",
            "Epoch [102/10000], iteration 65, Loss: 0.041846\n",
            "Epoch [102/10000], iteration 70, Loss: 0.039714\n",
            "Epoch [102/10000], iteration 75, Loss: 0.040133\n",
            "Epoch [102/10000], iteration 80, Loss: 0.039750\n",
            "Epoch [102/10000], iteration 85, Loss: 0.039176\n",
            "Epoch [102/10000], iteration 90, Loss: 0.039444\n",
            "Epoch [102/10000], iteration 95, Loss: 0.040737\n",
            "Epoch [102/10000], iteration 100, Loss: 0.039400\n",
            "validating....\n",
            "Epoch 102, validation loss is 0.0351, training loss is 0.0394\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 83\n",
            "Epoch [103/10000], iteration 5, Loss: 0.046000\n",
            "Epoch [103/10000], iteration 10, Loss: 0.035000\n",
            "Epoch [103/10000], iteration 15, Loss: 0.052000\n",
            "Epoch [103/10000], iteration 20, Loss: 0.044500\n",
            "Epoch [103/10000], iteration 25, Loss: 0.038000\n",
            "Epoch [103/10000], iteration 30, Loss: 0.034667\n",
            "Epoch [103/10000], iteration 35, Loss: 0.032571\n",
            "Epoch [103/10000], iteration 40, Loss: 0.030750\n",
            "Epoch [103/10000], iteration 45, Loss: 0.028889\n",
            "Epoch [103/10000], iteration 50, Loss: 0.028600\n",
            "Epoch [103/10000], iteration 55, Loss: 0.026909\n",
            "Epoch [103/10000], iteration 60, Loss: 0.028000\n",
            "Epoch [103/10000], iteration 65, Loss: 0.027385\n",
            "Epoch [103/10000], iteration 70, Loss: 0.026000\n",
            "Epoch [103/10000], iteration 75, Loss: 0.026533\n",
            "Epoch [103/10000], iteration 80, Loss: 0.026125\n",
            "Epoch [103/10000], iteration 85, Loss: 0.025529\n",
            "Epoch [103/10000], iteration 90, Loss: 0.025000\n",
            "Epoch [103/10000], iteration 95, Loss: 0.024632\n",
            "Epoch [103/10000], iteration 100, Loss: 0.023800\n",
            "validating....\n",
            "Epoch 103, validation loss is 0.0306, training loss is 0.0238\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 84\n",
            "Epoch [104/10000], iteration 5, Loss: 0.014000\n",
            "Epoch [104/10000], iteration 10, Loss: 0.014000\n",
            "Epoch [104/10000], iteration 15, Loss: 0.014000\n",
            "Epoch [104/10000], iteration 20, Loss: 0.033500\n",
            "Epoch [104/10000], iteration 25, Loss: 0.042800\n",
            "Epoch [104/10000], iteration 30, Loss: 0.037667\n",
            "Epoch [104/10000], iteration 35, Loss: 0.036286\n",
            "Epoch [104/10000], iteration 40, Loss: 0.033250\n",
            "Epoch [104/10000], iteration 45, Loss: 0.032222\n",
            "Epoch [104/10000], iteration 50, Loss: 0.036400\n",
            "Epoch [104/10000], iteration 55, Loss: 0.034727\n",
            "Epoch [104/10000], iteration 60, Loss: 0.033833\n",
            "Epoch [104/10000], iteration 65, Loss: 0.032308\n",
            "Epoch [104/10000], iteration 70, Loss: 0.031571\n",
            "Epoch [104/10000], iteration 75, Loss: 0.031333\n",
            "Epoch [104/10000], iteration 80, Loss: 0.034000\n",
            "Epoch [104/10000], iteration 85, Loss: 0.033294\n",
            "Epoch [104/10000], iteration 90, Loss: 0.032556\n",
            "Epoch [104/10000], iteration 95, Loss: 0.034737\n",
            "Epoch [104/10000], iteration 100, Loss: 0.033900\n",
            "validating....\n",
            "Epoch 104, validation loss is 0.0352, training loss is 0.0339\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 85\n",
            "Epoch [105/10000], iteration 5, Loss: 0.008000\n",
            "Epoch [105/10000], iteration 10, Loss: 0.010000\n",
            "Epoch [105/10000], iteration 15, Loss: 0.010000\n",
            "Epoch [105/10000], iteration 20, Loss: 0.013000\n",
            "Epoch [105/10000], iteration 25, Loss: 0.014800\n",
            "Epoch [105/10000], iteration 30, Loss: 0.015333\n",
            "Epoch [105/10000], iteration 35, Loss: 0.015143\n",
            "Epoch [105/10000], iteration 40, Loss: 0.016000\n",
            "Epoch [105/10000], iteration 45, Loss: 0.016000\n",
            "Epoch [105/10000], iteration 50, Loss: 0.022800\n",
            "Epoch [105/10000], iteration 55, Loss: 0.021636\n",
            "Epoch [105/10000], iteration 60, Loss: 0.022000\n",
            "Epoch [105/10000], iteration 65, Loss: 0.020923\n",
            "Epoch [105/10000], iteration 70, Loss: 0.021429\n",
            "Epoch [105/10000], iteration 75, Loss: 0.021333\n",
            "Epoch [105/10000], iteration 80, Loss: 0.022625\n",
            "Epoch [105/10000], iteration 85, Loss: 0.022000\n",
            "Epoch [105/10000], iteration 90, Loss: 0.025111\n",
            "Epoch [105/10000], iteration 95, Loss: 0.024842\n",
            "Epoch [105/10000], iteration 100, Loss: 0.027100\n",
            "validating....\n",
            "Epoch 105, validation loss is 0.0357, training loss is 0.0271\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 86\n",
            "Epoch [106/10000], iteration 5, Loss: 0.024000\n",
            "Epoch [106/10000], iteration 10, Loss: 0.024000\n",
            "Epoch [106/10000], iteration 15, Loss: 0.019333\n",
            "Epoch [106/10000], iteration 20, Loss: 0.019500\n",
            "Epoch [106/10000], iteration 25, Loss: 0.018800\n",
            "Epoch [106/10000], iteration 30, Loss: 0.030667\n",
            "Epoch [106/10000], iteration 35, Loss: 0.031714\n",
            "Epoch [106/10000], iteration 40, Loss: 0.030000\n",
            "Epoch [106/10000], iteration 45, Loss: 0.028222\n",
            "Epoch [106/10000], iteration 50, Loss: 0.034800\n",
            "Epoch [106/10000], iteration 55, Loss: 0.032909\n",
            "Epoch [106/10000], iteration 60, Loss: 0.032333\n",
            "Epoch [106/10000], iteration 65, Loss: 0.032615\n",
            "Epoch [106/10000], iteration 70, Loss: 0.031429\n",
            "Epoch [106/10000], iteration 75, Loss: 0.034400\n",
            "Epoch [106/10000], iteration 80, Loss: 0.034250\n",
            "Epoch [106/10000], iteration 85, Loss: 0.040353\n",
            "Epoch [106/10000], iteration 90, Loss: 0.039667\n",
            "Epoch [106/10000], iteration 95, Loss: 0.038526\n",
            "Epoch [106/10000], iteration 100, Loss: 0.038200\n",
            "validating....\n",
            "Epoch 106, validation loss is 0.0336, training loss is 0.0382\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 87\n",
            "Epoch [107/10000], iteration 5, Loss: 0.020000\n",
            "Epoch [107/10000], iteration 10, Loss: 0.022000\n",
            "Epoch [107/10000], iteration 15, Loss: 0.020000\n",
            "Epoch [107/10000], iteration 20, Loss: 0.047000\n",
            "Epoch [107/10000], iteration 25, Loss: 0.043600\n",
            "Epoch [107/10000], iteration 30, Loss: 0.038000\n",
            "Epoch [107/10000], iteration 35, Loss: 0.033714\n",
            "Epoch [107/10000], iteration 40, Loss: 0.031750\n",
            "Epoch [107/10000], iteration 45, Loss: 0.030000\n",
            "Epoch [107/10000], iteration 50, Loss: 0.029400\n",
            "Epoch [107/10000], iteration 55, Loss: 0.030727\n",
            "Epoch [107/10000], iteration 60, Loss: 0.028667\n",
            "Epoch [107/10000], iteration 65, Loss: 0.032154\n",
            "Epoch [107/10000], iteration 70, Loss: 0.031429\n",
            "Epoch [107/10000], iteration 75, Loss: 0.031600\n",
            "Epoch [107/10000], iteration 80, Loss: 0.031500\n",
            "Epoch [107/10000], iteration 85, Loss: 0.031647\n",
            "Epoch [107/10000], iteration 90, Loss: 0.030889\n",
            "Epoch [107/10000], iteration 95, Loss: 0.030842\n",
            "Epoch [107/10000], iteration 100, Loss: 0.032600\n",
            "validating....\n",
            "Epoch 107, validation loss is 0.0373, training loss is 0.0326\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 88\n",
            "Epoch [108/10000], iteration 5, Loss: 0.026000\n",
            "Epoch [108/10000], iteration 10, Loss: 0.048000\n",
            "Epoch [108/10000], iteration 15, Loss: 0.038667\n",
            "Epoch [108/10000], iteration 20, Loss: 0.052000\n",
            "Epoch [108/10000], iteration 25, Loss: 0.044400\n",
            "Epoch [108/10000], iteration 30, Loss: 0.038667\n",
            "Epoch [108/10000], iteration 35, Loss: 0.035429\n",
            "Epoch [108/10000], iteration 40, Loss: 0.039250\n",
            "Epoch [108/10000], iteration 45, Loss: 0.037111\n",
            "Epoch [108/10000], iteration 50, Loss: 0.034200\n",
            "Epoch [108/10000], iteration 55, Loss: 0.036909\n",
            "Epoch [108/10000], iteration 60, Loss: 0.034500\n",
            "Epoch [108/10000], iteration 65, Loss: 0.032615\n",
            "Epoch [108/10000], iteration 70, Loss: 0.037143\n",
            "Epoch [108/10000], iteration 75, Loss: 0.035600\n",
            "Epoch [108/10000], iteration 80, Loss: 0.034500\n",
            "Epoch [108/10000], iteration 85, Loss: 0.033176\n",
            "Epoch [108/10000], iteration 90, Loss: 0.032222\n",
            "Epoch [108/10000], iteration 95, Loss: 0.031579\n",
            "Epoch [108/10000], iteration 100, Loss: 0.031200\n",
            "validating....\n",
            "Epoch 108, validation loss is 0.0344, training loss is 0.0312\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 89\n",
            "Epoch [109/10000], iteration 5, Loss: 0.070000\n",
            "Epoch [109/10000], iteration 10, Loss: 0.043000\n",
            "Epoch [109/10000], iteration 15, Loss: 0.049333\n",
            "Epoch [109/10000], iteration 20, Loss: 0.044000\n",
            "Epoch [109/10000], iteration 25, Loss: 0.038000\n",
            "Epoch [109/10000], iteration 30, Loss: 0.051000\n",
            "Epoch [109/10000], iteration 35, Loss: 0.045429\n",
            "Epoch [109/10000], iteration 40, Loss: 0.043250\n",
            "Epoch [109/10000], iteration 45, Loss: 0.041778\n",
            "Epoch [109/10000], iteration 50, Loss: 0.039400\n",
            "Epoch [109/10000], iteration 55, Loss: 0.040909\n",
            "Epoch [109/10000], iteration 60, Loss: 0.039833\n",
            "Epoch [109/10000], iteration 65, Loss: 0.039846\n",
            "Epoch [109/10000], iteration 70, Loss: 0.041857\n",
            "Epoch [109/10000], iteration 75, Loss: 0.040133\n",
            "Epoch [109/10000], iteration 80, Loss: 0.038750\n",
            "Epoch [109/10000], iteration 85, Loss: 0.037059\n",
            "Epoch [109/10000], iteration 90, Loss: 0.036333\n",
            "Epoch [109/10000], iteration 95, Loss: 0.035053\n",
            "Epoch [109/10000], iteration 100, Loss: 0.034700\n",
            "validating....\n",
            "Epoch 109, validation loss is 0.0364, training loss is 0.0347\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 90\n",
            "Epoch [110/10000], iteration 5, Loss: 0.062000\n",
            "Epoch [110/10000], iteration 10, Loss: 0.034000\n",
            "Epoch [110/10000], iteration 15, Loss: 0.026000\n",
            "Epoch [110/10000], iteration 20, Loss: 0.023000\n",
            "Epoch [110/10000], iteration 25, Loss: 0.023600\n",
            "Epoch [110/10000], iteration 30, Loss: 0.024333\n",
            "Epoch [110/10000], iteration 35, Loss: 0.022857\n",
            "Epoch [110/10000], iteration 40, Loss: 0.022500\n",
            "Epoch [110/10000], iteration 45, Loss: 0.027333\n",
            "Epoch [110/10000], iteration 50, Loss: 0.026600\n",
            "Epoch [110/10000], iteration 55, Loss: 0.026909\n",
            "Epoch [110/10000], iteration 60, Loss: 0.026333\n",
            "Epoch [110/10000], iteration 65, Loss: 0.031231\n",
            "Epoch [110/10000], iteration 70, Loss: 0.030286\n",
            "Epoch [110/10000], iteration 75, Loss: 0.030000\n",
            "Epoch [110/10000], iteration 80, Loss: 0.029000\n",
            "Epoch [110/10000], iteration 85, Loss: 0.028353\n",
            "Epoch [110/10000], iteration 90, Loss: 0.028111\n",
            "Epoch [110/10000], iteration 95, Loss: 0.027263\n",
            "Epoch [110/10000], iteration 100, Loss: 0.027100\n",
            "validating....\n",
            "Epoch 110, validation loss is 0.0423, training loss is 0.0271\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 91\n",
            "Epoch [111/10000], iteration 5, Loss: 0.018000\n",
            "Epoch [111/10000], iteration 10, Loss: 0.018000\n",
            "Epoch [111/10000], iteration 15, Loss: 0.024000\n",
            "Epoch [111/10000], iteration 20, Loss: 0.020500\n",
            "Epoch [111/10000], iteration 25, Loss: 0.019600\n",
            "Epoch [111/10000], iteration 30, Loss: 0.020000\n",
            "Epoch [111/10000], iteration 35, Loss: 0.020000\n",
            "Epoch [111/10000], iteration 40, Loss: 0.022000\n",
            "Epoch [111/10000], iteration 45, Loss: 0.020444\n",
            "Epoch [111/10000], iteration 50, Loss: 0.019400\n",
            "Epoch [111/10000], iteration 55, Loss: 0.019273\n",
            "Epoch [111/10000], iteration 60, Loss: 0.019000\n",
            "Epoch [111/10000], iteration 65, Loss: 0.018154\n",
            "Epoch [111/10000], iteration 70, Loss: 0.018000\n",
            "Epoch [111/10000], iteration 75, Loss: 0.019867\n",
            "Epoch [111/10000], iteration 80, Loss: 0.020250\n",
            "Epoch [111/10000], iteration 85, Loss: 0.020471\n",
            "Epoch [111/10000], iteration 90, Loss: 0.020000\n",
            "Epoch [111/10000], iteration 95, Loss: 0.022421\n",
            "Epoch [111/10000], iteration 100, Loss: 0.024600\n",
            "validating....\n",
            "Epoch 111, validation loss is 0.0339, training loss is 0.0246\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 92\n",
            "Epoch [112/10000], iteration 5, Loss: 0.036000\n",
            "Epoch [112/10000], iteration 10, Loss: 0.036000\n",
            "Epoch [112/10000], iteration 15, Loss: 0.036000\n",
            "Epoch [112/10000], iteration 20, Loss: 0.032500\n",
            "Epoch [112/10000], iteration 25, Loss: 0.032000\n",
            "Epoch [112/10000], iteration 30, Loss: 0.030333\n",
            "Epoch [112/10000], iteration 35, Loss: 0.028000\n",
            "Epoch [112/10000], iteration 40, Loss: 0.027250\n",
            "Epoch [112/10000], iteration 45, Loss: 0.026444\n",
            "Epoch [112/10000], iteration 50, Loss: 0.028000\n",
            "Epoch [112/10000], iteration 55, Loss: 0.032364\n",
            "Epoch [112/10000], iteration 60, Loss: 0.030833\n",
            "Epoch [112/10000], iteration 65, Loss: 0.035692\n",
            "Epoch [112/10000], iteration 70, Loss: 0.034000\n",
            "Epoch [112/10000], iteration 75, Loss: 0.032133\n",
            "Epoch [112/10000], iteration 80, Loss: 0.032375\n",
            "Epoch [112/10000], iteration 85, Loss: 0.030588\n",
            "Epoch [112/10000], iteration 90, Loss: 0.030444\n",
            "Epoch [112/10000], iteration 95, Loss: 0.034211\n",
            "Epoch [112/10000], iteration 100, Loss: 0.035800\n",
            "validating....\n",
            "Epoch 112, validation loss is 0.0364, training loss is 0.0358\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 93\n",
            "Epoch [113/10000], iteration 5, Loss: 0.090000\n",
            "Epoch [113/10000], iteration 10, Loss: 0.062000\n",
            "Epoch [113/10000], iteration 15, Loss: 0.072667\n",
            "Epoch [113/10000], iteration 20, Loss: 0.060000\n",
            "Epoch [113/10000], iteration 25, Loss: 0.052400\n",
            "Epoch [113/10000], iteration 30, Loss: 0.058000\n",
            "Epoch [113/10000], iteration 35, Loss: 0.056000\n",
            "Epoch [113/10000], iteration 40, Loss: 0.049750\n",
            "Epoch [113/10000], iteration 45, Loss: 0.045556\n",
            "Epoch [113/10000], iteration 50, Loss: 0.043600\n",
            "Epoch [113/10000], iteration 55, Loss: 0.040545\n",
            "Epoch [113/10000], iteration 60, Loss: 0.038167\n",
            "Epoch [113/10000], iteration 65, Loss: 0.037538\n",
            "Epoch [113/10000], iteration 70, Loss: 0.035000\n",
            "Epoch [113/10000], iteration 75, Loss: 0.037733\n",
            "Epoch [113/10000], iteration 80, Loss: 0.036750\n",
            "Epoch [113/10000], iteration 85, Loss: 0.035647\n",
            "Epoch [113/10000], iteration 90, Loss: 0.034667\n",
            "Epoch [113/10000], iteration 95, Loss: 0.033684\n",
            "Epoch [113/10000], iteration 100, Loss: 0.032700\n",
            "validating....\n",
            "Epoch 113, validation loss is 0.0422, training loss is 0.0327\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 94\n",
            "Epoch [114/10000], iteration 5, Loss: 0.064000\n",
            "Epoch [114/10000], iteration 10, Loss: 0.038000\n",
            "Epoch [114/10000], iteration 15, Loss: 0.030667\n",
            "Epoch [114/10000], iteration 20, Loss: 0.029000\n",
            "Epoch [114/10000], iteration 25, Loss: 0.035600\n",
            "Epoch [114/10000], iteration 30, Loss: 0.045000\n",
            "Epoch [114/10000], iteration 35, Loss: 0.039429\n",
            "Epoch [114/10000], iteration 40, Loss: 0.036500\n",
            "Epoch [114/10000], iteration 45, Loss: 0.036444\n",
            "Epoch [114/10000], iteration 50, Loss: 0.034000\n",
            "Epoch [114/10000], iteration 55, Loss: 0.032182\n",
            "Epoch [114/10000], iteration 60, Loss: 0.031333\n",
            "Epoch [114/10000], iteration 65, Loss: 0.030308\n",
            "Epoch [114/10000], iteration 70, Loss: 0.029143\n",
            "Epoch [114/10000], iteration 75, Loss: 0.032267\n",
            "Epoch [114/10000], iteration 80, Loss: 0.031375\n",
            "Epoch [114/10000], iteration 85, Loss: 0.030588\n",
            "Epoch [114/10000], iteration 90, Loss: 0.030667\n",
            "Epoch [114/10000], iteration 95, Loss: 0.033368\n",
            "Epoch [114/10000], iteration 100, Loss: 0.032100\n",
            "validating....\n",
            "Epoch 114, validation loss is 0.0309, training loss is 0.0321\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 95\n",
            "Epoch [115/10000], iteration 5, Loss: 0.040000\n",
            "Epoch [115/10000], iteration 10, Loss: 0.025000\n",
            "Epoch [115/10000], iteration 15, Loss: 0.040000\n",
            "Epoch [115/10000], iteration 20, Loss: 0.033000\n",
            "Epoch [115/10000], iteration 25, Loss: 0.028000\n",
            "Epoch [115/10000], iteration 30, Loss: 0.034000\n",
            "Epoch [115/10000], iteration 35, Loss: 0.031429\n",
            "Epoch [115/10000], iteration 40, Loss: 0.037000\n",
            "Epoch [115/10000], iteration 45, Loss: 0.035111\n",
            "Epoch [115/10000], iteration 50, Loss: 0.034400\n",
            "Epoch [115/10000], iteration 55, Loss: 0.032545\n",
            "Epoch [115/10000], iteration 60, Loss: 0.030167\n",
            "Epoch [115/10000], iteration 65, Loss: 0.032154\n",
            "Epoch [115/10000], iteration 70, Loss: 0.032286\n",
            "Epoch [115/10000], iteration 75, Loss: 0.031733\n",
            "Epoch [115/10000], iteration 80, Loss: 0.032750\n",
            "Epoch [115/10000], iteration 85, Loss: 0.031647\n",
            "Epoch [115/10000], iteration 90, Loss: 0.034222\n",
            "Epoch [115/10000], iteration 95, Loss: 0.032737\n",
            "Epoch [115/10000], iteration 100, Loss: 0.031800\n",
            "validating....\n",
            "Epoch 115, validation loss is 0.0334, training loss is 0.0318\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 96\n",
            "Epoch [116/10000], iteration 5, Loss: 0.078000\n",
            "Epoch [116/10000], iteration 10, Loss: 0.048000\n",
            "Epoch [116/10000], iteration 15, Loss: 0.032667\n",
            "Epoch [116/10000], iteration 20, Loss: 0.029500\n",
            "Epoch [116/10000], iteration 25, Loss: 0.026400\n",
            "Epoch [116/10000], iteration 30, Loss: 0.023667\n",
            "Epoch [116/10000], iteration 35, Loss: 0.022286\n",
            "Epoch [116/10000], iteration 40, Loss: 0.027750\n",
            "Epoch [116/10000], iteration 45, Loss: 0.027333\n",
            "Epoch [116/10000], iteration 50, Loss: 0.026400\n",
            "Epoch [116/10000], iteration 55, Loss: 0.026182\n",
            "Epoch [116/10000], iteration 60, Loss: 0.026500\n",
            "Epoch [116/10000], iteration 65, Loss: 0.031077\n",
            "Epoch [116/10000], iteration 70, Loss: 0.030571\n",
            "Epoch [116/10000], iteration 75, Loss: 0.030933\n",
            "Epoch [116/10000], iteration 80, Loss: 0.029500\n",
            "Epoch [116/10000], iteration 85, Loss: 0.028706\n",
            "Epoch [116/10000], iteration 90, Loss: 0.027444\n",
            "Epoch [116/10000], iteration 95, Loss: 0.026421\n",
            "Epoch [116/10000], iteration 100, Loss: 0.025600\n",
            "validating....\n",
            "Epoch 116, validation loss is 0.0401, training loss is 0.0256\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 97\n",
            "Epoch [117/10000], iteration 5, Loss: 0.010000\n",
            "Epoch [117/10000], iteration 10, Loss: 0.017000\n",
            "Epoch [117/10000], iteration 15, Loss: 0.023333\n",
            "Epoch [117/10000], iteration 20, Loss: 0.022500\n",
            "Epoch [117/10000], iteration 25, Loss: 0.022000\n",
            "Epoch [117/10000], iteration 30, Loss: 0.022000\n",
            "Epoch [117/10000], iteration 35, Loss: 0.021429\n",
            "Epoch [117/10000], iteration 40, Loss: 0.029500\n",
            "Epoch [117/10000], iteration 45, Loss: 0.027333\n",
            "Epoch [117/10000], iteration 50, Loss: 0.025600\n",
            "Epoch [117/10000], iteration 55, Loss: 0.030182\n",
            "Epoch [117/10000], iteration 60, Loss: 0.029667\n",
            "Epoch [117/10000], iteration 65, Loss: 0.028769\n",
            "Epoch [117/10000], iteration 70, Loss: 0.031143\n",
            "Epoch [117/10000], iteration 75, Loss: 0.029333\n",
            "Epoch [117/10000], iteration 80, Loss: 0.028375\n",
            "Epoch [117/10000], iteration 85, Loss: 0.029176\n",
            "Epoch [117/10000], iteration 90, Loss: 0.029222\n",
            "Epoch [117/10000], iteration 95, Loss: 0.030105\n",
            "Epoch [117/10000], iteration 100, Loss: 0.029200\n",
            "validating....\n",
            "Epoch 117, validation loss is 0.0315, training loss is 0.0292\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 98\n",
            "Epoch [118/10000], iteration 5, Loss: 0.008000\n",
            "Epoch [118/10000], iteration 10, Loss: 0.035000\n",
            "Epoch [118/10000], iteration 15, Loss: 0.032667\n",
            "Epoch [118/10000], iteration 20, Loss: 0.028000\n",
            "Epoch [118/10000], iteration 25, Loss: 0.026000\n",
            "Epoch [118/10000], iteration 30, Loss: 0.024667\n",
            "Epoch [118/10000], iteration 35, Loss: 0.023143\n",
            "Epoch [118/10000], iteration 40, Loss: 0.021500\n",
            "Epoch [118/10000], iteration 45, Loss: 0.020667\n",
            "Epoch [118/10000], iteration 50, Loss: 0.019200\n",
            "Epoch [118/10000], iteration 55, Loss: 0.022364\n",
            "Epoch [118/10000], iteration 60, Loss: 0.021500\n",
            "Epoch [118/10000], iteration 65, Loss: 0.022154\n",
            "Epoch [118/10000], iteration 70, Loss: 0.025286\n",
            "Epoch [118/10000], iteration 75, Loss: 0.025067\n",
            "Epoch [118/10000], iteration 80, Loss: 0.024000\n",
            "Epoch [118/10000], iteration 85, Loss: 0.023882\n",
            "Epoch [118/10000], iteration 90, Loss: 0.026111\n",
            "Epoch [118/10000], iteration 95, Loss: 0.026105\n",
            "Epoch [118/10000], iteration 100, Loss: 0.025500\n",
            "validating....\n",
            "Epoch 118, validation loss is 0.0396, training loss is 0.0255\n",
            "Validation loss does not decrease from 0.0286, num_epoch_no_improvement 99\n",
            "Early Stopping\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "# ref https://github.com/MrGiovanni/ModelsGenesis\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import sys\n",
        "# from utils import *\n",
        "# from unet_model2 import UNet\n",
        "# from config_cluster import models_genesis_config\n",
        "# from data_load import Dataset_Loader\n",
        "import logging\n",
        "import os\n",
        "\n",
        "print(\"torch = {}\".format(torch.__version__))\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "\n",
        "conf = models_genesis_config()\n",
        "conf.display()\n",
        "img_size = [256,256]\n",
        "\n",
        "# data load\n",
        "\n",
        "'''\n",
        "in unsupervised learning,\n",
        "train set = 1600\n",
        "validation set = 400\n",
        "test 1: fix the train set for the first 1600\n",
        "pixel value scale :[0,1]\n",
        "resize: 16*N  = 256\n",
        "'''\n",
        "train_path = '/content/drive/MyDrive/Spring_research_2023/data/GrayData/imgs_train.npy'\n",
        "train_set = Dataset_Loader(train_path,img_size)\n",
        "\n",
        "train_num =  1600 # Lakmali: 1600\n",
        "valid_num =  400 # Lakmali: 400\n",
        "total_num = len(train_set)\n",
        "x_train = train_set[0:train_num]\n",
        "x_valid = train_set[train_num:train_num+valid_num]\n",
        "\n",
        "logging.basicConfig(filename=conf.shotdir+\"/\"+\"snapshot.txt\", level=logging.INFO,\n",
        "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
        "logging.info(str(conf))\n",
        "\n",
        "print(\"x_train: {} | {:.2f} ~ {:.2f}\".format(x_train.shape, np.min(x_train), np.max(x_train)))\n",
        "print(\"x_valid: {} | {:.2f} ~ {:.2f}\".format(x_valid.shape, np.min(x_valid), np.max(x_valid)))\n",
        "\n",
        "training_generator = generate_pair(x_train,conf.batch_size, conf)\n",
        "validation_generator = generate_pair(x_valid,conf.batch_size, conf)\n",
        "\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Lakmali: model = UNet(n_channels=1, n_classes=conf.nb_class).cuda()\n",
        "model = UNet(n_channels=1, n_classes=conf.nb_class).cuda()\n",
        "#model = SwinUNet(input_channels=1, num_classes=conf.nb_class)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Total CUDA devices: \", torch.cuda.device_count())\n",
        "\n",
        "summary(model, (1,conf.input_rows,conf.input_cols), batch_size=-1)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "if conf.optimizer == \"sgd\":\n",
        "\toptimizer = torch.optim.SGD(model.parameters(), conf.lr, momentum=0.9, weight_decay=0.0, nesterov=False)\n",
        "elif conf.optimizer == \"adam\":\n",
        "\toptimizer = torch.optim.Adam(model.parameters(), conf.lr)\n",
        "else:\n",
        "\traise\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(conf.patience * 0.8), gamma=0.5)\n",
        "\n",
        "# to track the training loss as the model trains\n",
        "train_losses = []\n",
        "# to track the validation loss as the model trains\n",
        "valid_losses = []\n",
        "# to track the average training loss per epoch as the model trains\n",
        "avg_train_losses = []\n",
        "# to track the average validation loss per epoch as the model trains\n",
        "avg_valid_losses = []\n",
        "#best_loss = 100000\n",
        "best_loss = 0.03 # Lakmali: 0.02\n",
        "intial_epoch =0\n",
        "num_epoch_no_improvement = 0\n",
        "sys.stdout.flush()\n",
        "\n",
        "print(conf.weights) # Lakali:\n",
        "\n",
        "'''\n",
        "# Lakmali:\n",
        "file_path = conf.weights\n",
        "\n",
        "# Save the model weights as a .pt file\n",
        "torch.save(model.state_dict(), file_path)\n",
        "\n",
        "\n",
        "if conf.weights != None:    \n",
        "\tcheckpoint=torch.load(conf.weights)\n",
        "\tmodel.load_state_dict(checkpoint['state_dict'])\n",
        "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\tintial_epoch=checkpoint['epoch']\n",
        "\tprint(\"Loading weights from \",conf.weights)\n",
        "sys.stdout.flush()\n",
        "'''\n",
        "\n",
        "for epoch in range(intial_epoch,conf.nb_epoch):\n",
        "    scheduler.step(epoch)\n",
        "    model.train()\n",
        "    for iteration in range(100): # Lakmali:  for iteration in range(int(x_train.shape[0]//conf.batch_size)):\n",
        "        image, gt = next(training_generator)\n",
        "        gt = np.repeat(gt,conf.nb_class,axis=1)\n",
        "        image,gt = torch.from_numpy(image).float().to(device), torch.from_numpy(gt).float().to(device)\n",
        "        pred=model(image)\n",
        "        pred=torch.sigmoid(pred)\n",
        "        \n",
        "        loss = criterion(pred,gt)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(round(loss.item(), 2))\n",
        "        if (iteration + 1) % 5 ==0:\n",
        "            print('Epoch [{}/{}], iteration {}, Loss: {:.6f}'.format(epoch + 1, conf.nb_epoch, iteration + 1, np.average(train_losses)))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        print(\"validating....\")\n",
        "        for i in range(int(x_valid.shape[0]//conf.batch_size)):\n",
        "            x,y = next(validation_generator)\n",
        "            y = np.repeat(y,conf.nb_class,axis=1)\n",
        "            image,gt = torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
        "            image=image.to(device)\n",
        "            gt=gt.to(device)\n",
        "            pred=model(image)\n",
        "            pred=torch.sigmoid(pred)\n",
        "            loss = criterion(pred,gt)\n",
        "            valid_losses.append(loss.item())\n",
        "    \n",
        "    #logging\n",
        "    train_loss=np.average(train_losses)\n",
        "    valid_loss=np.average(valid_losses)\n",
        "    avg_train_losses.append(train_loss)\n",
        "    avg_valid_losses.append(valid_loss)\n",
        "    print(\"Epoch {}, validation loss is {:.4f}, training loss is {:.4f}\".format(epoch+1,valid_loss,train_loss))\n",
        "    train_losses=[]\n",
        "    valid_losses=[]\n",
        "    if valid_loss < best_loss:\n",
        "        print(\"Validation loss decreases from {:.4f} to {:.4f}\".format(best_loss, valid_loss))\n",
        "        best_loss = valid_loss\n",
        "        num_epoch_no_improvement = 0\n",
        "        #save model\n",
        "        torch.save({\n",
        "            'epoch': epoch+1,\n",
        "            'state_dict' : model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        },os.path.join(conf.model_path, \"ISIC_Unsup.pt\"))\n",
        "        print(\"Saving model \",os.path.join(conf.model_path, \"ISIC_Unsup.pt\"))\n",
        "    else:\n",
        "        print(\"Validation loss does not decrease from {:.4f}, num_epoch_no_improvement {}\".format(best_loss,num_epoch_no_improvement))\n",
        "        num_epoch_no_improvement += 1\n",
        "    if num_epoch_no_improvement == conf.patience:\n",
        "        print(\"Early Stopping\")\n",
        "        break\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Z9qceypzrJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}